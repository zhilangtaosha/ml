{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../../mltestdata/03_predict_hourly_wage/Income_training.csv\")\n",
    "test = pd.read_csv(\"../../../mltestdata/03_predict_hourly_wage/Income_testing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploring\n",
    "* * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessiong\n",
    "* * * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train.compositeHourlyWages.values\n",
    "\n",
    "test_ID = test['ID']\n",
    "test.drop(\"ID\", axis = 1, inplace = True)\n",
    "\n",
    "train.drop(['compositeHourlyWages'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train,train_target,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a cross validation strategy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=2017).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, train_target, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **LASSO  Regression**  : \n",
    "\n",
    "The sklearn's  **Robustscaler()**  method on pipeline is used since LASSO is easily affected by outliers.\n",
    "\n",
    ">[RobustScaler](https://blog.nownabe.com/2017/11/19/1185.html)\n",
    ">平均値と分散のかわりに中央地と四分位数を用いる\n",
    ">外れ値を無視する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "\n",
    "LAS = make_pipeline(RobustScaler(), Lasso(alpha =1e-05, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Elastic Net Regression** :\n",
    "\n",
    "As well as Lasso, we made this robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "\n",
    "ENE = make_pipeline(RobustScaler(), ElasticNet(alpha=0.064, l1_ratio=.9, random_state=2017))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Kernel Ridge Regression** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "KRR = KernelRidge(alpha=0.1, coef0=20, degree=2, gamma=100.0, kernel='polynomial',kernel_params=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **XGBoost** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGR = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "#                             learning_rate=0.05, max_depth=3, \n",
    "#                             min_child_weight=1.7817, n_estimators=2200,\n",
    "#                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "#                            subsample=0.5213, silent=1,\n",
    "#                             seed =0, nthread = -1)\n",
    "\n",
    "XGR = xgb.XGBRegressor(colsample_bytree= 1.0, learning_rate= 0.1, max_depth=2, min_child_weight= 14, n_estimators= 100, objective= 'reg:gamma', subsample= 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Gradient Boosting Regression** :\n",
    "\n",
    "With **huber**  loss that makes it robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBR = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "#                                   max_depth=4, max_features='sqrt',\n",
    "#                                   min_samples_leaf=15, min_samples_split=10, \n",
    "#                                   loss='huber', random_state =5)\n",
    "\n",
    "GBR = GradientBoostingRegressor(n_estimators=100, learning_rate=0.05,\n",
    "                                max_depth=4, max_features=0.1,\n",
    "                                min_samples_leaf=17, min_samples_split=10, \n",
    "                                loss='huber', random_state =2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LightGBM** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LGB = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "#                              learning_rate=0.05, n_estimators=720,\n",
    "#                              max_bin = 55, bagging_fraction = 0.8,\n",
    "#                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "#                              feature_fraction_seed=9, bagging_seed=9,\n",
    "#                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "\n",
    "LGB = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                        learning_rate=0.05, n_estimators=100,\n",
    "                        reg_alpha= 1.2, reg_lambda= 1.4, \n",
    "                        subsample= 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base models scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso score: 6.4988 (0.3139)\n",
      "\n",
      "ElasticNet score: 6.5024 (0.3112)\n",
      "\n",
      "Kernel Ridge score: 6.1857 (0.3479)\n",
      "\n",
      "Gradient Boosting score: 6.2250 (0.3464)\n",
      "\n",
      "Xgboost score: 6.1819 (0.3542)\n",
      "\n",
      "LGBM score: 6.1850 (0.3577)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(LAS)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(ENE)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(KRR)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(GBR)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(XGR)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(LGB)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking  models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with this simple approach of averaging base models.  We build a new **class**  to extend scikit-learn with our model and also to laverage encapsulation and code reuse ([inheritance][1]) \n",
    "\n",
    "\n",
    "  [1]: https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Averaged base model\n",
    "**Averaged base models class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Averaged base models score**\n",
    "\n",
    "We just average four models here **ENet, GBoost,  KRR and lasso**.  Of course we could easily add more models in the mix. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Averaged base models score: 6.1693 (0.3511)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#averaged_models = AveragingModels(models = (ENE, GBR, KRR, LAS))\n",
    "averaged_models = AveragingModels(models = (KRR, GBR, XGR))\n",
    "\n",
    "score = rmsle_cv(averaged_models)\n",
    "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Less simple Stacking : Adding a Meta-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stacking averaged Models Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Averaged models score: 6.1755 (0.3456)\n"
     ]
    }
   ],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENE, GBR, KRR),\n",
    "                                                 meta_model = LAS)\n",
    "\n",
    "score = rmsle_cv(stacked_averaged_models)\n",
    "print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ensembling StackedRegressor, XGBoost and LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StackedRegressor:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.09256738344\n"
     ]
    }
   ],
   "source": [
    "stacked_averaged_models.fit(train.values, train_target)\n",
    "stacked_train_pred = stacked_averaged_models.predict(train.values)\n",
    "stacked_pred = stacked_averaged_models.predict(test.values)\n",
    "print(rmsle(train_target, stacked_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.09006203477\n"
     ]
    }
   ],
   "source": [
    "XGR.fit(train, train_target)\n",
    "xgb_train_pred = XGR.predict(train)\n",
    "xgb_pred = XGR.predict(test)\n",
    "print(rmsle(train_target, xgb_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LightGBM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.06337519612\n"
     ]
    }
   ],
   "source": [
    "LGB.fit(train, train_target)\n",
    "lgb_train_pred = LGB.predict(train)\n",
    "lgb_pred = LGB.predict(test.values)\n",
    "print(rmsle(train_target, lgb_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "c = 0\n",
    "\n",
    "scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_i in range(a,100,1):\n",
    "    if 100 > a_i:\n",
    "        for b_i in range(b,100,1):\n",
    "            if 100 - a_i > b_i:\n",
    "                c_i = 100 - a_i - b_i\n",
    "                \n",
    "                scores.append([a_i,b_i,c_i])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score 6.062367\n",
      "weight_s 0.150000\n",
      "weight_x 0.000000\n",
      "weight_l 0.850000\n"
     ]
    }
   ],
   "source": [
    "best_score = 10\n",
    "scores = []\n",
    "weight_s = 0.0\n",
    "weight_x = 0.0\n",
    "weight_l = 0.0\n",
    "\n",
    "for ws in np.arange(weight_s,1.01,0.01):\n",
    "    if 100 > ws:\n",
    "        for wx in np.arange(weight_x,1.01,0.01):\n",
    "            if 100 - ws > wx:\n",
    "                wl = 1 - ws - wx\n",
    "        \n",
    "                rmsle_tmp = rmsle(train_target,stacked_train_pred*ws + xgb_train_pred*wx + lgb_train_pred*wl )\n",
    "                scores.append(rmsle_tmp)\n",
    "\n",
    "                if best_score > rmsle_tmp:\n",
    "                    best_score = rmsle_tmp\n",
    "                    weight_s = ws\n",
    "                    weight_x = wx\n",
    "                    weight_l = wl\n",
    "\n",
    "print(\"best_score %f\" % best_score)\n",
    "print(\"weight_s %f\" % weight_s)\n",
    "print(\"weight_x %f\" % weight_x)\n",
    "print(\"weight_l %f\" % weight_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE score on train data:\n",
      "6.0784396635\n"
     ]
    }
   ],
   "source": [
    "'''RMSE on the entire Train data when averaging'''\n",
    "\n",
    "print('RMSLE score on train data:')\n",
    "print(rmsle(train_target,stacked_train_pred*0.70 +\n",
    "               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE score on train data:\n",
      "6.0624836888\n"
     ]
    }
   ],
   "source": [
    "'''RMSE on the entire Train data when averaging'''\n",
    "\n",
    "print('RMSLE score on train data:')\n",
    "print(rmsle(train_target,stacked_train_pred*0.15 +\n",
    "               xgb_train_pred*0.01 + lgb_train_pred*0.84 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE score on train data:\n",
      "6.06236654826\n"
     ]
    }
   ],
   "source": [
    "'''RMSE on the entire Train data when averaging'''\n",
    "\n",
    "print('RMSLE score on train data:')\n",
    "print(rmsle(train_target,stacked_train_pred*0.15 +\n",
    "               xgb_train_pred*0.00 + lgb_train_pred*0.85 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE score on train data:\n",
      "6.06369702241\n"
     ]
    }
   ],
   "source": [
    "'''RMSE on the entire Train data when averaging'''\n",
    "\n",
    "print('RMSLE score on train data:')\n",
    "print(rmsle(train_target,stacked_train_pred*0.15 +\n",
    "               xgb_train_pred*0.10 + lgb_train_pred*0.75 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble prediction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = stacked_pred*0.15 + xgb_pred*0.01 + lgb_pred*0.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = stacked_pred*0.15 + xgb_pred*0.00 + lgb_pred*0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = stacked_pred*0.15 + xgb_pred*0.07 + lgb_pred*0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['ID'] = test_ID\n",
    "sub['compositeHourlyWages'] = ensemble\n",
    "sub.to_csv('rs_hourly_submission_02Jan18_03.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=2017).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, train_target, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regression\n",
    "\n",
    "Reference\n",
    "- [Gradient Boosted Regression Trees](https://www.datarobot.com/blog/gradient-boosted-regression-trees/)\n",
    "- [Caifornia house price predictions with Gradient Boosted Regression Trees](https://shankarmsy.github.io/stories/gbrt-sklearn.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________\n",
      "Result of Gridsearch\n",
      "Best params:  {'learning_rate': 0.05, 'max_depth': 4, 'max_features': 0.1, 'min_samples_leaf': 17, 'n_estimators': 100}\n",
      "Best Estimator:  GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.05, loss='ls', max_depth=4, max_features=0.1,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=17,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)\n",
      "MSE:  0.336578545079\n",
      "______________________________\n",
      "vs Prediction\n",
      "RMSE from local train:  6.00336616982\n",
      "MSE from local train:  36.040405369\n",
      "R2 from local train:  0.39952132305\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# model_selection.GridSearchCV\n",
    "# =================================================\n",
    "# 1) Create model\n",
    "# Set params\n",
    "# Scores XXX\n",
    "est = GradientBoostingRegressor(n_estimators=3000)\n",
    "\n",
    "# 2) Set the grid\n",
    "param_grid = {'n_estimators':[100,1000,3000], \n",
    "              'learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
    "              'max_depth': [4, 6, 8],\n",
    "              'min_samples_leaf': [3, 5, 9, 17],\n",
    "#              'min_saples_split': [5, 10, 15],\n",
    "              'max_features': [1.0, 0.3, 0.1] ## not possible in our example (only 1 fx)\n",
    "              }\n",
    "# 3) Run GridSearch\n",
    "grid = GridSearchCV(est, param_grid, n_jobs=5).fit(X_train, y_train)\n",
    "\n",
    "# 4) Show best Params and Score\n",
    "print(\"_\"*30)\n",
    "print(\"Result of Gridsearch\")\n",
    "print(\"Best params: \", grid.best_params_)\n",
    "print(\"Best Estimator: \", grid.best_estimator_)\n",
    "print(\"MSE: \", grid.best_score_)\n",
    "\n",
    "# 5) Learning with best params\n",
    "gbm_g = GradientBoostingRegressor(**grid.best_params_)\n",
    "gbm_g.fit(X_train, y_train)\n",
    "y_pred_gs = gbm_g.predict(X_test)\n",
    "\n",
    "# 6) The error metric: RMSE\n",
    "print(\"_\"*30)\n",
    "print(\"vs Prediction\")\n",
    "print(\"RMSE from local train: \", rmse(y_test, y_pred_gs))\n",
    "print(\"MSE from local train: \", mean_squared_error(y_test, y_pred_gs))\n",
    "print(\"R2 from local train: \", r2_score(y_test, y_pred_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 648 candidates, totalling 3240 fits\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# model_selection.GridSearchCV\n",
    "# =================================================\n",
    "# 1) Create model\n",
    "mdl = lgb.LGBMRegressor(boosting_type= 'gbdt', \n",
    "                        n_jobs = 5,\n",
    "                        metric='RMSE'\n",
    "                       )\n",
    "\n",
    "\n",
    "#Best params:  {'colsample_bytree': 0.8, 'learning_rate': 0.05, \n",
    "#'max_depth': 3, 'n_estimators': 100, 'num_leaves': 5, \n",
    "#'objective': 'regression', 'reg_alpha': 1.2, 'reg_lambda': 1.4, 'subsample': 0.75}\n",
    "\n",
    "# 2) Set params for gridsearch\n",
    "gridParams = {\n",
    "    'objective': ['binary','regression'],\n",
    "    'num_leaves': [4,5,6], #2,10,20,100   \n",
    "    'learning_rate': [0.05, 0.06], # 0.005,\n",
    "    'n_estimators': [100], #8,24,\n",
    "    'colsample_bytree' :[0.8, 0.85, 0.9], #0.64,\n",
    "    'reg_lambda' : [1.3,1.4,1.5], #1,1.2,\n",
    "    'max_depth' :[2,3,4], #1,2,5,10\n",
    "    'subsample' :[0.7,0.75], \n",
    "    'reg_alpha' : [1.2], #0.1,0.51,\n",
    "#    'min_split_gain' :[],\n",
    "#    'subsample_for_bin' :[],\n",
    "#    'max_drop' :[], \n",
    "#    'gaussian_eta' :[], \n",
    "#    'drop_rate' :[],\n",
    "#    'silent' :[], \n",
    "#    'boosting_type' :['gbdt'], \n",
    "#    'min_child_weight' :[], \n",
    "#    'skip_drop' :[], \n",
    "#    'fair_c' :[], \n",
    "#    'seed' :[], \n",
    "#    'poisson_max_delta_step' :[], \n",
    "#    'subsample_freq' :[], \n",
    "#    'max_bin' :[],  #55\n",
    "#    'nthread' :[], \n",
    "#    'min_child_samples' :[], \n",
    "#    'huber_delta' :[], \n",
    "#    'use_missing' :[], \n",
    "#    'uniform_drop' :[], \n",
    "#    'bagging_fraction': [] #0.8,\n",
    "#    'bagging_freq': [] # 5\n",
    "#    'feature_fraction': [] # 0.2319,\n",
    "#    'feature_fraction_seed': [] #9\n",
    "#    'bagging_seed': [] #9,\n",
    "#    'min_data_in_leaf': [] #6\n",
    "#    'min_sum_hessian_in_leaf': [] # 11                              \n",
    "#    'xgboost_dart_mode' :[]\n",
    "}\n",
    "\n",
    "# 3) Run GridSearch\n",
    "grid = GridSearchCV(mdl, gridParams, verbose=1, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# 4) Show best Params and Score\n",
    "print(\"_\"*30)\n",
    "print(\"Result of Gridsearch\")\n",
    "print(\"Best params: \", grid.best_params_)\n",
    "print(\"Best Estimator: \", grid.best_estimator_)\n",
    "print(\"MSE: \", grid.best_score_)\n",
    "\n",
    "# 5) Learning with best params\n",
    "lgm_g = lgb.LGBMRegressor(**grid.best_params_)\n",
    "lgm_g.fit(X_train, y_train)\n",
    "y_pred_gs = lgm_g.predict(X_test)\n",
    "\n",
    "# 6) The error metric: RMSE\n",
    "print(\"_\"*30)\n",
    "print(\"vs Prediction\")\n",
    "print(\"RMSE from local train: \", rmse(y_test, y_pred_gs))\n",
    "print(\"MSE from local train: \", mean_squared_error(y_test, y_pred_gs))\n",
    "print(\"R2 from local train: \", r2_score(y_test, y_pred_gs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Score: __\n",
    "```\n",
    "______________________________\n",
    "Result of Gridsearch\n",
    "Best params:  {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'num_leaves': 5, 'objective': 'regression', 'reg_alpha': 1.2, 'reg_lambda': 1.4, 'subsample': 0.75}\n",
    "Best Estimator:  LGBMRegressor(boosting_type='gbdt', colsample_bytree=0.8, learning_rate=0.05,\n",
    "       max_bin=255, max_depth=3, metric='RMSE', min_child_samples=20,\n",
    "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
    "       n_jobs=5, num_leaves=5, objective='regression', random_state=None,\n",
    "       reg_alpha=1.2, reg_lambda=1.4, silent=True, subsample=0.75,\n",
    "       subsample_for_bin=200000, subsample_freq=1)\n",
    "MSE:  0.349386487223\n",
    "______________________________\n",
    "vs Prediction\n",
    "RMSE from local train:  6.02327567741\n",
    "MSE from local train:  36.2798498861\n",
    "R2 from local train:  0.395531874947\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________\n",
      "Result of Gridsearch\n",
      "Best params:  {'alpha': 0.1, 'coef0': 20, 'degree': 2, 'gamma': 100.0, 'kernel': 'polynomial'}\n",
      "Best Estimator:  KernelRidge(alpha=0.1, coef0=20, degree=2, gamma=100.0, kernel='polynomial',\n",
      "      kernel_params=None)\n",
      "MSE:  -38.9194812638\n",
      "______________________________\n",
      "vs Prediction\n",
      "RMSE from local train:  5.9962579498\n",
      "MSE from local train:  35.9551094006\n",
      "R2 from local train:  0.400942461623\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# model_selection.GridSearchCV\n",
    "# =================================================\n",
    "# 1) Create model\n",
    "model = KernelRidge()\n",
    "\n",
    "# 2) Set params for gridsearch\n",
    "param_grid = {\n",
    "    \"alpha\": [1e0, 0.1, 1e-2, 1e-3],\n",
    "    \"gamma\": np.logspace(-2, 2, 5),\n",
    "    \"kernel\" : ['polynomial','rbf'],\n",
    "    \"degree\" : [2,5,10,20], \n",
    "    \"coef0\" : [2.5,5,10,20],\n",
    "}\n",
    "\n",
    "# 3) Run GridSearch\n",
    "model_ = GridSearchCV(estimator= model, param_grid= param_grid, scoring='neg_mean_squared_error',cv=5, n_jobs=-1)\n",
    "model_.fit(X_train, y_train)\n",
    "\n",
    "# 4) Show best Params and Score\n",
    "print(\"_\"*30)\n",
    "print(\"Result of Gridsearch\")\n",
    "print(\"Best params: \", model_.best_params_)\n",
    "print(\"Best Estimator: \", model_.best_estimator_)\n",
    "print(\"MSE: \", model_.best_score_)\n",
    "\n",
    "# 5) Learning with best params\n",
    "krr_g = KernelRidge(**model_.best_params_)\n",
    "krr_g.fit(X_train, y_train)\n",
    "y_pred_gs = krr_g.predict(X_test)\n",
    "\n",
    "# 6) The error metric: RMSE\n",
    "print(\"_\"*30)\n",
    "print(\"vs Prediction\")\n",
    "print(\"RMSE from local train: \", rmse(y_test, y_pred_gs))\n",
    "print(\"MSE from local train: \", mean_squared_error(y_test, y_pred_gs))\n",
    "print(\"R2 from local train: \", r2_score(y_test, y_pred_gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array([  1.00000000e-02,   1.00000000e-01,   1.00000000e+00,\n",
    "#         1.00000000e+01,   1.00000000e+02])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referring to followings\n",
    "- [Exploring features and regression models](https://www.kaggle.com/youssefer/xgb-and-lasso-regression)\n",
    "- [House Prices # Regression and Bagging techniques](https://www.kaggle.com/aarti1/house-prices-regression-and-bagging-techniques)\n",
    "- [XGB and Lasso Regression](https://www.kaggle.com/youssefer/xgb-and-lasso-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\tLasso regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11edafcf8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nudzNzCqQy93MrAK53M3MKtD/ByZxOamgmNAnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112c31d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#------------------------------------------------------------\n",
    "print (\"10\tLasso regression\")\n",
    "# importance of train set size: first, we set a relevant alpha\n",
    "\n",
    "rm_tr=[]\n",
    "rm_te=[]\n",
    "\n",
    "opti=[]\n",
    "alphas=[1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,1]\n",
    "# alphas=np.linspace(1e-4,1e-2,20)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     feat, price, test_size=0.8, random_state=42)\n",
    "\n",
    "for al in alphas:\n",
    "\tls=Lasso(alpha=al, copy_X=True, fit_intercept=True, max_iter=5000,\n",
    "\t   normalize=False, positive=False, precompute=False, random_state=111,\n",
    "\t   selection='cyclic', tol=0.0001, warm_start=False)\n",
    "\tls.fit(X_train,y_train)\n",
    "\trm_tr.append(np.sqrt(mean_squared_error(y_train,ls.predict(X_train))))\n",
    "\trm_te.append(np.sqrt(mean_squared_error(y_test,ls.predict(X_test))))\n",
    "\n",
    "plt.figure()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.plot(np.log(alphas),rm_tr,np.log(alphas),rm_te,\"r\")\t\n",
    "plt.title(\"Train and test error vs log(alphas)\")\n",
    "#plt.savefig('fig4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________\n",
      "Result of Gridsearch\n",
      "Best params:  {'alpha': 1e-05}\n",
      "Best Estimator:  Lasso(alpha=1e-05, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "MSE:  -42.9326318668\n",
      "______________________________\n",
      "vs Prediction\n",
      "RMSE from local train:  6.02475299555\n",
      "MSE from local train:  36.2976486573\n",
      "R2 from local train:  0.395235324935\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# model_selection.GridSearchCV\n",
    "# =================================================\n",
    "# 1) Create model\n",
    "model = Lasso()\n",
    "\n",
    "# 2) Set params for gridsearch\n",
    "param_grid = { 'alpha': [i/100000 for i in range(1,50000)]}\n",
    "\n",
    "# 3) Run GridSearch\n",
    "model_ = GridSearchCV(estimator= model, param_grid= param_grid, scoring='neg_mean_squared_error',cv=5, n_jobs=-1)\n",
    "model_.fit(X_train, y_train)\n",
    "\n",
    "# 4) Show best Params and Score\n",
    "print(\"_\"*30)\n",
    "print(\"Result of Gridsearch\")\n",
    "print(\"Best params: \", model_.best_params_)\n",
    "print(\"Best Estimator: \", model_.best_estimator_)\n",
    "print(\"MSE: \", model_.best_score_)\n",
    "\n",
    "# 5) Learning with best params\n",
    "las_g = xgb.XGBRegressor(**model_.best_params_)\n",
    "las_g.fit(X_train, y_train)\n",
    "y_pred_gs = las_g.predict(X_test)\n",
    "\n",
    "# 6) The error metric: RMSE\n",
    "print(\"_\"*30)\n",
    "print(\"vs Prediction\")\n",
    "print(\"RMSE from local train: \", rmse(y_test, y_pred_gs))\n",
    "print(\"MSE from local train: \", mean_squared_error(y_test, y_pred_gs))\n",
    "print(\"R2 from local train: \", r2_score(y_test, y_pred_gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute alpha = 1e-05 - 6.4987767707721575\n",
      "Compute alpha = 1.6681005372000593e-05 - 6.498776706041236\n",
      "Compute alpha = 2.782559402207126e-05 - 6.498776727807145\n",
      "Compute alpha = 4.641588833612782e-05 - 6.498777125134102\n",
      "Compute alpha = 7.742636826811278e-05 - 6.498778792472601\n",
      "Compute alpha = 0.0001291549665014884 - 6.498784368998028\n",
      "Compute alpha = 0.00021544346900318823 - 6.498801449091047\n",
      "Compute alpha = 0.00035938136638046257 - 6.498851582625532\n",
      "Compute alpha = 0.0005994842503189409 - 6.498995429448667\n",
      "Compute alpha = 0.001 - 6.499402929830128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x1139ea710>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASIAAADXCAYAAAC+llyoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1129c8780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =================================================\n",
    "# Manual GridSearch\n",
    "# =================================================\n",
    "# Search `alpha` regression parameters\n",
    "#x_train = x[x.train_test == 1]\n",
    "lasso = Lasso(max_iter=1e2, normalize=True)\n",
    "alphas = np.logspace(-5, -3, 10)\n",
    "scores = []\n",
    "scores_std = []\n",
    "for alpha in alphas:\n",
    "    lasso.alpha = alpha\n",
    "    this_scores = np.sqrt(-cross_val_score(lasso, train, train_target, cv=5, scoring='mean_squared_error'))\n",
    "    print('Compute alpha = {} - {}'.format(alpha, np.mean(this_scores)))\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.semilogx(alphas, scores)\n",
    "plt.semilogx(alphas, np.array(scores) + np.array(scores_std) / np.sqrt(len(train)), 'b--')\n",
    "plt.semilogx(alphas, np.array(scores) - np.array(scores_std) / np.sqrt(len(train)), 'b--')\n",
    "plt.ylabel('CV score')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "#plt.savefig('lasso_lars.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0649678226388\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "enr_cv = ElasticNetCV(cv=5, random_state=2017)\n",
    "enr_cv.fit(X_train, y_train)\n",
    "print(enr_cv.alpha_)\n",
    "print(enr_cv.l1_ratio_)\n",
    "#print(enr_cv.intercept_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________\n",
      "Result of Gridsearch\n",
      "Best params:  {'alpha': 0.064, 'l1_ratio': 0.9}\n",
      "Best Estimator:  ElasticNet(alpha=0.064, copy_X=True, fit_intercept=True, l1_ratio=0.9,\n",
      "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "MSE:  -42.9581056118\n",
      "______________________________\n",
      "vs Prediction\n",
      "RMSE from local train:  6.2730675813\n",
      "MSE from local train:  39.3513768796\n",
      "R2 from local train:  0.344356355515\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# model_selection.GridSearchCV\n",
    "# =================================================\n",
    "# 1) Create model\n",
    "regr = ElasticNet()\n",
    "\n",
    "# 2) Set params for gridsearch\n",
    "enr_params = {\n",
    "    'alpha' : [0.0640, 0.0645, 0.0649678226388, 0.65],\n",
    "    'l1_ratio' : [0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "}\n",
    "\n",
    "# 3) Run gridsearch\n",
    "grid_enr = GridSearchCV(regr,enr_params,scoring='neg_mean_squared_error',cv=5,n_jobs=-1)\n",
    "grid_enr.fit(X_train, y_train)\n",
    "\n",
    "# 4) Show best params and score\n",
    "print(\"_\"*30)\n",
    "print(\"Result of Gridsearch\")\n",
    "print(\"Best params: \", grid_enr.best_params_)\n",
    "print(\"Best Estimator: \", grid_enr.best_estimator_)\n",
    "print(\"MSE: \", grid_enr.best_score_)\n",
    "\n",
    "# 5) Learning with best params\n",
    "eln_g = ElasticNet(**grid_enr.best_params_)\n",
    "eln_g.fit(X_train, y_train)\n",
    "y_pred_gs = eln_g.predict(X_test)\n",
    "\n",
    "# 6) The error metric: RMSE\n",
    "print(\"_\"*30)\n",
    "print(\"vs Prediction\")\n",
    "print(\"RMSE from local train: \", rmse(y_test, y_pred_gs))\n",
    "print(\"MSE from local train: \", mean_squared_error(y_test, y_pred_gs))\n",
    "print(\"R2 from local train: \", r2_score(y_test, y_pred_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 2, 'min_child_weight': 14, 'n_estimators': 100, 'objective': 'reg:gamma', 'subsample': 0.8}\n",
      "Best Estimator:  XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1.0, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=2, min_child_weight=14, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='reg:gamma', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.8)\n",
      "MSE:  -38.7922204065\n",
      "RMSE from local train:  5.99283617268\n",
      "MSE from local train:  35.9140853925\n",
      "R2 from local train:  0.401625973415\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# model_selection.GridSearchCV\n",
    "# =================================================\n",
    "# 1) xgboostモデルの作成\n",
    "reg = xgb.XGBRegressor()\n",
    "\n",
    "# 2) XGBoost params\n",
    "xgb_params = {\n",
    "    'objective' : ['reg:gamma','reg:linear'],\n",
    "    'learning_rate' : [0.05,0.75,0.1,0.125],\n",
    "    'n_estimators' : [50,100,200],\n",
    "    'max_depth' : [2,4,6],\n",
    "    'subsample' : [0.79,0.8,0.81,0.85],\n",
    "    'colsample_bytree' : [0.9,1.0],\n",
    "    'min_child_weight' : [13,14, 15, 16]\n",
    "}\n",
    "\n",
    "# 3) Run GridSearch\n",
    "grid_xgb = GridSearchCV(reg,xgb_params,scoring='neg_mean_squared_error',cv=5,n_jobs=-1)\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "# 4) Show best Params and Score\n",
    "print(\"_\"*30)\n",
    "print(\"Result of Gridsearch\")\n",
    "print(\"Best params: \", grid_xgb.best_params_)\n",
    "print(\"Best Estimator: \", grid_xgb.best_estimator_)\n",
    "print(\"MSE: \", grid_xgb.best_score_)\n",
    "\n",
    "# 5) Learning with best params\n",
    "xgr_g = xgb.XGBRegressor(**grid_xgb.best_params_)\n",
    "xgr_g.fit(X_train, y_train)\n",
    "y_pred_gs = xgr_g.predict(X_test)\n",
    "\n",
    "# 6) The error metric: RMSE\n",
    "print(\"_\"*30)\n",
    "print(\"vs Prediction\")\n",
    "print(\"RMSE from local train: \", rmse(y_test, y_pred_gs))\n",
    "print(\"MSE from local train: \", mean_squared_error(y_test, y_pred_gs))\n",
    "print(\"R2 from local train: \", r2_score(y_test, y_pred_gs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
