{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../../mltestdata/03_predict_hourly_wage/Income_training.csv\")\n",
    "test = pd.read_csv(\"../../../mltestdata/03_predict_hourly_wage/Income_testing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploring\n",
    "* * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessiong\n",
    "* * * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train.compositeHourlyWages.values\n",
    "\n",
    "test_ID = test['ID']\n",
    "test.drop(\"ID\", axis = 1, inplace = True)\n",
    "\n",
    "train.drop(['compositeHourlyWages'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train,train_target,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a cross validation strategy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=2017).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, train_target, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **LASSO  Regression**  : \n",
    "\n",
    "The sklearn's  **Robustscaler()**  method on pipeline is used since LASSO is easily affected by outliers.\n",
    "\n",
    ">[RobustScaler](https://blog.nownabe.com/2017/11/19/1185.html)\n",
    ">平均値と分散のかわりに中央地と四分位数を用いる\n",
    ">外れ値を無視する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "\n",
    "LAS = make_pipeline(RobustScaler(), Lasso(alpha =1e-05, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Elastic Net Regression** :\n",
    "\n",
    "As well as Lasso, we made this robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "\n",
    "ENE = make_pipeline(RobustScaler(), ElasticNet(alpha=0.064, l1_ratio=.9, random_state=2017))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Kernel Ridge Regression** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "KRR = KernelRidge(alpha=0.1, coef0=20, degree=2, gamma=100.0, kernel='polynomial',kernel_params=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **XGBoost** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGR = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "#                             learning_rate=0.05, max_depth=3, \n",
    "#                             min_child_weight=1.7817, n_estimators=2200,\n",
    "#                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "#                            subsample=0.5213, silent=1,\n",
    "#                             seed =0, nthread = -1)\n",
    "\n",
    "XGR = xgb.XGBRegressor(colsample_bytree= 1.0, learning_rate= 0.1, max_depth=2, min_child_weight= 14, n_estimators= 100, objective= 'reg:gamma', subsample= 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Gradient Boosting Regression** :\n",
    "\n",
    "With **huber**  loss that makes it robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBR = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "#                                   max_depth=4, max_features='sqrt',\n",
    "#                                   min_samples_leaf=15, min_samples_split=10, \n",
    "#                                   loss='huber', random_state =5)\n",
    "\n",
    "GBR = GradientBoostingRegressor(n_estimators=100, learning_rate=0.05,\n",
    "                                max_depth=4, max_features=0.1,\n",
    "                                min_samples_leaf=17, min_samples_split=10, \n",
    "                                loss='huber', random_state =2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LightGBM** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LGB = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "#                              learning_rate=0.05, n_estimators=720,\n",
    "#                              max_bin = 55, bagging_fraction = 0.8,\n",
    "#                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "#                              feature_fraction_seed=9, bagging_seed=9,\n",
    "#                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "\n",
    "LGB = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                        learning_rate=0.05, n_estimators=100,\n",
    "                        reg_alpha= 1.2, reg_lambda= 1.4, \n",
    "                        subsample= 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base models scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso score: 6.4988 (0.3139)\n",
      "\n",
      "ElasticNet score: 6.5024 (0.3112)\n",
      "\n",
      "Kernel Ridge score: 6.1857 (0.3479)\n",
      "\n",
      "Gradient Boosting score: 6.2250 (0.3464)\n",
      "\n",
      "Xgboost score: 6.1819 (0.3542)\n",
      "\n",
      "LGBM score: 6.1850 (0.3577)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(LAS)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(ENE)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(KRR)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(GBR)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(XGR)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmsle_cv(LGB)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking  models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with this simple approach of averaging base models.  We build a new **class**  to extend scikit-learn with our model and also to laverage encapsulation and code reuse ([inheritance][1]) \n",
    "\n",
    "\n",
    "  [1]: https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Averaged base model\n",
    "**Averaged base models class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Averaged base models score**\n",
    "\n",
    "We just average four models here **ENet, GBoost,  KRR and lasso**.  Of course we could easily add more models in the mix. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Averaged base models score: 6.1693 (0.3511)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#averaged_models = AveragingModels(models = (ENE, GBR, KRR, LAS))\n",
    "averaged_models = AveragingModels(models = (KRR, GBR, XGR))\n",
    "\n",
    "score = rmsle_cv(averaged_models)\n",
    "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Less simple Stacking : Adding a Meta-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stacking averaged Models Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Averaged models score: 6.1755 (0.3456)\n"
     ]
    }
   ],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENE, GBR, KRR),\n",
    "                                                 meta_model = LAS)\n",
    "\n",
    "score = rmsle_cv(stacked_averaged_models)\n",
    "print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ensembling StackedRegressor, XGBoost and LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StackedRegressor:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.09256738344\n"
     ]
    }
   ],
   "source": [
    "stacked_averaged_models.fit(train.values, train_target)\n",
    "stacked_train_pred = stacked_averaged_models.predict(train.values)\n",
    "stacked_pred = stacked_averaged_models.predict(test.values)\n",
    "print(rmsle(train_target, stacked_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.09006203477\n"
     ]
    }
   ],
   "source": [
    "XGR.fit(train, train_target)\n",
    "xgb_train_pred = XGR.predict(train)\n",
    "xgb_pred = XGR.predict(test)\n",
    "print(rmsle(train_target, xgb_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LightGBM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.06337519612\n"
     ]
    }
   ],
   "source": [
    "LGB.fit(train, train_target)\n",
    "lgb_train_pred = LGB.predict(train)\n",
    "lgb_pred = LGB.predict(test.values)\n",
    "print(rmsle(train_target, lgb_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score 6.062484\n",
      "weight_s 0.150000\n",
      "weight_x 0.010000\n",
      "weight_l 0.840000\n"
     ]
    }
   ],
   "source": [
    "best_score = 10\n",
    "scores = []\n",
    "weight_s = 0.01\n",
    "weight_x = 0.01\n",
    "weight_l = 0.01\n",
    "\n",
    "for ws in np.arange(weight_s,1,0.01):\n",
    "    for wx in np.arange(weight_x,1,0.01):\n",
    "        wl = 1 - ws - wx\n",
    "#        print(\"{} {} {} \".format(type(ws),type(wx),type(wl)))\n",
    "        \n",
    "        rmsle_tmp = rmsle(train_target,stacked_train_pred*ws + xgb_train_pred*wx + lgb_train_pred*wl )\n",
    "        scores.append(rmsle_tmp)\n",
    "\n",
    "        if best_score > rmsle_tmp:\n",
    "            best_score = rmsle_tmp\n",
    "            weight_s = ws\n",
    "            weight_x = wx\n",
    "            weight_l = wl\n",
    "\n",
    "print(\"best_score %f\" % best_score)\n",
    "print(\"weight_s %f\" % weight_s)\n",
    "print(\"weight_x %f\" % weight_x)\n",
    "print(\"weight_l %f\" % weight_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE score on train data:\n",
      "6.0784396635\n"
     ]
    }
   ],
   "source": [
    "'''RMSE on the entire Train data when averaging'''\n",
    "\n",
    "print('RMSLE score on train data:')\n",
    "print(rmsle(train_target,stacked_train_pred*0.70 +\n",
    "               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE score on train data:\n",
      "6.0624836888\n"
     ]
    }
   ],
   "source": [
    "'''RMSE on the entire Train data when averaging'''\n",
    "\n",
    "print('RMSLE score on train data:')\n",
    "print(rmsle(train_target,stacked_train_pred*0.15 +\n",
    "               xgb_train_pred*0.01 + lgb_train_pred*0.84 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble prediction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = stacked_pred*0.15 + xgb_pred*0.01 + lgb_pred*0.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 21.75990486,  20.92606163,  10.36920643,  18.2219162 ,\n",
       "        14.21235752,  19.13841438,  19.19187546,  20.61090851,\n",
       "        13.68530369,  15.39455414,   9.51374245,   8.23761272,\n",
       "         9.03273964,  13.11780643,  13.67373848,  24.95092392,\n",
       "        17.84705925,  10.36620522,  14.25238514,  23.91424942,\n",
       "        15.53333092,  16.64477158,  13.93181515,  24.95711136,\n",
       "        14.03795052,  17.84705925,  18.97889137,   9.03273964,\n",
       "         6.52648544,  12.98662853,   7.54840183,  15.30398369,\n",
       "        20.51613045,  14.04156876,  11.69105148,  16.67649269,\n",
       "        14.30447674,   9.41928959,  18.84169769,  16.17910385,\n",
       "        12.4957962 ,  18.84169769,  19.38986015,   8.26354027,\n",
       "        14.78979301,  15.38266277,  13.96742153,  24.59828568,\n",
       "        18.83620453,   8.03920746,   8.03920746,  21.86898422,\n",
       "        13.62928772,   7.14982224,   8.21809959,  13.67373848,\n",
       "        25.14419937,  20.19174004,  16.85387993,  16.14746094,\n",
       "        24.83498383,  18.2173996 ,  13.58207035,  14.03795052,\n",
       "        13.90967083,   8.02016544,  21.86898422,  17.52148056,\n",
       "        13.90967083,  12.73621273,  13.31328964,  17.52148056,\n",
       "        15.09383011,   8.23761272,  18.8401432 ,  28.25723648,\n",
       "        18.38478851,  11.914217  ,  14.30447674,   7.14982224,\n",
       "        27.93823242,  18.45384026,  15.18176365,  17.52148056,\n",
       "        15.53333092,  11.98818111,  20.92606163,  17.92983055,\n",
       "         9.96124935,  18.2219162 ,  14.52326584,  17.69843102,\n",
       "        24.59828568,  16.85387993,  14.109128  ,  15.00124931,\n",
       "         8.43539906,  16.28668404,  12.05778599,  24.21582794,\n",
       "        24.59828568,  14.01001835,   5.76945162,  16.12546349,\n",
       "         7.44081879,   8.05562782,  24.73293686,   7.3315382 ,\n",
       "         9.65303612,  18.2219162 ,  14.85303593,  24.95092392,\n",
       "        14.78979301,  14.38159275,  11.65198421,  23.92017937,\n",
       "        10.45833969,  13.11780643,   9.05284405,  18.35896683,\n",
       "        12.10146999,  15.87357235,  10.44517517,  14.95497894,\n",
       "        23.29957581,  14.51422501,  18.38478851,  16.76864815,\n",
       "        16.14746094,  20.61364174,  28.25723648,  19.38986015,\n",
       "         7.31417179,  17.16639137,  14.30447674,  13.90967083,\n",
       "        14.03795052,  16.54230118,   7.14982224,  11.0414629 ,\n",
       "        14.30447674,  18.7203598 ,   8.28316116,  24.95711136,\n",
       "        24.47791481,  24.97025871,  20.77182388,  17.41255188,\n",
       "        15.09383011,  14.30447674,  23.01036072,  26.00906754,\n",
       "        23.29957581,  20.19174004,  18.8401432 ,  15.21372223,\n",
       "        20.77182388,  18.18968391,  15.28244209,  17.69843102,\n",
       "        25.09372711,  17.28514671,  12.29723454,  17.22650909,\n",
       "         7.44081879,  26.34702492,  16.43798256,  20.19174004,\n",
       "        17.92983055,  13.35792637,  12.73621273,  10.45833969,\n",
       "        14.21235752,  16.30452728,  20.92606163,  12.84204865,\n",
       "        17.52780342,  17.74915314,  13.64614677,  12.61982822,\n",
       "        27.95027924,  20.35536957,  12.54896355,  18.3216629 ,\n",
       "        17.69843102,  24.97025871,  11.79951954,  14.51422501,\n",
       "         8.25443745,  14.03795052,  15.87357235,  12.57422543,\n",
       "        17.41255188,  22.51270866,  18.97889137,  10.27533627,\n",
       "        15.30398369,  16.53245163,  23.43390465,  13.11780643,\n",
       "        17.74915314,  14.81942844,  14.51422501,  16.18289948,\n",
       "        21.86111832,  16.53374481,  15.09383011,  19.80711174,\n",
       "        18.97889137,  26.38847351,   9.41928959,  12.01593494,\n",
       "        20.078825  ,  16.14746094,  16.86664772,  14.16144943,\n",
       "         8.227355  ,  23.92017937,  14.78979301,   8.92291069,\n",
       "        17.41255188,  16.82614899,  18.2173996 ,  15.00124931,\n",
       "        18.2219162 ,  14.13765621,  11.97809029,  20.92606163,\n",
       "        12.8141346 ,  16.14746094,  17.0683918 ,  16.67649269,\n",
       "        14.51422501,  12.8141346 ,  15.49922657,   7.14982224,\n",
       "        17.52148056,  22.28425026,  18.45384026,  20.84814262,\n",
       "        14.04156876,  15.6275835 ,  16.14746094,  15.87357235,\n",
       "        16.76864815,  14.21235752,  15.18176365,  11.61820507,\n",
       "        12.84204865,  21.75488853,  20.00722694,  18.47768021,\n",
       "        18.2219162 ,  14.13765621,  24.68415451,  14.43914795,\n",
       "        26.7746563 ,  14.38159275,  20.72648048,  13.57417774,\n",
       "        16.54230118,   6.92427158,  14.30447674,  23.50423813,\n",
       "        14.21235752,  14.21235752,  12.8141346 ,  18.8401432 ,\n",
       "        13.11780643,  15.09383011,  12.11110115,  10.39919281,\n",
       "        19.62280083,  23.29957581,  12.73621273,   8.23761272,\n",
       "        17.13133049,  27.93823242,  20.73483849,  11.98818111,\n",
       "        18.97889137,   9.06259632,  17.74915314,  18.7203598 ,\n",
       "        16.76455498,  26.7746563 ,  22.28425026,  17.41255188,\n",
       "        19.50282097,  24.73293686,  24.11641693,  17.28668022,\n",
       "        13.64614677,   6.92427158,  20.72648048,  20.22638702,\n",
       "        15.04026985,  16.50824165,  22.20267487,  14.25238514,\n",
       "         8.21809959,  18.2219162 ,  22.4591465 ,  12.16291237,\n",
       "        18.97889137,  21.75488853,  10.45833969,   9.17520142,\n",
       "        15.77372169,  21.75488853,  22.20267487,  17.21285248,\n",
       "        12.29723454,   6.89378929,  12.30356026,  13.46872902,\n",
       "        10.11626339,  20.61364174,   8.02016544,  18.35896683,\n",
       "        11.92536068,  13.46872902,  12.61982822,  20.73352242,\n",
       "        22.34996796,  11.62477779,  21.33170509,  13.90967083,\n",
       "        10.5352602 ,  21.75488853,  17.32673264,  16.66967201,\n",
       "        21.59762764,  20.77825928,  16.23510551,  28.01185417,\n",
       "        14.03795052,  12.16291237,  27.95027924,  16.98974037,\n",
       "         9.41928959,  15.5552454 ,  22.31722069,  20.19174004,\n",
       "        18.8401432 ,  16.20969009,  15.09383011,  28.14495277,\n",
       "         8.02016544,  10.44517517,  11.92536068,  10.45833969,\n",
       "        22.26815224,  14.51422501,  13.87933636,  21.33170509,\n",
       "         7.31417179,  10.31724739,  13.95759392,  17.28514671,\n",
       "        20.61364174,   8.06578064,  14.03795052,  19.07993507,\n",
       "        12.73621273,  18.84169769,  12.28316593,   8.02016544,\n",
       "        27.95027924,  13.87309361,  23.29957581,  12.01593494,\n",
       "        15.87357235,  12.16291237,  21.75488853,  12.73621273,\n",
       "        18.2219162 ,  15.04799938,  18.97889137,  14.13765621,\n",
       "         7.31417179,  17.23846054,  26.38847351,  14.10365582,\n",
       "        13.04828739,  18.2219162 ,  24.97025871,  11.914217  ,\n",
       "         8.92291069,   9.41928959,  20.5127697 ,  15.06433964,\n",
       "        16.53245163,   7.74058723,  21.33170509,  14.00549316,\n",
       "         7.31417179,  13.44357014,  13.43252659,  20.078825  ,\n",
       "        14.68447304,  18.2219162 ,  12.13871193,  28.01185417,\n",
       "        15.92150021,  11.13572979,   8.03920746,  16.39315414,\n",
       "        17.74915314,  24.95711136,   8.05562782,  12.95145035,\n",
       "        13.11780643,  14.89889431,  24.83498383,  20.61364174,\n",
       "        13.35792637,  12.49536705,   8.74112034,  14.03795052,\n",
       "        13.90967083,  17.52857208,  14.30447674,  14.51422501,\n",
       "        16.92172432,  25.24448967,   8.56853962,  14.83279419,\n",
       "        16.52587891,   7.40806246,  20.73483849,  16.39315414,\n",
       "        11.914217  ,  20.00722694,  21.22731972,  18.7203598 ,\n",
       "        13.86019325,  12.65184021,  21.6284008 ,   6.52648544,\n",
       "        11.92536068,  17.74915314,  11.35504913,  15.58197784,\n",
       "        23.92017937,  16.85387993,  26.7746563 ,  26.48758698,\n",
       "        25.09372711,  13.64614677,  12.14826584,  21.22731972,\n",
       "        18.2219162 ,  18.2173996 ,  19.67307472,  14.48534489,\n",
       "        15.74491787,  14.04156876,  11.55945015,  14.03795052,\n",
       "        18.7203598 ,  14.40350246,  18.7203598 ,  18.99849129,\n",
       "        13.90967083,  11.13572979,   9.36601925,  13.57417774,\n",
       "        21.33170509,  14.03795052,   7.31417179,  16.86664772,\n",
       "        15.06433964,  11.57238102,  21.17480087,  17.16639137,\n",
       "        18.8401432 ,  14.51422501,  13.67373848,  16.85387993,\n",
       "        15.18176365,  15.21372223,  17.41255188,  16.20969009,\n",
       "         9.41928959,  12.10146999,  12.73621273,  14.21235752,\n",
       "         9.38825798,  13.11780643,  22.34996796,   9.36601925,\n",
       "        24.68415451,  19.62280083,   9.38825798,  15.58085251,\n",
       "        14.34721184,  20.61364174,   9.36591434,  13.67373848,\n",
       "        20.00722694,  24.11641693,   8.03920746,  19.0493679 ,\n",
       "        13.83239365,   8.06578064,  11.92536068,  13.90967083,\n",
       "         9.22738647,  12.57422543,  13.67373848,  11.61980057,\n",
       "        14.69454956,  13.35792637,   9.66584969,  22.28278351,\n",
       "        26.4838028 ,  19.13841438,  16.98974037,  12.95145035,\n",
       "        16.54230118,  11.13572979,  16.98974037,  14.75970745,\n",
       "        19.19032478,  22.51270866,  16.94825172,  19.80711174,\n",
       "        16.50641441,   8.30007935,  16.29637909,  14.1530714 ,\n",
       "        14.18964291,   9.10290051,  18.35133934,  12.54670143,\n",
       "        21.95112419,   9.06259632,  22.00052643,  11.31463146,\n",
       "        14.21235752,  28.25723648,   9.01134396,  21.59762764,\n",
       "        20.84814262,  11.84059906,  16.67649269,  13.35792637,\n",
       "        14.51422501,  18.84169769,  20.61090851,  13.96019363,\n",
       "         8.52817345,  26.4838028 ,  11.62477779,   7.34651279,\n",
       "        13.43252659,  15.53333092,  14.78979301,  14.38159275,\n",
       "        16.85387993,  13.31328964,  16.67649269,  15.6275835 ,\n",
       "        13.90967083,  14.69454956,  13.90967083,  12.86515236,\n",
       "        19.50282097,  26.7746563 ,  17.83573341,  16.76597023,\n",
       "        20.25607681,  22.51270866,  14.03795052,  21.33170509,\n",
       "        13.06451225,  15.87357235,   8.03920746,  17.16639137,\n",
       "        12.61982822,  14.03795052,  21.86111832,  13.54591179,\n",
       "        20.77182388,  20.77182388,  10.67160606,  16.29637909,\n",
       "        20.22638702,  15.53333092,  13.50301361,  24.73293686,\n",
       "        18.2219162 ,  11.92536068,  14.03795052,  13.08478642,\n",
       "        12.4957962 ,  14.89889431,  18.42427444,  13.90967083,\n",
       "         7.14982224,  13.46872902,   6.92427158,  17.28514671,\n",
       "        15.77372169,  17.41255188,   7.54840183,  20.61364174,\n",
       "        13.62928772,  11.52143669,  16.35869789,  11.0902853 ,\n",
       "        15.18176365,  12.10146999,  20.84814262,  24.73293686,\n",
       "        15.1605711 ,  26.38847351,   8.45542717,   9.05118847,\n",
       "         8.92291069,  15.93386078,  16.67649269,  12.26293182,\n",
       "        21.75488853,   6.92427158,  16.92172432,  18.7203598 ,\n",
       "        23.43390465,  16.14746094,  27.95027924,  15.30398369,\n",
       "        23.92017937,   8.48127651,   9.22738647,  16.14746094,\n",
       "        17.41255188,  15.74491787,  20.19174004,  17.52148056,\n",
       "        14.30447674,  12.54896355,   6.26971292,  21.33170509,\n",
       "        18.2219162 ,  28.01185417,  13.83239365,  14.30447674,\n",
       "        14.21235752,  14.03795052,  14.73660183,  11.31463146,\n",
       "        16.75440025,  25.09372711,  14.21235752,   8.25443745,\n",
       "        17.12719345,  10.42388439,  14.30447674,  10.45833969,\n",
       "        12.57422543,  18.35133934,  10.45475197,   9.06259632,\n",
       "         9.03273964,  20.61090851,  15.92150021,  18.99849129,\n",
       "        19.66786766,  23.80312729,  18.42427444,  19.66786766,\n",
       "        14.21235752,  17.16639137,  14.30447674,   8.06578064,\n",
       "         9.36591434,  12.81785011,  17.0683918 ,   7.40806246,\n",
       "        18.472826  ,  13.14062214,  18.8401432 ,  14.13765621,\n",
       "        24.59828568,  20.61090851,  21.62202263,  11.61980057,\n",
       "        18.2219162 ,  15.68236065,  20.00722694,  12.61982822,\n",
       "        13.93181515,   7.67176437,  13.55183697,   8.45542717,\n",
       "         7.31417179,  16.54230118,  13.90967083,  12.73621273,\n",
       "        20.10729027,   7.31417179,  18.35133934,  20.92606163,\n",
       "         9.05284405,  13.14062214,  20.92606163,   8.02016544,\n",
       "         8.02016544,  16.03856277,  15.92150021,  14.40350246,\n",
       "        11.01360226,  11.57238102,  24.11641693,  14.34721184,\n",
       "        11.61980057,  15.68236065,  14.03795052,  14.21235752,\n",
       "        26.38847351,  19.62280083,   6.89378929,   9.03273964,\n",
       "        12.71087074,  18.97889137,  14.78979301,  20.19174004,\n",
       "        13.67373848,  11.13572979,  11.914217  ,  10.39919281,\n",
       "        27.95027924,  18.13274956,  22.77899933,  20.72648048,\n",
       "        17.74915314,  24.21582794,  17.32798576,   7.54840183,\n",
       "         9.22738647,   7.40806246,  23.92017937,   9.01134396,\n",
       "        18.97889137,  12.05778599,  23.92017937,  26.10675621,\n",
       "         9.05118847,   8.06578064,  20.84814262,   8.02016544,\n",
       "        27.80591393,  24.47791481,  14.03795052,  13.31328964,\n",
       "        18.97889137,  14.03795052,   6.52648544,   7.14982224,\n",
       "        16.90415573,  13.87877083,  16.43798256,  24.47791481,\n",
       "        21.75990486,  15.18176365,  16.28668404,  15.74491787,\n",
       "        13.06451225,  10.45833969,  16.03856277,  12.05778599,\n",
       "        13.11780643,  13.08478642,  14.75970745,  21.66135025,\n",
       "        24.97025871,  16.32987785,  12.10146999,   7.3315382 ,\n",
       "        16.23529434,  18.13655281,  16.52587891,  17.78172874,\n",
       "        16.77140617,  21.66135025,  14.03795052,  14.61538601,\n",
       "        10.31724739,  24.97025871,  18.2219162 ,  18.8401432 ,\n",
       "         7.3315382 ,  20.00722694,  11.52143669,  11.79924107,\n",
       "        12.71087074,  14.48534489,  10.15155315,  17.83573341], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 22.47678865,  19.86842463,   9.68178017,  18.58525551,\n",
       "        14.55424242,  18.47764662,  18.16970013,  19.11865651,\n",
       "        13.67122549,  16.27709079,  10.49427538,   8.27641372,\n",
       "         9.06919932,  13.14179762,  13.8287025 ,  24.62986078,\n",
       "        18.53175897,   7.2736464 ,  13.90973492,  22.84514214,\n",
       "        15.5990843 ,  16.32844242,  14.19355024,  24.30173182,\n",
       "        14.29035765,  18.10238474,  19.01906632,   9.06919932,\n",
       "         5.8686659 ,  12.90077121,   7.97550288,  14.87881781,\n",
       "        19.71593534,  13.77945727,  11.67860812,  16.67416508,\n",
       "        14.47468501,  10.68316131,  20.15331278,  16.77277169,\n",
       "        11.64023814,  20.15331278,  19.04761712,   8.41107612,\n",
       "        15.39334469,  15.00144088,  14.60464799,  23.42897986,\n",
       "        18.4397995 ,   8.56773691,   8.56773691,  22.0143821 ,\n",
       "        13.65191041,   6.72287665,   8.07285608,  13.8287025 ,\n",
       "        24.88779554,  19.03938923,  17.26529896,  16.42305882,\n",
       "        25.6998138 ,  19.44452062,  13.76832908,  14.17741323,\n",
       "        14.19149228,   7.93335814,  22.0143821 ,  18.22241072,\n",
       "        13.75295258,  12.90268703,  13.38423073,  18.22241072,\n",
       "        14.96043663,   8.27641372,  18.91612603,  28.56692586,\n",
       "        19.84497885,  11.27448951,  13.93848001,   7.15650045,\n",
       "        27.79000891,  19.91850129,  15.25855107,  18.29352655,\n",
       "        15.5990843 ,  12.02781576,  20.46350052,  17.5431667 ,\n",
       "         9.31150949,  17.9306685 ,  14.22710112,  18.05372655,\n",
       "        23.06712943,  16.95159382,  14.18830943,  14.90059466,\n",
       "         8.78718065,  16.25467645,  11.54823331,  24.79867166,\n",
       "        22.80313191,  13.86128169,   6.56919892,  16.64868081,\n",
       "         6.81261042,   7.84856281,  24.65189103,   7.2168863 ,\n",
       "        11.97643467,  18.20870918,  15.88454986,  25.98196336,\n",
       "        15.49773634,  15.16462299,  11.53705441,  22.48801183,\n",
       "        11.00800098,  13.14179762,   9.44558019,  17.88987223,\n",
       "        11.70166591,  15.74718464,  10.59214604,  15.77333934,\n",
       "        21.26374298,  14.93178941,  19.84497885,  16.31644257,\n",
       "        16.40297165,  20.07742388,  27.24915057,  19.04761712,\n",
       "         7.01489393,  16.46143207,  13.24449078,  14.37564438,\n",
       "        14.41275424,  16.5985113 ,   6.72287665,  10.56436281,\n",
       "        14.39104094,  20.01934587,   8.64136457,  23.2927437 ,\n",
       "        23.87378081,  24.16396496,  20.73480917,  16.98178928,\n",
       "        14.96043663,  13.45525254,  25.46089706,  25.69418327,\n",
       "        21.26374298,  19.66862208,  18.91612603,  15.39536716,\n",
       "        20.73480917,  17.04534937,  15.23121788,  18.17021292,\n",
       "        25.00058242,  17.17532328,  11.42192181,  17.98055379,\n",
       "         7.26032287,  26.68232988,  16.46823732,  19.22475504,\n",
       "        17.5431667 ,  13.56012098,  13.12303422,  11.25223631,\n",
       "        14.60265193,  16.18071385,  20.05520238,  13.16112517,\n",
       "        17.2639951 ,  17.76448685,  13.0672651 ,  13.02977613,\n",
       "        27.05734529,  21.09296767,  12.25351467,  18.80315752,\n",
       "        18.18296694,  24.19900182,  11.76047805,  14.93178941,\n",
       "         7.87181399,  14.17741323,  15.55023457,  12.29477484,\n",
       "        16.75603663,  22.80916851,  19.32063335,   7.40999247,\n",
       "        15.21641661,  16.99361838,  22.09978745,  13.14179762,\n",
       "        17.55697291,  14.6952884 ,  15.14660992,  16.61044754,\n",
       "        21.85540136,  16.78617365,  14.96043663,  19.17063595,\n",
       "        20.45186758,  27.70964908,  10.68316131,  11.63772118,\n",
       "        19.06086041,  16.42305882,  16.13043005,  14.21051424,\n",
       "         9.35726895,  21.95274226,  15.44958552,   9.65200416,\n",
       "        16.98178928,  17.14489452,  19.44452062,  14.90059466,\n",
       "        17.75101468,  14.5069426 ,  11.31047597,  20.46350052,\n",
       "        12.62342823,  16.42305882,  18.09590763,  16.67416508,\n",
       "        15.04179363,  12.62342823,  17.65815221,   7.15650045,\n",
       "        18.29352655,  21.45301457,  18.97147668,  20.90204713,\n",
       "        13.77945727,  15.45495538,  16.42305882,  15.74718464,\n",
       "        16.4699679 ,  14.62194756,  15.25855107,  11.73406106,\n",
       "        13.16112517,  21.87357706,  19.83506538,  17.65161425,\n",
       "        17.9306685 ,  14.62960525,  23.67421144,  13.89101789,\n",
       "        26.50155212,  15.13433133,  21.05231904,  13.47047227,\n",
       "        16.5985113 ,   6.45340539,  14.30903208,  25.88622858,\n",
       "        14.62194756,  14.60265193,  12.62342823,  18.91612603,\n",
       "        13.14179762,  14.96043663,  11.09205906,  10.73025793,\n",
       "        20.8809017 ,  21.16328181,  13.04535271,   8.27641372,\n",
       "        17.28374116,  31.07618616,  20.69401827,  12.02781576,\n",
       "        19.27541112,   9.2725011 ,  17.76448685,  20.15203144,\n",
       "        17.2613688 ,  26.70979317,  21.93416792,  17.03764413,\n",
       "        19.46387496,  24.35484325,  22.37134306,  17.07576665,\n",
       "        13.0672651 ,   6.45340539,  20.93625007,  19.67469676,\n",
       "        14.93396154,  16.97018832,  21.94039314,  13.90973492,\n",
       "         8.07285608,  18.87486229,  20.50015782,  12.19850461,\n",
       "        19.1986566 ,  22.24636242,  11.25223631,   9.49831649,\n",
       "        15.81321778,  21.87357706,  21.97425583,  17.70638214,\n",
       "        10.52859841,   6.63525225,  11.73558066,  14.71500141,\n",
       "         6.62169357,  19.61678058,   8.44401141,  17.88987223,\n",
       "        11.77193326,  14.71500141,  12.82791627,  20.96227996,\n",
       "        22.49031522,   9.53187734,  20.67303315,  13.64388806,\n",
       "        11.52713817,  22.301508  ,  17.32918022,  15.98259234,\n",
       "        21.76950741,  19.51124096,  17.20022862,  30.85407259,\n",
       "        14.2109842 ,  12.09453971,  26.98383906,  18.61883029,\n",
       "        10.49701276,  16.6337189 ,  22.97638004,  19.22475504,\n",
       "        19.60715387,  16.11626464,  14.96043663,  31.20686156,\n",
       "         8.44401141,  10.59214604,  11.77193326,  11.00800098,\n",
       "        22.27952908,  14.80878479,  13.56101874,  20.14076003,\n",
       "         7.01489393,  11.14466276,  14.37079761,  16.92300918,\n",
       "        19.61678058,   8.78561559,  14.29035765,  20.67098745,\n",
       "        13.04535271,  20.15331278,  11.94573968,   7.93335814,\n",
       "        27.92765003,  14.09447019,  21.16328181,  11.63772118,\n",
       "        15.74718464,  12.09453971,  22.20304748,  13.12303422,\n",
       "        17.75101468,  14.34831895,  19.29019798,  14.62960525,\n",
       "         7.42604164,  17.02870859,  27.08431523,  13.30308431,\n",
       "        13.03401443,  17.57332566,  24.27003541,  11.27448951,\n",
       "         9.65200416,  10.68316131,  20.49273661,  14.43639673,\n",
       "        16.99361838,   8.21705675,  20.14076003,  14.13075967,\n",
       "         7.01489393,  13.05863853,  12.91564373,  18.85847725,\n",
       "        13.93040886,  18.20870918,  12.22616848,  30.85407259,\n",
       "        15.96257857,  10.89375163,   8.04794089,  17.46516248,\n",
       "        17.76448685,  23.92738104,   8.34367011,  13.44249683,\n",
       "        13.14179762,  14.79342298,  24.36733592,  20.07742388,\n",
       "        13.04565322,  12.21139719,   9.1893419 ,  13.54916863,\n",
       "        13.72176903,  18.41348624,  13.71130814,  15.04179363,\n",
       "        16.90289277,  24.48298567,   9.90504323,  15.29071946,\n",
       "        17.10979352,   7.34924322,  20.38446715,  17.46516248,\n",
       "        11.27448951,  19.84859378,  21.08046383,  19.12104393,\n",
       "        13.32556166,  12.8847785 ,  20.4581113 ,   6.21618944,\n",
       "        11.77193326,  17.76448685,  11.42800982,  16.30549366,\n",
       "        21.95274226,  17.26529896,  26.71205885,  26.4552346 ,\n",
       "        25.69485571,  13.0672651 ,  12.25647146,  21.08046383,\n",
       "        18.42062842,  19.44452062,  19.03145778,  15.57562572,\n",
       "        17.22123022,  13.77945727,  11.57065847,  14.41275424,\n",
       "        19.12104393,  14.44709346,  20.01934587,  18.25509896,\n",
       "        14.19149228,  10.89375163,  10.23273024,  13.41203447,\n",
       "        20.91857836,  14.17741323,   7.42604164,  16.13043005,\n",
       "        14.43639673,  10.55084142,  22.08516712,  17.22130915,\n",
       "        19.60715387,  14.80878479,  13.99849334,  17.05980834,\n",
       "        15.25855107,  15.39536716,  16.98178928,  16.11626464,\n",
       "        10.68316131,  11.70166591,  13.00392454,  14.60265193,\n",
       "        10.40758436,  12.9727698 ,  22.49031522,  10.23273024,\n",
       "        24.01577793,  20.8809017 ,  10.40758436,  15.34599704,\n",
       "        14.16258011,  19.79969579,   9.6777438 ,  13.8287025 ,\n",
       "        19.76851647,  22.45784791,   8.56773691,  20.99306424,\n",
       "        14.10351879,   8.78561559,  11.77193326,  14.35165443,\n",
       "         9.83588432,  12.29477484,  13.42114091,  11.29771686,\n",
       "        15.61378503,  13.56012098,  10.28011795,  20.80361272,\n",
       "        26.15614437,  18.47764662,  19.32190627,  13.44249683,\n",
       "        16.5985113 ,  10.89375163,  19.32190627,  15.25218786,\n",
       "        20.89106074,  22.8719087 ,  16.62674828,  19.81279536,\n",
       "        15.39293055,   8.19773255,  16.13218896,  13.78869724,\n",
       "        14.45489781,   9.81151881,  18.24172898,  12.35086173,\n",
       "        22.23364288,   9.2725011 ,  23.31671781,  10.69906921,\n",
       "        14.62194756,  27.52138849,   8.92060719,  21.76950741,\n",
       "        20.76427506,   9.73276366,  16.67416508,  13.56012098,\n",
       "        14.80878479,  19.11179964,  19.11865651,  14.01767104,\n",
       "         8.92882747,  26.13266587,  10.00063177,   7.23313524,\n",
       "        12.91110738,  15.43934256,  15.44958552,  15.16462299,\n",
       "        17.42552024,  13.38423073,  16.67416508,  15.45495538,\n",
       "        14.37650354,  15.61378503,  14.37564438,  12.87435135,\n",
       "        19.46387496,  26.50155212,  17.81421044,  16.13360073,\n",
       "        19.69555862,  22.80916851,  14.17741323,  20.91857836,\n",
       "        12.84933736,  15.55023457,   8.04794089,  17.22130915,\n",
       "        12.97230201,  14.29035765,  21.81437374,  13.21507251,\n",
       "        20.73480917,  20.88270968,   9.24715446,  16.13218896,\n",
       "        19.67469676,  15.43934256,  12.65070412,  24.35484325,\n",
       "        18.42062842,  11.77193326,  14.29035765,  12.50120123,\n",
       "        11.2392608 ,  14.79342298,  20.16556486,  14.37650354,\n",
       "         6.72287665,  14.71500141,   6.45340539,  16.92300918,\n",
       "        15.81321778,  16.81529617,   7.97550288,  20.07742388,\n",
       "        13.79098153,  10.96783005,  15.58917615,  10.17255047,\n",
       "        15.25855107,  11.70166591,  20.34988227,  24.35484325,\n",
       "        14.38178631,  27.70964908,   9.04420666,   8.81630354,\n",
       "         9.65200416,  16.38657179,  16.67416508,  11.5193015 ,\n",
       "        22.02739799,   6.07767076,  16.90289277,  20.15203144,\n",
       "        22.09978745,  16.40297165,  28.09128089,  14.87881781,\n",
       "        22.24195987,   8.54839722,   9.83588432,  16.40297165,\n",
       "        16.75603663,  17.43791169,  19.22475504,  18.22241072,\n",
       "        13.07187241,  12.25351467,   6.1104849 ,  20.67303315,\n",
       "        18.42062842,  30.85407259,  14.10351879,  13.07187241,\n",
       "        14.62194756,  14.29035765,  13.84361885,  10.69906921,\n",
       "        16.89769552,  25.69485571,  14.60265193,   7.87181399,\n",
       "        17.67038524,  10.89382546,  13.71130814,  11.00800098,\n",
       "        12.29477484,  18.24172898,  12.06467529,   9.2725011 ,\n",
       "         9.06919932,  19.11865651,  15.96257857,  18.25509896,\n",
       "        18.7956262 ,  22.63816078,  20.11598295,  18.7956262 ,\n",
       "        13.92465303,  17.22130915,  14.01150599,   8.78561559,\n",
       "         9.6777438 ,  12.55776618,  18.09590763,   7.34924322,\n",
       "        19.08167489,  13.19378272,  19.08740009,  14.62960525,\n",
       "        23.60290972,  19.11865651,  20.94496774,  11.29771686,\n",
       "        18.58525551,  15.53597885,  19.84859378,  12.99705311,\n",
       "        14.19355024,   7.97809563,  11.63058883,   9.04420666,\n",
       "         7.42604164,  16.5985113 ,  14.19149228,  13.12303422,\n",
       "        19.96181208,   7.42604164,  18.13331145,  19.86842463,\n",
       "         9.44558019,  13.19378272,  20.32030952,   8.44401141,\n",
       "         8.44401141,  15.98807534,  15.96257857,  14.44709346,\n",
       "        10.58596483,  10.55084142,  22.37134306,  14.16258011,\n",
       "        11.29771686,  15.53597885,  14.2109842 ,  14.60265193,\n",
       "        26.2747813 ,  21.13216865,   6.63525225,   9.06919932,\n",
       "        13.03069806,  20.3069099 ,  15.44958552,  19.66862208,\n",
       "        13.8287025 ,  10.89375163,  11.27448951,  10.73025793,\n",
       "        26.98383906,  19.16657574,  24.00758163,  20.93625007,\n",
       "        17.76448685,  23.77808881,  16.79276727,   7.55693852,\n",
       "         9.83588432,   7.34924322,  21.95274226,   8.92060719,\n",
       "        20.35799474,  11.54823331,  21.67652124,  25.8399402 ,\n",
       "         8.81630354,   8.78561559,  20.76427506,   7.93335814,\n",
       "        27.72321947,  23.87378081,  14.29035765,  12.90776546,\n",
       "        20.19319063,  14.17741323,   5.8686659 ,   6.72287665,\n",
       "        17.57597865,  14.24346887,  16.46823732,  23.87378081,\n",
       "        22.47678865,  15.25855107,  16.25467645,  17.59062575,\n",
       "        12.84933736,  11.25223631,  15.98807534,  11.54823331,\n",
       "        12.9727698 ,  12.91500916,  15.25218786,  22.10094616,\n",
       "        24.27003541,  16.49054136,  11.70166591,   7.63048079,\n",
       "        16.14237562,  18.74576999,  17.10979352,  18.51244086,\n",
       "        17.53950486,  22.57251209,  14.2109842 ,  14.65574289,\n",
       "        11.14466276,  24.27003541,  18.58525551,  19.08740009,\n",
       "         7.63048079,  19.49393327,  10.96783005,  11.41678957,\n",
       "        13.03069806,  15.57562572,   6.83953   ,  17.81421044])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test_ID\n",
    "sub['SalePrice'] = ensemble\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['ID'] = test_ID\n",
    "sub['compositeHourlyWages'] = ensemble\n",
    "sub.to_csv('rs_hourly_submission_31Dec17_02_rs_predict_hourly_wage_v09_stacking.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=2017).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, train_target, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regression\n",
    "\n",
    "Reference\n",
    "- [Gradient Boosted Regression Trees](https://www.datarobot.com/blog/gradient-boosted-regression-trees/)\n",
    "- [Caifornia house price predictions with Gradient Boosted Regression Trees](https://shankarmsy.github.io/stories/gbrt-sklearn.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________\n",
      "Result of Gridsearch\n",
      "Best params:  {'learning_rate': 0.05, 'max_depth': 4, 'max_features': 0.1, 'min_samples_leaf': 17, 'n_estimators': 100}\n",
      "Best Estimator:  GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.05, loss='ls', max_depth=4, max_features=0.1,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=17,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)\n",
      "MSE:  0.336578545079\n",
      "______________________________\n",
      "vs Prediction\n",
      "RMSE from local train:  6.00336616982\n",
      "MSE from local train:  36.040405369\n",
      "R2 from local train:  0.39952132305\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# model_selection.GridSearchCV\n",
    "# =================================================\n",
    "# 1) Create model\n",
    "# Set params\n",
    "# Scores XXX\n",
    "est = GradientBoostingRegressor(n_estimators=3000)\n",
    "\n",
    "# 2) Set the grid\n",
    "param_grid = {'n_estimators':[100,1000,3000], \n",
    "              'learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
    "              'max_depth': [4, 6, 8],\n",
    "              'min_samples_leaf': [3, 5, 9, 17],\n",
    "#              'min_saples_split': [5, 10, 15],\n",
    "              'max_features': [1.0, 0.3, 0.1] ## not possible in our example (only 1 fx)\n",
    "              }\n",
    "# 3) Run GridSearch\n",
    "grid = GridSearchCV(est, param_grid, n_jobs=5).fit(X_train, y_train)\n",
    "\n",
    "# 4) Show best Params and Score\n",
    "print(\"_\"*30)\n",
    "print(\"Result of Gridsearch\")\n",
    "print(\"Best params: \", grid.best_params_)\n",
    "print(\"Best Estimator: \", grid.best_estimator_)\n",
    "print(\"MSE: \", grid.best_score_)\n",
    "\n",
    "# 5) Learning with best params\n",
    "gbm_g = GradientBoostingRegressor(**grid.best_params_)\n",
    "gbm_g.fit(X_train, y_train)\n",
    "y_pred_gs = gbm_g.predict(X_test)\n",
    "\n",
    "# 6) The error metric: RMSE\n",
    "print(\"_\"*30)\n",
    "print(\"vs Prediction\")\n",
    "print(\"RMSE from local train: \", rmse(y_test, y_pred_gs))\n",
    "print(\"MSE from local train: \", mean_squared_error(y_test, y_pred_gs))\n",
    "print(\"R2 from local train: \", r2_score(y_test, y_pred_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 648 candidates, totalling 3240 fits\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# model_selection.GridSearchCV\n",
    "# =================================================\n",
    "# 1) Create model\n",
    "mdl = lgb.LGBMRegressor(boosting_type= 'gbdt', \n",
    "                        n_jobs = 5,\n",
    "                        metric='RMSE'\n",
    "                       )\n",
    "\n",
    "\n",
    "#Best params:  {'colsample_bytree': 0.8, 'learning_rate': 0.05, \n",
    "#'max_depth': 3, 'n_estimators': 100, 'num_leaves': 5, \n",
    "#'objective': 'regression', 'reg_alpha': 1.2, 'reg_lambda': 1.4, 'subsample': 0.75}\n",
    "\n",
    "# 2) Set params for gridsearch\n",
    "gridParams = {\n",
    "    'objective': ['binary','regression'],\n",
    "    'num_leaves': [4,5,6], #2,10,20,100   \n",
    "    'learning_rate': [0.05, 0.06], # 0.005,\n",
    "    'n_estimators': [100], #8,24,\n",
    "    'colsample_bytree' :[0.8, 0.85, 0.9], #0.64,\n",
    "    'reg_lambda' : [1.3,1.4,1.5], #1,1.2,\n",
    "    'max_depth' :[2,3,4], #1,2,5,10\n",
    "    'subsample' :[0.7,0.75], \n",
    "    'reg_alpha' : [1.2], #0.1,0.51,\n",
    "#    'min_split_gain' :[],\n",
    "#    'subsample_for_bin' :[],\n",
    "#    'max_drop' :[], \n",
    "#    'gaussian_eta' :[], \n",
    "#    'drop_rate' :[],\n",
    "#    'silent' :[], \n",
    "#    'boosting_type' :['gbdt'], \n",
    "#    'min_child_weight' :[], \n",
    "#    'skip_drop' :[], \n",
    "#    'fair_c' :[], \n",
    "#    'seed' :[], \n",
    "#    'poisson_max_delta_step' :[], \n",
    "#    'subsample_freq' :[], \n",
    "#    'max_bin' :[],  #55\n",
    "#    'nthread' :[], \n",
    "#    'min_child_samples' :[], \n",
    "#    'huber_delta' :[], \n",
    "#    'use_missing' :[], \n",
    "#    'uniform_drop' :[], \n",
    "#    'bagging_fraction': [] #0.8,\n",
    "#    'bagging_freq': [] # 5\n",
    "#    'feature_fraction': [] # 0.2319,\n",
    "#    'feature_fraction_seed': [] #9\n",
    "#    'bagging_seed': [] #9,\n",
    "#    'min_data_in_leaf': [] #6\n",
    "#    'min_sum_hessian_in_leaf': [] # 11                              \n",
    "#    'xgboost_dart_mode' :[]\n",
    "}\n",
    "\n",
    "# 3) Run GridSearch\n",
    "grid = GridSearchCV(mdl, gridParams, verbose=1, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# 4) Show best Params and Score\n",
    "print(\"_\"*30)\n",
    "print(\"Result of Gridsearch\")\n",
    "print(\"Best params: \", grid.best_params_)\n",
    "print(\"Best Estimator: \", grid.best_estimator_)\n",
    "print(\"MSE: \", grid.best_score_)\n",
    "\n",
    "# 5) Learning with best params\n",
    "lgm_g = lgb.LGBMRegressor(**grid.best_params_)\n",
    "lgm_g.fit(X_train, y_train)\n",
    "y_pred_gs = lgm_g.predict(X_test)\n",
    "\n",
    "# 6) The error metric: RMSE\n",
    "print(\"_\"*30)\n",
    "print(\"vs Prediction\")\n",
    "print(\"RMSE from local train: \", rmse(y_test, y_pred_gs))\n",
    "print(\"MSE from local train: \", mean_squared_error(y_test, y_pred_gs))\n",
    "print(\"R2 from local train: \", r2_score(y_test, y_pred_gs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Score: __\n",
    "```\n",
    "______________________________\n",
    "Result of Gridsearch\n",
    "Best params:  {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'num_leaves': 5, 'objective': 'regression', 'reg_alpha': 1.2, 'reg_lambda': 1.4, 'subsample': 0.75}\n",
    "Best Estimator:  LGBMRegressor(boosting_type='gbdt', colsample_bytree=0.8, learning_rate=0.05,\n",
    "       max_bin=255, max_depth=3, metric='RMSE', min_child_samples=20,\n",
    "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
    "       n_jobs=5, num_leaves=5, objective='regression', random_state=None,\n",
    "       reg_alpha=1.2, reg_lambda=1.4, silent=True, subsample=0.75,\n",
    "       subsample_for_bin=200000, subsample_freq=1)\n",
    "MSE:  0.349386487223\n",
    "______________________________\n",
    "vs Prediction\n",
    "RMSE from local train:  6.02327567741\n",
    "MSE from local train:  36.2798498861\n",
    "R2 from local train:  0.395531874947\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________\n",
      "Result of Gridsearch\n",
      "Best params:  {'alpha': 0.1, 'coef0': 20, 'degree': 2, 'gamma': 100.0, 'kernel': 'polynomial'}\n",
      "Best Estimator:  KernelRidge(alpha=0.1, coef0=20, degree=2, gamma=100.0, kernel='polynomial',\n",
      "      kernel_params=None)\n",
      "MSE:  -38.9194812638\n",
      "______________________________\n",
      "vs Prediction\n",
      "RMSE from local train:  5.9962579498\n",
      "MSE from local train:  35.9551094006\n",
      "R2 from local train:  0.400942461623\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# model_selection.GridSearchCV\n",
    "# =================================================\n",
    "# 1) Create model\n",
    "model = KernelRidge()\n",
    "\n",
    "# 2) Set params for gridsearch\n",
    "param_grid = {\n",
    "    \"alpha\": [1e0, 0.1, 1e-2, 1e-3],\n",
    "    \"gamma\": np.logspace(-2, 2, 5),\n",
    "    \"kernel\" : ['polynomial','rbf'],\n",
    "    \"degree\" : [2,5,10,20], \n",
    "    \"coef0\" : [2.5,5,10,20],\n",
    "}\n",
    "\n",
    "# 3) Run GridSearch\n",
    "model_ = GridSearchCV(estimator= model, param_grid= param_grid, scoring='neg_mean_squared_error',cv=5, n_jobs=-1)\n",
    "model_.fit(X_train, y_train)\n",
    "\n",
    "# 4) Show best Params and Score\n",
    "print(\"_\"*30)\n",
    "print(\"Result of Gridsearch\")\n",
    "print(\"Best params: \", model_.best_params_)\n",
    "print(\"Best Estimator: \", model_.best_estimator_)\n",
    "print(\"MSE: \", model_.best_score_)\n",
    "\n",
    "# 5) Learning with best params\n",
    "krr_g = KernelRidge(**model_.best_params_)\n",
    "krr_g.fit(X_train, y_train)\n",
    "y_pred_gs = krr_g.predict(X_test)\n",
    "\n",
    "# 6) The error metric: RMSE\n",
    "print(\"_\"*30)\n",
    "print(\"vs Prediction\")\n",
    "print(\"RMSE from local train: \", rmse(y_test, y_pred_gs))\n",
    "print(\"MSE from local train: \", mean_squared_error(y_test, y_pred_gs))\n",
    "print(\"R2 from local train: \", r2_score(y_test, y_pred_gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array([  1.00000000e-02,   1.00000000e-01,   1.00000000e+00,\n",
    "#         1.00000000e+01,   1.00000000e+02])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referring to followings\n",
    "- [Exploring features and regression models](https://www.kaggle.com/youssefer/xgb-and-lasso-regression)\n",
    "- [House Prices # Regression and Bagging techniques](https://www.kaggle.com/aarti1/house-prices-regression-and-bagging-techniques)\n",
    "- [XGB and Lasso Regression](https://www.kaggle.com/youssefer/xgb-and-lasso-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\tLasso regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11edafcf8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nudzNzCqQy93MrAK53M3MKtD/ByZxOamgmNAnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112c31d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#------------------------------------------------------------\n",
    "print (\"10\tLasso regression\")\n",
    "# importance of train set size: first, we set a relevant alpha\n",
    "\n",
    "rm_tr=[]\n",
    "rm_te=[]\n",
    "\n",
    "opti=[]\n",
    "alphas=[1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,1]\n",
    "# alphas=np.linspace(1e-4,1e-2,20)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     feat, price, test_size=0.8, random_state=42)\n",
    "\n",
    "for al in alphas:\n",
    "\tls=Lasso(alpha=al, copy_X=True, fit_intercept=True, max_iter=5000,\n",
    "\t   normalize=False, positive=False, precompute=False, random_state=111,\n",
    "\t   selection='cyclic', tol=0.0001, warm_start=False)\n",
    "\tls.fit(X_train,y_train)\n",
    "\trm_tr.append(np.sqrt(mean_squared_error(y_train,ls.predict(X_train))))\n",
    "\trm_te.append(np.sqrt(mean_squared_error(y_test,ls.predict(X_test))))\n",
    "\n",
    "plt.figure()\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.plot(np.log(alphas),rm_tr,np.log(alphas),rm_te,\"r\")\t\n",
    "plt.title(\"Train and test error vs log(alphas)\")\n",
    "#plt.savefig('fig4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________\n",
      "Result of Gridsearch\n",
      "Best params:  {'alpha': 1e-05}\n",
      "Best Estimator:  Lasso(alpha=1e-05, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "MSE:  -42.9326318668\n",
      "______________________________\n",
      "vs Prediction\n",
      "RMSE from local train:  6.02475299555\n",
      "MSE from local train:  36.2976486573\n",
      "R2 from local train:  0.395235324935\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# model_selection.GridSearchCV\n",
    "# =================================================\n",
    "# 1) Create model\n",
    "model = Lasso()\n",
    "\n",
    "# 2) Set params for gridsearch\n",
    "param_grid = { 'alpha': [i/100000 for i in range(1,50000)]}\n",
    "\n",
    "# 3) Run GridSearch\n",
    "model_ = GridSearchCV(estimator= model, param_grid= param_grid, scoring='neg_mean_squared_error',cv=5, n_jobs=-1)\n",
    "model_.fit(X_train, y_train)\n",
    "\n",
    "# 4) Show best Params and Score\n",
    "print(\"_\"*30)\n",
    "print(\"Result of Gridsearch\")\n",
    "print(\"Best params: \", model_.best_params_)\n",
    "print(\"Best Estimator: \", model_.best_estimator_)\n",
    "print(\"MSE: \", model_.best_score_)\n",
    "\n",
    "# 5) Learning with best params\n",
    "las_g = xgb.XGBRegressor(**model_.best_params_)\n",
    "las_g.fit(X_train, y_train)\n",
    "y_pred_gs = las_g.predict(X_test)\n",
    "\n",
    "# 6) The error metric: RMSE\n",
    "print(\"_\"*30)\n",
    "print(\"vs Prediction\")\n",
    "print(\"RMSE from local train: \", rmse(y_test, y_pred_gs))\n",
    "print(\"MSE from local train: \", mean_squared_error(y_test, y_pred_gs))\n",
    "print(\"R2 from local train: \", r2_score(y_test, y_pred_gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute alpha = 1e-05 - 6.4987767707721575\n",
      "Compute alpha = 1.6681005372000593e-05 - 6.498776706041236\n",
      "Compute alpha = 2.782559402207126e-05 - 6.498776727807145\n",
      "Compute alpha = 4.641588833612782e-05 - 6.498777125134102\n",
      "Compute alpha = 7.742636826811278e-05 - 6.498778792472601\n",
      "Compute alpha = 0.0001291549665014884 - 6.498784368998028\n",
      "Compute alpha = 0.00021544346900318823 - 6.498801449091047\n",
      "Compute alpha = 0.00035938136638046257 - 6.498851582625532\n",
      "Compute alpha = 0.0005994842503189409 - 6.498995429448667\n",
      "Compute alpha = 0.001 - 6.499402929830128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x1139ea710>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASIAAADXCAYAAAC+llyoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1129c8780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =================================================\n",
    "# Manual GridSearch\n",
    "# =================================================\n",
    "# Search `alpha` regression parameters\n",
    "#x_train = x[x.train_test == 1]\n",
    "lasso = Lasso(max_iter=1e2, normalize=True)\n",
    "alphas = np.logspace(-5, -3, 10)\n",
    "scores = []\n",
    "scores_std = []\n",
    "for alpha in alphas:\n",
    "    lasso.alpha = alpha\n",
    "    this_scores = np.sqrt(-cross_val_score(lasso, train, train_target, cv=5, scoring='mean_squared_error'))\n",
    "    print('Compute alpha = {} - {}'.format(alpha, np.mean(this_scores)))\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.semilogx(alphas, scores)\n",
    "plt.semilogx(alphas, np.array(scores) + np.array(scores_std) / np.sqrt(len(train)), 'b--')\n",
    "plt.semilogx(alphas, np.array(scores) - np.array(scores_std) / np.sqrt(len(train)), 'b--')\n",
    "plt.ylabel('CV score')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "#plt.savefig('lasso_lars.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0649678226388\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "enr_cv = ElasticNetCV(cv=5, random_state=2017)\n",
    "enr_cv.fit(X_train, y_train)\n",
    "print(enr_cv.alpha_)\n",
    "print(enr_cv.l1_ratio_)\n",
    "#print(enr_cv.intercept_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________\n",
      "Result of Gridsearch\n",
      "Best params:  {'alpha': 0.064, 'l1_ratio': 0.9}\n",
      "Best Estimator:  ElasticNet(alpha=0.064, copy_X=True, fit_intercept=True, l1_ratio=0.9,\n",
      "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "MSE:  -42.9581056118\n",
      "______________________________\n",
      "vs Prediction\n",
      "RMSE from local train:  6.2730675813\n",
      "MSE from local train:  39.3513768796\n",
      "R2 from local train:  0.344356355515\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# model_selection.GridSearchCV\n",
    "# =================================================\n",
    "# 1) Create model\n",
    "regr = ElasticNet()\n",
    "\n",
    "# 2) Set params for gridsearch\n",
    "enr_params = {\n",
    "    'alpha' : [0.0640, 0.0645, 0.0649678226388, 0.65],\n",
    "    'l1_ratio' : [0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "}\n",
    "\n",
    "# 3) Run gridsearch\n",
    "grid_enr = GridSearchCV(regr,enr_params,scoring='neg_mean_squared_error',cv=5,n_jobs=-1)\n",
    "grid_enr.fit(X_train, y_train)\n",
    "\n",
    "# 4) Show best params and score\n",
    "print(\"_\"*30)\n",
    "print(\"Result of Gridsearch\")\n",
    "print(\"Best params: \", grid_enr.best_params_)\n",
    "print(\"Best Estimator: \", grid_enr.best_estimator_)\n",
    "print(\"MSE: \", grid_enr.best_score_)\n",
    "\n",
    "# 5) Learning with best params\n",
    "eln_g = ElasticNet(**grid_enr.best_params_)\n",
    "eln_g.fit(X_train, y_train)\n",
    "y_pred_gs = eln_g.predict(X_test)\n",
    "\n",
    "# 6) The error metric: RMSE\n",
    "print(\"_\"*30)\n",
    "print(\"vs Prediction\")\n",
    "print(\"RMSE from local train: \", rmse(y_test, y_pred_gs))\n",
    "print(\"MSE from local train: \", mean_squared_error(y_test, y_pred_gs))\n",
    "print(\"R2 from local train: \", r2_score(y_test, y_pred_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 2, 'min_child_weight': 14, 'n_estimators': 100, 'objective': 'reg:gamma', 'subsample': 0.8}\n",
      "Best Estimator:  XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1.0, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=2, min_child_weight=14, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='reg:gamma', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.8)\n",
      "MSE:  -38.7922204065\n",
      "RMSE from local train:  5.99283617268\n",
      "MSE from local train:  35.9140853925\n",
      "R2 from local train:  0.401625973415\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# model_selection.GridSearchCV\n",
    "# =================================================\n",
    "# 1) xgboostモデルの作成\n",
    "reg = xgb.XGBRegressor()\n",
    "\n",
    "# 2) XGBoost params\n",
    "xgb_params = {\n",
    "    'objective' : ['reg:gamma','reg:linear'],\n",
    "    'learning_rate' : [0.05,0.75,0.1,0.125],\n",
    "    'n_estimators' : [50,100,200],\n",
    "    'max_depth' : [2,4,6],\n",
    "    'subsample' : [0.79,0.8,0.81,0.85],\n",
    "    'colsample_bytree' : [0.9,1.0],\n",
    "    'min_child_weight' : [13,14, 15, 16]\n",
    "}\n",
    "\n",
    "# 3) Run GridSearch\n",
    "grid_xgb = GridSearchCV(reg,xgb_params,scoring='neg_mean_squared_error',cv=5,n_jobs=-1)\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "# 4) Show best Params and Score\n",
    "print(\"_\"*30)\n",
    "print(\"Result of Gridsearch\")\n",
    "print(\"Best params: \", grid_xgb.best_params_)\n",
    "print(\"Best Estimator: \", grid_xgb.best_estimator_)\n",
    "print(\"MSE: \", grid_xgb.best_score_)\n",
    "\n",
    "# 5) Learning with best params\n",
    "xgr_g = xgb.XGBRegressor(**grid_xgb.best_params_)\n",
    "xgr_g.fit(X_train, y_train)\n",
    "y_pred_gs = xgr_g.predict(X_test)\n",
    "\n",
    "# 6) The error metric: RMSE\n",
    "print(\"_\"*30)\n",
    "print(\"vs Prediction\")\n",
    "print(\"RMSE from local train: \", rmse(y_test, y_pred_gs))\n",
    "print(\"MSE from local train: \", mean_squared_error(y_test, y_pred_gs))\n",
    "print(\"R2 from local train: \", r2_score(y_test, y_pred_gs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
