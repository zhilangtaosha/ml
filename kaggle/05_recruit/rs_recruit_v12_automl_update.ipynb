{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test H2O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>43 mins 49 secs</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.16.0.4</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>11 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_suzukiry_vzadrg</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>2.387 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.2 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         43 mins 49 secs\n",
       "H2O cluster version:        3.16.0.4\n",
       "H2O cluster version age:    11 days\n",
       "H2O cluster name:           H2O_from_python_suzukiry_vzadrg\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    2.387 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.2 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing done!\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Contributions from:\n",
    "DSEverything - Mean Mix - Math, Geo, Harmonic (LB 0.493) \n",
    "https://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493\n",
    "JdPaletto - Surprised Yet? - Part2 - (LB: 0.503)\n",
    "https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503\n",
    "hklee - weighted mean comparisons, LB 0.497, 1ST\n",
    "https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st\n",
    "\n",
    "Also all comments for changes, encouragement, and forked scripts rock\n",
    "\n",
    "Keep the Surprise Going\n",
    "\"\"\"\n",
    "\n",
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "h2o.init()\n",
    "\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../../mltestdata/05_recruit/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "    }\n",
    "\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "# data['tra']['dom'] = data['tra']['visit_date'].dt.day\n",
    "data['tra']['doy'] = data['tra']['visit_date'].dt.dayofyear\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['week'] = data['tra']['visit_date'].dt.week\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "# data['tes']['dom'] = data['tes']['visit_date'].dt.day\n",
    "data['tes']['doy'] = data['tes']['visit_date'].dt.dayofyear\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['week'] = data['tes']['visit_date'].dt.week\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "#sure it can be compressed...\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "# tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "# stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(4):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name' +str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date'])\n",
    "\n",
    "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n",
    "\n",
    "# NEW FEATURES FROM JMBULL\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "\n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "train = train.fillna(-999)\n",
    "test = test.fillna(-999)\n",
    "\n",
    "train['visitors'] = np.log1p(train['visitors'].values)\n",
    "\n",
    "print('Pre-processing done!')\n",
    "\n",
    "htrain = h2o.H2OFrame(train)\n",
    "htest = h2o.H2OFrame(test)\n",
    "\n",
    "htrain.drop(['id', 'air_store_id', 'visit_date'])\n",
    "htest.drop(['id', 'air_store_id', 'visit_date'])\n",
    "\n",
    "x =htrain.columns\n",
    "y ='visitors'\n",
    "x.remove(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "def RMSLE(y_, pred):\n",
    "    return metrics.mean_squared_error(y_, pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting h2o autoML model!\n",
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "print('Starting h2o autoML model!')  \n",
    "\n",
    "#aml = H2OAutoML(max_runtime_secs = 8000)\n",
    "aml = H2OAutoML(max_runtime_secs = 3350)\n",
    "aml.train(x=x, y=y, training_frame=htrain, leaderboard_frame = htest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate predictions...\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "RMSLE H2O automl leader:  24.3894775844\n"
     ]
    }
   ],
   "source": [
    "print('Generate predictions...')\n",
    "htrain.drop(['visitors'])\n",
    "preds = aml.leader.predict(htrain)\n",
    "preds = preds.as_data_frame()\n",
    "print('RMSLE H2O automl leader: ', RMSLE(train_automl['visitors'].values, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "preds = aml.leader.predict(htest)\n",
    "preds = preds.as_data_frame()\n",
    "\n",
    "test['visitors'] = preds\n",
    "test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\n",
    "sub1 = test[['id','visitors']].copy()\n",
    "del train; del data; del htrain; del htest;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaderboard :  "
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                             </th><th style=\"text-align: right;\">  mean_residual_deviance</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">    mae</th><th style=\"text-align: right;\">    rmsle</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>StackedEnsemble_AllModels_0_AutoML_20180127_171601   </td><td style=\"text-align: right;\">                 7.66361</td><td style=\"text-align: right;\">2.76832</td><td style=\"text-align: right;\">2.6608 </td><td style=\"text-align: right;\">  1.29405</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180127_171601_model_2            </td><td style=\"text-align: right;\">                 7.66879</td><td style=\"text-align: right;\">2.76926</td><td style=\"text-align: right;\">2.66921</td><td style=\"text-align: right;\">  1.29592</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180127_171601_model_1            </td><td style=\"text-align: right;\">                 7.67258</td><td style=\"text-align: right;\">2.76994</td><td style=\"text-align: right;\">2.66953</td><td style=\"text-align: right;\">  1.29599</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180127_171601_model_0            </td><td style=\"text-align: right;\">                 7.67368</td><td style=\"text-align: right;\">2.77014</td><td style=\"text-align: right;\">2.66904</td><td style=\"text-align: right;\">  1.29594</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_0_AutoML_20180127_171601</td><td style=\"text-align: right;\">                 7.68574</td><td style=\"text-align: right;\">2.77232</td><td style=\"text-align: right;\">2.67007</td><td style=\"text-align: right;\">nan      </td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180127_171601_model_10           </td><td style=\"text-align: right;\">                 7.70084</td><td style=\"text-align: right;\">2.77504</td><td style=\"text-align: right;\">2.76247</td><td style=\"text-align: right;\">  1.32448</td></tr>\n",
       "<tr><td>DRF_0_AutoML_20180127_171601                         </td><td style=\"text-align: right;\">                 7.71299</td><td style=\"text-align: right;\">2.77723</td><td style=\"text-align: right;\">2.67763</td><td style=\"text-align: right;\">  1.2982 </td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180127_171601_model_3            </td><td style=\"text-align: right;\">                 7.72759</td><td style=\"text-align: right;\">2.77985</td><td style=\"text-align: right;\">2.68737</td><td style=\"text-align: right;\">  1.30108</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180127_171601_model_4            </td><td style=\"text-align: right;\">                 7.72809</td><td style=\"text-align: right;\">2.77994</td><td style=\"text-align: right;\">2.69803</td><td style=\"text-align: right;\">  1.3043 </td></tr>\n",
       "<tr><td>XRT_0_AutoML_20180127_171601                         </td><td style=\"text-align: right;\">                 7.7529 </td><td style=\"text-align: right;\">2.7844 </td><td style=\"text-align: right;\">2.69015</td><td style=\"text-align: right;\">  1.3018 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " H2O automl leader performace :  Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_0_AutoML_20180127_171601\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsRegressionGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.196135394689592\n",
      "RMSE: 0.44287175874014817\n",
      "MAE: 0.3248263679715371\n",
      "RMSLE: 0.13953665477829305\n",
      "R^2: 0.6993486552762503\n",
      "Mean Residual Deviance: 0.196135394689592\n",
      "Null degrees of freedom: 201535\n",
      "Residual degrees of freedom: 201529\n",
      "Null deviance: 131475.68969122303\n",
      "Residual deviance: 39528.34290416162\n",
      "AIC: 243658.31134907017\n",
      "\n",
      "ModelMetricsRegressionGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.22746901607569878\n",
      "RMSE: 0.4769371196244834\n",
      "MAE: 0.35049623829551546\n",
      "RMSLE: 0.14906579281667567\n",
      "R^2: 0.6514497531278864\n",
      "Mean Residual Deviance: 0.22746901607569878\n",
      "Null degrees of freedom: 50571\n",
      "Residual degrees of freedom: 50565\n",
      "Null deviance: 33004.18690112252\n",
      "Residual deviance: 11503.563080980239\n",
      "AIC: 68649.07286448657\n",
      "\n",
      "ModelMetricsRegressionGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.233583251279126\n",
      "RMSE: 0.48330451195817115\n",
      "MAE: 0.3538499512129127\n",
      "RMSLE: 0.1510221886341565\n",
      "R^2: 0.641945714524449\n",
      "Mean Residual Deviance: 0.233583251279126\n",
      "Null degrees of freedom: 201535\n",
      "Residual degrees of freedom: 201530\n",
      "Null deviance: 131477.11138260184\n",
      "Residual deviance: 47075.43412978994\n",
      "AIC: 278871.3697823143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../../../mltestdata/05_recruit/*.csv')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n",
    "    on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')\n",
    "\n",
    "sub_merge['visitors'] = 0.5*sub_merge['visitors_x'] + 0.5*sub_merge['visitors_y']*1.1\n",
    "sub_merge[['id', 'visitors']].to_csv('submission_automl_test.csv', index=False)\n",
    "\n",
    "print('Leaderboard : ', aml.leaderboard)\n",
    "\n",
    "print(' H2O automl leader performace : ', aml.leader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
