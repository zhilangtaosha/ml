{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "e268f1a6-7850-4594-adf7-8dcf173b99fb",
    "_uuid": "9998b6e3d6028323220cb8b7fbdd3a45eb0b0c32",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_type.csv\n",
      "log_feature.csv\n",
      "resource_type.csv\n",
      "sample_submission.csv\n",
      "severity_type.csv\n",
      "test.csv\n",
      "train.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# the bayesian optimisation library throws a lot of warning message, so for readability we disable warning in this notebook.\n",
    "# *NOT* encouraged if you want to find out what is going on under the cover :) \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../../../mltestdata/07_telstra/\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8c635922-747a-4180-851f-07b663ff2bef",
    "_uuid": "1d795424d24af356bc7e1064c77f423adef81c44"
   },
   "source": [
    "# The case against gridsearch\n",
    "\n",
    "We haven't touched on hyper-parameters tuning in this seris of notebook. In many of the examples you found on the internet, you will see people using gridsearch techniques to identify as set of parameters for a given machine learning algorithms. This would make sense if you are looking a small set of paraemters.\n",
    "\n",
    "However, when the number of hyper-parameters are larger - for instance there are at least 10 parameters from XGB which people often used, the time cost associate with exhaustive gridsearch quickly become prohibitive.  You can refer to this article to see a concise arguement on [*why gridsearch is plain stupid*](https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881)\n",
    "\n",
    "In this notebook, we have a look an alternative technique to perform hyper-parameters tuning using Bayesian Optimisation. More technical details about this approach can be seen [here](https://github.com/fmfn/BayesianOptimization). Here we focus on the illustration on how we can apply this technique on the Telstra dataset.\n",
    "\n",
    "First of all, let's replicate the whole pipeline from data loading to some simple feature engineering:\n",
    "\n",
    "# Data loading and reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "db2977b0-b62e-4cfa-8207-b06117a8b22c",
    "_uuid": "87b9994fae744e32c2e5975ec71135e24ec6c787",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_to_num(string):\n",
    "    return int(string.split(\" \")[1])\n",
    "\n",
    "train=pd.read_csv('../../../mltestdata/07_telstra/train.csv', converters={'location':str_to_num})\n",
    "test=pd.read_csv('../../../mltestdata/07_telstra/test.csv', converters={'location':str_to_num})\n",
    "event=pd.read_csv('../../../mltestdata/07_telstra/event_type.csv', converters={'event_type':str_to_num})\n",
    "log_feature=pd.read_csv('../../../mltestdata/07_telstra/log_feature.csv', converters={'log_feature':str_to_num})\n",
    "severity=pd.read_csv('../../../mltestdata/07_telstra/severity_type.csv', converters={'severity_type':str_to_num})\n",
    "resource=pd.read_csv('../../../mltestdata/07_telstra/resource_type.csv', converters={'resource_type':str_to_num})\n",
    "\n",
    "sample=pd.read_csv('../../../mltestdata/07_telstra/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "a3efdcc5-039e-49f8-8f36-8ef59af1a8dd",
    "_uuid": "973a0d048c9be052fe9c258d9139e2b2ccaa2755",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge train and test set for now\n",
    "traintest=train.append(test)\n",
    "\n",
    "# create resource one-hot data per id\n",
    "resource_by_id=pd.get_dummies(resource,columns=['resource_type'])\n",
    "resource_by_id=resource_by_id.groupby(['id']).sum().reset_index(drop=False)\n",
    "\n",
    "# create event one-hot data per id\n",
    "event_by_id=pd.get_dummies(event,columns=['event_type'])\n",
    "event_by_id=event_by_id.groupby(['id']).sum().reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "6e324a08-f6b4-4690-97da-90002cd61994",
    "_uuid": "35f890f9f1d2403f24268afa6c30142130c0ee7d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_feature_dict={}\n",
    "\n",
    "for row in log_feature.itertuples():\n",
    "    if row.id not in log_feature_dict:\n",
    "        log_feature_dict[row.id]={}\n",
    "    if row.log_feature not in log_feature_dict[row.id]:\n",
    "        log_feature_dict[row.id][row.log_feature]=row.volume\n",
    "\n",
    "colnames=['id']\n",
    "for i in range(1,387):\n",
    "    colnames.append('log_feature_'+str(i))\n",
    "\n",
    "log_feature_by_id_np=np.zeros((18552,387))\n",
    "count=0\n",
    "for key, feature_dict in log_feature_dict.items():\n",
    "    log_feature_by_id_np[count, 0]=np.int(key)\n",
    "    for feature, volume in feature_dict.items():\n",
    "        log_feature_by_id_np[count, feature]=np.int(volume)\n",
    "    count+=1\n",
    "log_feature_by_id=pd.DataFrame(data=log_feature_by_id_np, columns=colnames, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "d12fa878-e862-49fc-948d-8888a2fb6b38",
    "_uuid": "859931f003af2f020110f465087e523aea9230bc",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18552, 4)\n",
      "(18552, 14)\n",
      "(18552, 67)\n",
      "(18552, 453)\n"
     ]
    }
   ],
   "source": [
    "# Merge datasets together for ml input dataframe\n",
    "\n",
    "traintest=traintest.merge(right=severity, on='id')\n",
    "print(traintest.shape)\n",
    "\n",
    "traintest=traintest.merge(right=resource_by_id, on='id')\n",
    "print(traintest.shape)\n",
    "\n",
    "traintest=traintest.merge(right=event_by_id, on='id')\n",
    "print(traintest.shape)\n",
    "\n",
    "traintest=traintest.merge(right=log_feature_by_id, on='id')\n",
    "print(traintest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "28d10632-34d4-42b7-9fd5-1391386888d6",
    "_uuid": "60df6eaed2fc21cd6239f09963d412668586a09e",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input shape is (7381, 453)\n",
      "test_input shape is (11171, 453)\n"
     ]
    }
   ],
   "source": [
    "# Seperate the traintest dataframe into train and test input dataframes\n",
    "train_input=traintest.loc[0:train.shape[0]-1].copy()\n",
    "print(\"train_input shape is\", train_input.shape)\n",
    "\n",
    "test_input=traintest.loc[train.shape[0]::].copy()\n",
    "print(\"test_input shape is\", test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "a582b1eb-a5c6-4b06-9490-571accf8ce69",
    "_uuid": "fb8dc264508b129bf7e83185db41bd6201a7be07",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=train_input.fault_severity\n",
    "train_input.drop(['fault_severity'], axis=1, inplace=True)\n",
    "test_input.drop(['fault_severity'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fbd81fd8-9ad3-4813-906a-bc604da442a2",
    "_uuid": "a2b59d7f51b38a47dafb28c93eda8004857195bd"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "cf1b5ffe-5a03-40c9-80db-ed0c3f9ada33",
    "_uuid": "45bfb3ff9a5b72838f4c89371767f877365b4b24",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sum feature for resource and event\n",
    "resource_cols=train_input.columns[train_input.columns.str.find('resource')==0].tolist()\n",
    "event_cols=train_input.columns[train_input.columns.str.find('event')==0].tolist()\n",
    "\n",
    "train_input['resource_sum']=train_input[resource_cols].sum(axis=1)\n",
    "train_input['event_sum']=train_input[event_cols].sum(axis=1)\n",
    "\n",
    "test_input['resource_sum']=test_input[resource_cols].sum(axis=1)\n",
    "test_input['event_sum']=test_input[event_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "55987dda-80b9-48d5-82e3-9407ef1b741b",
    "_uuid": "2b4df6c832bbac078a196a3066c55305fb32e9ff",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frequency feature for location\n",
    "\n",
    "# create a dataframe with with the location and the location frequency as features\n",
    "location_frequency=traintest.location.value_counts()\n",
    "location_frequency.name='location_frequency'\n",
    "location_frequency=pd.DataFrame(location_frequency).reset_index()\n",
    "location_frequency.rename(columns={'index':'location'}, inplace=True)\n",
    "\n",
    "# merge this location frequency dataframe with the training and testing ML input data on location\n",
    "train_input=train_input.merge(right=location_frequency, on='location', how='left')\n",
    "test_input=test_input.merge(right=location_frequency, on='location', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "2eb64877-ee36-4b02-8a2d-34a18555d412",
    "_uuid": "d4b4cd27ce2a594e28b6cf737c745091b2a5330b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pattern feature for log feature\n",
    "log_feature_cols=traintest.columns[traintest.columns.str.find('log_feature')==0].tolist()\n",
    "traintest_log_feature=traintest[log_feature_cols].copy()\n",
    "\n",
    "mask=(traintest_log_feature>0)\n",
    "traintest_log_feature.where(mask, other=0, inplace=True)\n",
    "\n",
    "mask=(traintest_log_feature<1)\n",
    "traintest_log_feature.where(mask, other=1, inplace=True)\n",
    "\n",
    "traintest_log_feature['log_feature_pattern_raw']= traintest_log_feature.apply(lambda x: ''.join(x.astype(str)), axis=1)\n",
    "\n",
    "\n",
    "log_feature_pattern_df=pd.DataFrame(traintest_log_feature.log_feature_pattern_raw.drop_duplicates())\n",
    "log_feature_pattern_df.reset_index(inplace=True)\n",
    "log_feature_pattern_df.rename(columns={'index':'log_feature_pattern_id'}, inplace=True)\n",
    "\n",
    "# merge log_feature_pattern_df back to traintest_log_feature on log_feature_pattern_raw\n",
    "traintest_log_feature=traintest_log_feature.merge(right=log_feature_pattern_df, on='log_feature_pattern_raw', how='left')\n",
    "\n",
    "# finally insert the log_feature_pattern_id column into input dataframes as new feature\n",
    "train_input['log_feature_pattern_id']=traintest_log_feature.loc[0:train.shape[0]-1, 'log_feature_pattern_id'].values\n",
    "test_input['log_feature_pattern_id']=traintest_log_feature.loc[train.shape[0]::]['log_feature_pattern_id'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "27a5041f-a7f1-4009-ad00-64b4c1bbe058",
    "_uuid": "153d94c099816a73f031ca98bf7e3a05f445a812",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove id column\n",
    "train_input.drop(['id'], axis=1, inplace=True)\n",
    "test_input.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "train_input_fe1=train_input.copy()\n",
    "test_input_fe1=test_input.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b906e1de-ff8f-43b1-843c-b4f5a273fb1f",
    "_uuid": "5dc9f0cbafe6fe7704752d4f557b2b5201fed9f0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pattern feature for resource\n",
    "resource_cols=traintest.columns[traintest.columns.str.find('resource')==0].tolist()\n",
    "traintest_resource=traintest[resource_cols].copy()\n",
    "\n",
    "traintest_resource['resource_pattern_raw']= traintest_resource.apply(lambda x: ''.join(x.astype(str)), axis=1)\n",
    "\n",
    "\n",
    "resource_pattern_df=pd.DataFrame(traintest_resource.resource_pattern_raw.drop_duplicates())\n",
    "resource_pattern_df.reset_index(inplace=True)\n",
    "resource_pattern_df.rename(columns={'index':'resource_pattern_id'}, inplace=True)\n",
    "\n",
    "# merge resource_pattern_df back to traintest_resource on resource_pattern_raw\n",
    "traintest_resource=traintest_resource.merge(right=resource_pattern_df, on='resource_pattern_raw', how='left')\n",
    "\n",
    "# finally insert the resource_pattern_id column into input dataframes as new feature\n",
    "train_input['resource_pattern_id']=traintest_resource.loc[0:train.shape[0]-1, 'resource_pattern_id'].values\n",
    "test_input['resource_pattern_id']=traintest_resource.loc[train.shape[0]::]['resource_pattern_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "e10dc523-c07b-4255-bfff-92f43750a520",
    "_uuid": "1175fa18df65f28df58dbcf5f85b8ad15900d39d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# log feature indicator sum\n",
    "# improve private LB, decrease public LB \n",
    "traintest_log_feature['log_feat_ind_sum']=traintest_log_feature[log_feature_cols].sum(axis=1)\n",
    "train_input['log_feat_ind_sum']=traintest_log_feature.loc[0:train.shape[0]-1, 'log_feat_ind_sum'].values\n",
    "test_input['log_feat_ind_sum']=traintest_log_feature.loc[train.shape[0]::]['log_feat_ind_sum'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "b4e9619c-add4-492f-8404-35316fe8c857",
    "_uuid": "06ba96b20468d5f9a860d9cf16afdbea35ec4766",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traintest['volsum']=traintest[log_feature_cols].sum(axis=1)\n",
    "train_input['volsum']=traintest.loc[0:train.shape[0]-1, 'volsum'].values\n",
    "test_input['volsum']=traintest.loc[train.shape[0]::]['volsum'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "5a7ad009-ae9a-4978-8bab-2ca1844d8e5b",
    "_uuid": "6eeb03ffba7cf29ff92bf169f220a6ceea93f036",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traintest_input=train_input.append(test_input)\n",
    "\n",
    "# create a dataframe with with the logfeat_pat_freq and the logfeat_pat_freq frequency as features\n",
    "logfeat_pat_freq=traintest_input.log_feature_pattern_id.value_counts()\n",
    "logfeat_pat_freq.name='logfeat_pat_freq'\n",
    "logfeat_pat_freq=pd.DataFrame(logfeat_pat_freq).reset_index()\n",
    "logfeat_pat_freq.rename(columns={'index':'log_feature_pattern_id'}, inplace=True)\n",
    "\n",
    "# merge this logfeat_pat_freq frequency dataframe with the training and testing ML input data on logfeat_pat_freq\n",
    "\n",
    "traintest_input=traintest_input.merge(right=logfeat_pat_freq, on='log_feature_pattern_id', how='left')\n",
    "\n",
    "train_input=traintest_input.loc[0:train.shape[0]-1].copy()\n",
    "test_input=traintest_input.loc[train.shape[0]::].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b4cd8aae-314d-4811-b9a4-46ae980ba0b2",
    "_uuid": "1575346ff3586771f34c4b4d102ece9fba9e96db",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "severity_frequency=traintest.severity_type.value_counts()\n",
    "severity_frequency.name='severity_frequency'\n",
    "severity_frequency=pd.DataFrame(severity_frequency).reset_index()\n",
    "severity_frequency.rename(columns={'index':'severity_type'}, inplace=True)\n",
    "\n",
    "# merge this severity frequency dataframe with the training and testing ML input data on severity\n",
    "train_input=train_input.merge(right=severity_frequency, on='severity_type', how='left')\n",
    "test_input=test_input.merge(right=severity_frequency, on='severity_type', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a3ce10e9-8cba-473d-8a5e-40148d976dd6",
    "_uuid": "40004d8d387f019de56d7cf9fd283bcee67949d6"
   },
   "source": [
    "# Cross validation with XGB\n",
    "\n",
    "The application bayesian application require the usage of cross validation to compute each combination of parameters during the exploration, so we will also need to set up the cross validation function for XGB. In this case, we only return the cross validation score as the output of the function.  Essentially, the CV funciton is being treated as the target function to be optimised in our little exercise here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "7f0065d8-8870-4d8c-a2f7-a6b25d419588",
    "_uuid": "f0f8b893c888f8dcbfdc00b26e35b765df1f3571",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_xgb(params, x_train, y_train, kf, verbose=True, verbose_eval=50):\n",
    "    start_time=time.time()\n",
    "    nround=[]\n",
    "    # the prediction matrix need to contains 3 columns, one for the probability of each class\n",
    "    train_pred = np.zeros((x_train.shape[0],3))\n",
    "    \n",
    "    # use the k-fold object to enumerate indexes for each training and validation fold\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "        y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        d_train = xgboost.DMatrix(x_train_kf, y_train_kf)\n",
    "        d_val=xgboost.DMatrix(x_val_kf, y_val_kf)\n",
    "\n",
    "        watchlist= [(d_train, \"train\"), (d_val, 'val')]\n",
    "        bst = xgboost.train(params=params, dtrain=d_train, num_boost_round=3000, early_stopping_rounds=100,\n",
    "                            evals=watchlist, verbose_eval=verbose_eval)        \n",
    "        \n",
    "        y_val_kf_preds=bst.predict(d_val, ntree_limit=bst.best_ntree_limit)\n",
    "        nround.append(bst.best_ntree_limit)\n",
    "        \n",
    "        train_pred[val_index] += y_val_kf_preds\n",
    "        \n",
    "        fold_cv = log_loss(y_val_kf.values, y_val_kf_preds)\n",
    "        if verbose:\n",
    "            print('fold cv {} log_loss score is {:.6f}'.format(i, fold_cv))\n",
    "        \n",
    "    cv_score = log_loss(y_train, train_pred)\n",
    "    \n",
    "    if verbose:\n",
    "        print('cv log_loss score is {:.6f}'.format(cv_score))    \n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    "    return cv_score # for the purpose of bayesian optimisation, we only need to return the CV score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "24b5da39-3002-4043-8b12-ee296ce1452c",
    "_uuid": "9a9b1d75915afba9b969b7e0c61f83ac28a6be7f"
   },
   "source": [
    "let's test the above cross validation function with some simple xgb parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "f1350ef9-623b-417e-9c5d-acb428591027",
    "_uuid": "67e57e561798b4f76860d8a41a8c289f19c94784",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[0]\ttrain-mlogloss:1.06346\tval-mlogloss:1.06533\n",
      "Multiple eval metrics have been passed: 'val-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-mlogloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mlogloss:0.545517\tval-mlogloss:0.595939\n",
      "[100]\ttrain-mlogloss:0.46139\tval-mlogloss:0.534824\n",
      "[150]\ttrain-mlogloss:0.427132\tval-mlogloss:0.516735\n",
      "[200]\ttrain-mlogloss:0.402704\tval-mlogloss:0.507676\n",
      "[250]\ttrain-mlogloss:0.381913\tval-mlogloss:0.501956\n",
      "[300]\ttrain-mlogloss:0.363002\tval-mlogloss:0.498393\n",
      "[350]\ttrain-mlogloss:0.346867\tval-mlogloss:0.497183\n",
      "[400]\ttrain-mlogloss:0.332531\tval-mlogloss:0.496637\n",
      "[450]\ttrain-mlogloss:0.320011\tval-mlogloss:0.49666\n",
      "[500]\ttrain-mlogloss:0.307604\tval-mlogloss:0.496459\n",
      "[550]\ttrain-mlogloss:0.296493\tval-mlogloss:0.496676\n",
      "[600]\ttrain-mlogloss:0.287124\tval-mlogloss:0.498415\n",
      "Stopping. Best iteration:\n",
      "[507]\ttrain-mlogloss:0.306031\tval-mlogloss:0.496159\n",
      "\n",
      "[0]\ttrain-mlogloss:1.06459\tval-mlogloss:1.06517\n",
      "Multiple eval metrics have been passed: 'val-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-mlogloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mlogloss:0.55025\tval-mlogloss:0.592538\n",
      "[100]\ttrain-mlogloss:0.463464\tval-mlogloss:0.531759\n",
      "[150]\ttrain-mlogloss:0.426482\tval-mlogloss:0.513144\n",
      "[200]\ttrain-mlogloss:0.402812\tval-mlogloss:0.505615\n",
      "[250]\ttrain-mlogloss:0.382569\tval-mlogloss:0.50175\n",
      "[300]\ttrain-mlogloss:0.363193\tval-mlogloss:0.498791\n",
      "[350]\ttrain-mlogloss:0.348022\tval-mlogloss:0.498236\n",
      "[400]\ttrain-mlogloss:0.334408\tval-mlogloss:0.497617\n",
      "[450]\ttrain-mlogloss:0.322218\tval-mlogloss:0.497981\n",
      "Stopping. Best iteration:\n",
      "[373]\ttrain-mlogloss:0.341747\tval-mlogloss:0.497374\n",
      "\n",
      "[0]\ttrain-mlogloss:1.06543\tval-mlogloss:1.06714\n",
      "Multiple eval metrics have been passed: 'val-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-mlogloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mlogloss:0.547984\tval-mlogloss:0.601007\n",
      "[100]\ttrain-mlogloss:0.459125\tval-mlogloss:0.543962\n",
      "[150]\ttrain-mlogloss:0.420646\tval-mlogloss:0.529684\n",
      "[200]\ttrain-mlogloss:0.396169\tval-mlogloss:0.5232\n",
      "[250]\ttrain-mlogloss:0.37576\tval-mlogloss:0.52084\n",
      "[300]\ttrain-mlogloss:0.357345\tval-mlogloss:0.519079\n",
      "[350]\ttrain-mlogloss:0.340805\tval-mlogloss:0.518539\n",
      "[400]\ttrain-mlogloss:0.326513\tval-mlogloss:0.518119\n",
      "[450]\ttrain-mlogloss:0.313617\tval-mlogloss:0.518347\n",
      "[500]\ttrain-mlogloss:0.301378\tval-mlogloss:0.51976\n",
      "Stopping. Best iteration:\n",
      "[403]\ttrain-mlogloss:0.325424\tval-mlogloss:0.518019\n",
      "\n",
      "cv score is 0.503847\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"objective\" : \"multi:softprob\",\n",
    "    \"num_class\" : 3,\n",
    "    \"tree_method\" : \"hist\",\n",
    "    \"eval_metric\" : \"mlogloss\",\n",
    "    \"nthread\": 4,\n",
    "    \"seed\" : 0,\n",
    "    'silent': 1,\n",
    "\n",
    "    \"eta\":0.05,  # default 0.3\n",
    "    \"max_depth\" : 5, # default 6\n",
    "    \"subsample\" : 0.8, # default 1\n",
    "    \"colsample_bytree\" : 0.6, # default 1\n",
    "    \"gamma\": 0.5\n",
    "}\n",
    "\n",
    "# only do 3 fold CV here so that we save some running time on Kaggle Kernel\n",
    "kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2017)\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "\n",
    "cv_score =cross_validate_xgb(xgb_params, train_input, y, kf, verbose=False, verbose_eval=50)\n",
    "\n",
    "print('cv score is {:.6f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1b8a8460-0e7b-427a-b968-a65864f8aee6",
    "_uuid": "f8746ede025a8206313e0ae987ef38015078e5da"
   },
   "source": [
    "# Bayesian Optimsation - Setup\n",
    "\n",
    "The next step is to setup the search space for bayesian optimisation to explore for optimun cross validation score. \n",
    "This search space is denfied by sets of interval values for hyper-parameters of interest. In this example, we use the four parameters that are most "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "493760d5-ed10-48f4-a85e-c2062e7485c7",
    "_uuid": "68ef78443fb3bfda4b8c1c9a189496863eb86319",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params={'max_depth':(4,10),\n",
    "        'learning_rate':(0.05,0.3),\n",
    "        'subsample': (0.4, 1),\n",
    "        'colsample_bytree': (0.4, 1)\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "ef9e297c-a70d-481f-94c1-d49a4525bbf9",
    "_uuid": "4536fef4480041b78d32e763b56b2a5d0e2b1bad",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reload(xgb_wrapper)\n",
    "def xgbcv_func(max_depth, learning_rate, subsample, colsample_bytree, nthread=4, seed=0):\n",
    "    params = {\n",
    "        \"objective\" : \"multi:softprob\",\n",
    "        \"num_class\" : 3,\n",
    "        \"tree_method\" : \"hist\",\n",
    "        \"eval_metric\" : \"mlogloss\",\n",
    "        \"nthread\": nthread,\n",
    "        \"seed\" : 0,\n",
    "        'silent': 1,\n",
    "\n",
    "        \"eta\":learning_rate,  # default 0.3\n",
    "        \"max_depth\" : int(max_depth), # default 6\n",
    "        \"subsample\" : subsample, # default 1\n",
    "        \"colsample_bytree\" : colsample_bytree, # default 1\n",
    "    }\n",
    "    \n",
    "    # for a more ideal out-of-fold model prediction for this dataset, we use 10-fold CV\n",
    "    kf=StratifiedKFold(n_splits=10, shuffle=True, random_state=2017)\n",
    "    \n",
    "    # we will disable all the verbose setting in this functional call, so that we don't have too much information \n",
    "    # to read during the bayesian optimisation process.\n",
    "    return 1-cross_validate_xgb(params, train_input, y, kf, verbose=False, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "edece269-5d23-4658-862e-123d6f11c3c9",
    "_uuid": "fc5ffe093852af4fb249b9fb54fed0378f4257cf"
   },
   "source": [
    "now we can setup the bayesian optimisation object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "df037307-a1cc-4077-874d-990e30b1874d",
    "_uuid": "e5aa23792faf20194087633b7b0d58a0c23b31eb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_bo=BayesianOptimization(xgbcv_func, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "59431fa7-4530-4b96-b704-44c08859692e",
    "_uuid": "4fe580526121bfca88f7ee9ceaf185de8065a052"
   },
   "source": [
    "and... action time (In this example we use small values for *init_points* and *n_iter* so that the run can be fitted in 60 minutes of Kaggle's kernel run time limit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "d65dcb4a-08bf-4af5-99de-1d6120cdaf80",
    "_uuid": "ef3087788fd6a5f738192b3f383d997c18333a5d",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   subsample | \n",
      "    1 | 01m00s | \u001b[35m   0.50264\u001b[0m | \u001b[32m            0.9745\u001b[0m | \u001b[32m         0.2634\u001b[0m | \u001b[32m     5.4654\u001b[0m | \u001b[32m     0.9019\u001b[0m | \n",
      "    2 | 00m56s |    0.49491 |             0.9140 |          0.2311 |      5.6636 |      0.4037 | \n",
      "    3 | 01m14s | \u001b[35m   0.50482\u001b[0m | \u001b[32m            0.6768\u001b[0m | \u001b[32m         0.2154\u001b[0m | \u001b[32m     7.3889\u001b[0m | \u001b[32m     0.9879\u001b[0m | \n",
      "    4 | 01m20s |    0.50125 |             0.9249 |          0.1788 |      6.8017 |      0.6467 | \n",
      "    5 | 01m12s |    0.50104 |             0.8798 |          0.1801 |      4.1285 |      0.5372 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   subsample | \n",
      "    6 | 03m32s |    0.49228 |             1.0000 |          0.0500 |     10.0000 |      1.0000 | \n",
      "    7 | 04m49s |    0.50288 |             0.4000 |          0.0500 |      4.7812 |      1.0000 | \n",
      "    8 | 03m27s |    0.50144 |             1.0000 |          0.0500 |      6.6491 |      1.0000 | \n",
      "    9 | 01m34s |    0.48695 |             0.4000 |          0.3000 |     10.0000 |      0.4000 | \n",
      "   10 | 01m22s |    0.50166 |             0.4000 |          0.3000 |      4.0000 |      1.0000 | \n",
      "   11 | 01m27s |    0.48771 |             0.9891 |          0.2951 |      8.6588 |      0.4699 | \n",
      "   12 | 03m59s |    0.50014 |             0.9541 |          0.0500 |      4.0000 |      1.0000 | \n",
      "   13 | 01m28s |    0.50098 |             0.4000 |          0.3000 |     10.0000 |      1.0000 | \n",
      "   14 | 01m05s |    0.50265 |             0.4000 |          0.3000 |      6.3330 |      1.0000 | \n",
      "   15 | 03m22s | \u001b[35m   0.50993\u001b[0m | \u001b[32m            0.4000\u001b[0m | \u001b[32m         0.0500\u001b[0m | \u001b[32m     8.9846\u001b[0m | \u001b[32m     1.0000\u001b[0m | \n",
      "   16 | 02m51s |    0.50470 |             0.4000 |          0.0500 |      7.7482 |      0.4000 | \n",
      "   17 | 03m49s |    0.50855 |             0.4000 |          0.0500 |     10.0000 |      1.0000 | \n",
      "   18 | 03m42s | \u001b[35m   0.51152\u001b[0m | \u001b[32m            0.4049\u001b[0m | \u001b[32m         0.0538\u001b[0m | \u001b[32m     8.0589\u001b[0m | \u001b[32m     0.9605\u001b[0m | \n",
      "   19 | 03m21s |    0.51046 |             0.4010 |          0.0502 |      7.6252 |      0.9561 | \n",
      "   20 | 03m11s |    0.50991 |             0.4147 |          0.0560 |      8.3311 |      0.9966 | \n",
      "   21 | 02m16s |    0.50146 |             0.4026 |          0.0793 |      4.0103 |      0.4119 | \n",
      "   22 | 01m23s |    0.50619 |             0.4022 |          0.2983 |      7.8264 |      0.7933 | \n",
      "   23 | 03m22s |    0.51128 |             0.4033 |          0.0534 |      7.9585 |      0.8098 | \n",
      "   24 | 03m40s |    0.50970 |             0.4274 |          0.0531 |      9.5585 |      0.9931 | \n",
      "   25 | 03m47s |    0.50999 |             0.4629 |          0.0503 |      7.9026 |      0.9567 | \n"
     ]
    }
   ],
   "source": [
    "xgb_bo.maximize(init_points=5, n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_custom_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "83edf8cd-4d71-4efd-81f0-854b6f423077",
    "_uuid": "40887b3890e6e08ce94ab94d97f2403397ed3b24",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Initialization\n",
    "------------------------------------------------------------------------------------------------\n",
    " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   subsample | \n",
    "    1 | 01m00s |    0.50264 |             0.9745 |          0.2634 |      5.4654 |      0.9019 | \n",
    "    2 | 00m56s |    0.49491 |             0.9140 |          0.2311 |      5.6636 |      0.4037 | \n",
    "    3 | 01m14s |    0.50482 |             0.6768 |          0.2154 |      7.3889 |      0.9879 | \n",
    "    4 | 01m20s |    0.50125 |             0.9249 |          0.1788 |      6.8017 |      0.6467 | \n",
    "    5 | 01m12s |    0.50104 |             0.8798 |          0.1801 |      4.1285 |      0.5372 | \n",
    "Bayesian Optimization\n",
    "------------------------------------------------------------------------------------------------\n",
    " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   subsample | \n",
    "    6 | 03m32s |    0.49228 |             1.0000 |          0.0500 |     10.0000 |      1.0000 | \n",
    "    7 | 04m49s |    0.50288 |             0.4000 |          0.0500 |      4.7812 |      1.0000 | \n",
    "    8 | 03m27s |    0.50144 |             1.0000 |          0.0500 |      6.6491 |      1.0000 | \n",
    "    9 | 01m34s |    0.48695 |             0.4000 |          0.3000 |     10.0000 |      0.4000 | \n",
    "   10 | 01m22s |    0.50166 |             0.4000 |          0.3000 |      4.0000 |      1.0000 | \n",
    "   11 | 01m27s |    0.48771 |             0.9891 |          0.2951 |      8.6588 |      0.4699 | \n",
    "   12 | 03m59s |    0.50014 |             0.9541 |          0.0500 |      4.0000 |      1.0000 | \n",
    "   13 | 01m28s |    0.50098 |             0.4000 |          0.3000 |     10.0000 |      1.0000 | \n",
    "   14 | 01m05s |    0.50265 |             0.4000 |          0.3000 |      6.3330 |      1.0000 | \n",
    "   15 | 03m22s |    0.50993 |             0.4000 |          0.0500 |      8.9846 |      1.0000 | \n",
    "   16 | 02m51s |    0.50470 |             0.4000 |          0.0500 |      7.7482 |      0.4000 | \n",
    "   17 | 03m49s |    0.50855 |             0.4000 |          0.0500 |     10.0000 |      1.0000 | \n",
    "   18 | 03m42s |    0.51152 |             0.4049 |          0.0538 |      8.0589 |      0.9605 | \n",
    "   19 | 03m21s |    0.51046 |             0.4010 |          0.0502 |      7.6252 |      0.9561 | \n",
    "   20 | 03m11s |    0.50991 |             0.4147 |          0.0560 |      8.3311 |      0.9966 | \n",
    "   21 | 02m16s |    0.50146 |             0.4026 |          0.0793 |      4.0103 |      0.4119 | \n",
    "   22 | 01m23s |    0.50619 |             0.4022 |          0.2983 |      7.8264 |      0.7933 | \n",
    "   23 | 03m22s |    0.51128 |             0.4033 |          0.0534 |      7.9585 |      0.8098 | \n",
    "   24 | 03m40s |    0.50970 |             0.4274 |          0.0531 |      9.5585 |      0.9931 | \n",
    "   25 | 03m47s |    0.50999 |             0.4629 |          0.0503 |      7.9026 |      0.9567 |\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c8d5851f-13f7-4506-a669-f9dae96641f7",
    "_uuid": "8fd9d102c79a3a63c6359b62ab06d41ef94f644f",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0254e675-aba7-4e22-b80f-120e000a451a",
    "_uuid": "6df61936b6086b58a51383026a6766113e85d305",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b5eedd67-319b-4cbf-a847-0611eab17b92",
    "_uuid": "5d923037cdbe8f00f8bd54466bbdea41cbc90801",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9605eb2c-83ee-468d-9da7-8bc28a181954",
    "_uuid": "bd29b2fb2564de5cade56b1f8045b563f2c7fc14",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "adbf779c-96b4-4605-87d7-b8b3fa2ae074",
    "_uuid": "4ead07df7b87be420139d08e5a8dc2186ddfe0ad",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "760f9725-9958-4a91-8279-fc1022d2768c",
    "_uuid": "07bf7df335b8a0a01ac1aa9812c59860fdf75e3d",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "78752aeb-75a4-4c04-9545-1d7b161e2eae",
    "_uuid": "a1e37f0913959e14940aedd1eb43d9662c96836b",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
