{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contributions from:\n",
    "DSEverything - Mean Mix - Math, Geo, Harmonic (LB 0.493) \n",
    "https://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493\n",
    "JdPaletto - Surprised Yet? - Part2 - (LB: 0.503)\n",
    "https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503\n",
    "hklee - weighted mean comparisons, LB 0.497, 1ST\n",
    "https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st\n",
    "\n",
    "Also all comments for changes, encouragement, and forked scripts rock\n",
    "\n",
    "Keep the Surprise Going\n",
    "\"\"\"\n",
    "\n",
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../../mltestdata/05_recruit/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "    }\n",
    "\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "#sure it can be compressed...\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n",
    "\n",
    "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n",
    "\n",
    "# NEW FEATURES FROM JMBULL\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "\n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, dtype in zip(train.columns, train.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        train[c] = train[c].astype(np.float32)\n",
    "\n",
    "for c, dtype in zip(test.columns, test.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        test[c] = test[c].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Golden week flag and Post Golden week flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = [train, test]\n",
    "gw_list = ['2016-04-29','2016-04-30','2016-05-01','2016-05-02','2016-05-03','2016-05-04','2016-05-05','2017-04-29','2017-04-30','2017-05-01','2017-05-02','2017-05-03','2017-05-04','2017-05-05']\n",
    "post_gw_list=['2016-05-06']\n",
    "train['gw_flg'] = 0\n",
    "train['post_gw_flg'] = 0\n",
    "test['gw_flg'] = 0\n",
    "test['post_gw_flg'] = 0\n",
    "update_gw_list = [[\"0\" for i in range(3)] for j in range(len(gw_list))]\n",
    "update_post_gw_list = [[\"0\" for i in range(3)] for j in range(len(post_gw_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "for index, gw_date in enumerate(gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_gw_list[index][col_i]=int(temp_figure)\n",
    "        \n",
    "    #print(\"{}  {}  {}\".format(update_list[index][0],update_list[index][1],update_list[index][2]))\n",
    "    \n",
    "for index, gw_date in enumerate(post_gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_post_gw_list[index][col_i]=int(temp_figure)\n",
    "\n",
    "for dataset in combine:\n",
    "    for index in range(len(update_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_gw_list[index][0],update_gw_list[index][1],update_gw_list[index][2]), 'gw_flg'] = 1\n",
    "        \n",
    "for dataset in combine:\n",
    "    for index in range(len(update_post_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_post_gw_list[index][0],update_post_gw_list[index][1],update_post_gw_list[index][2]), 'post_gw_flg'] = 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "y_train=train['visitors']\n",
    "x_train=train.drop(drop_cols, axis=1)\n",
    "\n",
    "x_test=test.copy()\n",
    "x_test=x_test.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.visitors\n",
    "train_input = train.copy()\n",
    "test_input = test.copy()\n",
    "\n",
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "train_input=train_input.drop(drop_cols, axis=1)\n",
    "test_input=test_input.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation function\n",
    "\n",
    "def rmsle(preds, true):\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n",
    "    return float(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation matrix \n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "RMSLE = make_scorer(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for comparing predictions and true data.\n",
    "def compare_result(preds, true):\n",
    "    compare = pd.DataFrame({\"test_id\": true.index,\n",
    "                           \"real_cost\": true,\n",
    "                           \"pred_cost\": preds})\n",
    "    compare = compare[[\"test_id\", \"real_cost\", \"pred_cost\"]].reset_index(drop=True)\n",
    "    \n",
    "    compare[\"error_percent_(%)\"] = np.abs(compare.real_cost - compare.pred_cost) / compare.real_cost * 100\n",
    "    \n",
    "    return compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation with KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_sklearn(clf, x_train, y_train , x_test, kf,scale=False, verbose=True, scoreonly=False):\n",
    "    start_time=time.time()\n",
    "    \n",
    "    # initialise the size of out-of-fold train an test prediction\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "\n",
    "    # self-defined eval metric\n",
    "    # f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "    # binary error\n",
    "    def feval_rmsle(preds, train_data):\n",
    "        preds = np.expm1(preds)\n",
    "        true = np.expm1(train_data.get_label())\n",
    "        #return 'rmsle', rmsle(true, preds), False\n",
    "\n",
    "        return 'rmsle', rmsle(preds, true), False\n",
    "    \n",
    "    # use the kfold object to generate the required folds\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        # generate training folds and validation fold\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "\n",
    "        #y_train_kf, y_val_kf = y_train[train_index], y_train[test_index]\n",
    "        y_train_kf, y_val_kf = np.log1p(y_train[train_index]), np.log1p(y_train[val_index])\n",
    "        x_test_kf=x_test.copy()\n",
    "\n",
    "        # perform scaling if required i.e. for linear algorithms\n",
    "        if scale:\n",
    "            scaler = StandardScaler().fit(x_train_kf.values)\n",
    "            x_train_kf_values = scaler.transform(x_train_kf.values)\n",
    "            x_val_kf_values = scaler.transform(x_val_kf.values)\n",
    "            x_test_values = scaler.transform(x_test.values)\n",
    "        else:\n",
    "            x_train_kf_values = x_train_kf.values\n",
    "            x_val_kf_values = x_val_kf.values\n",
    "            x_test_values = x_test.values\n",
    "        \n",
    "        # fit the input classifier and perform prediction.\n",
    "        clf.fit(x_train_kf_values, y_train_kf.values)\n",
    "\n",
    "        #val_pred=clf.predict_proba(x_val_kf_values)[:,1]\n",
    "        val_pred=np.expm1(clf.predict(x_val_kf_values))\n",
    "        #train_pred[test_index] += val_pred\n",
    "        train_pred[val_index] += val_pred\n",
    "\n",
    "        #y_test_preds = clf.predict_proba(x_test_values)[:,1]\n",
    "        y_test_preds = np.expm1(clf.predict(x_test_values))\n",
    "        test_pred += y_test_preds\n",
    "\n",
    "        #fold_auc = roc_auc_score(y_val_kf.values, val_pred)\n",
    "        #fold_gini_norm = auc_to_gini_norm(fold_auc)\n",
    "        fold_rmsle = rmsle(train_pred[val_index],np.expm1(y_val_kf.values))\n",
    "\n",
    "        if verbose:\n",
    "            print('fold cv {} RMSLE score is {:.6f}'.format(i, fold_rmsle))\n",
    "\n",
    "    test_pred /= kf.n_splits\n",
    "\n",
    "    #cv_auc = roc_auc_score(y_train, train_pred)\n",
    "    #cv_gini_norm = auc_to_gini_norm(cv_auc)\n",
    "    cv_score = rmsle(train_pred, y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print('cv RMSLE score is {:.6f}'.format(cv_score))\n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    "\n",
    "    if scoreonly:\n",
    "        return cv_score\n",
    "    else:\n",
    "        return cv_score, train_pred, test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 RMSLE score is 0.556426\n",
      "fold cv 1 RMSLE score is 0.555690\n",
      "fold cv 2 RMSLE score is 0.549253\n",
      "fold cv 3 RMSLE score is 0.552633\n",
      "fold cv 4 RMSLE score is 0.546384\n",
      "cv RMSLE score is 0.552094\n",
      "it takes 2933.039 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "# cluster 3 with scaling\n",
    "knr = KNeighborsRegressor(n_jobs=-1, n_neighbors=3)\n",
    "\n",
    "outcomes =cross_validate_sklearn(knr, x_train, y_train ,x_test, kf, scale=True, verbose=True)\n",
    "\n",
    "knr_cv=outcomes[0]\n",
    "knr_train_pred=outcomes[1]\n",
    "knr_test_pred=outcomes[2]\n",
    "\n",
    "knr_train_pred_df=pd.DataFrame(columns=['visitors'], data=knr_train_pred)\n",
    "knr_test_pred_df=pd.DataFrame(columns=['visitors'], data=knr_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 RMSLE score is 0.557567\n",
      "fold cv 1 RMSLE score is 0.552856\n",
      "fold cv 2 RMSLE score is 0.551196\n",
      "fold cv 3 RMSLE score is 0.549202\n",
      "fold cv 4 RMSLE score is 0.547743\n",
      "cv RMSLE score is 0.551727\n",
      "it takes 1447.553 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "# Cluster 4 *no scale\n",
    "knr = KNeighborsRegressor(n_jobs=-1, n_neighbors=4)\n",
    "\n",
    "outcomes =cross_validate_sklearn(knr, x_train, y_train ,x_test, kf, scale=False, verbose=True)\n",
    "\n",
    "knr_cv=outcomes[0]\n",
    "knr_train_pred=outcomes[1]\n",
    "knr_test_pred=outcomes[2]\n",
    "\n",
    "knr_train_pred_df=pd.DataFrame(columns=['visitors'], data=knr_train_pred)\n",
    "knr_test_pred_df=pd.DataFrame(columns=['visitors'], data=knr_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 RMSLE score is 0.541592\n",
      "fold cv 1 RMSLE score is 0.539625\n",
      "fold cv 2 RMSLE score is 0.533373\n",
      "fold cv 3 RMSLE score is 0.537068\n",
      "fold cv 4 RMSLE score is 0.532714\n",
      "cv RMSLE score is 0.536889\n",
      "it takes 3476.677 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "# Cluster 4 with scaling\n",
    "knr = KNeighborsRegressor(n_jobs=-1, n_neighbors=4)\n",
    "\n",
    "outcomes =cross_validate_sklearn(knr, x_train, y_train ,x_test, kf, scale=True, verbose=True)\n",
    "\n",
    "knr_cv=outcomes[0]\n",
    "knr_train_pred=outcomes[1]\n",
    "knr_test_pred=outcomes[2]\n",
    "\n",
    "knr_train_pred_df=pd.DataFrame(columns=['visitors'], data=knr_train_pred)\n",
    "knr_test_pred_df=pd.DataFrame(columns=['visitors'], data=knr_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 RMSLE score is 0.532660\n",
      "fold cv 1 RMSLE score is 0.530332\n",
      "fold cv 2 RMSLE score is 0.524591\n",
      "fold cv 3 RMSLE score is 0.528426\n",
      "fold cv 4 RMSLE score is 0.524071\n",
      "cv RMSLE score is 0.528030\n",
      "it takes 2813.101 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "# Cluster 5 with scaling\n",
    "knr = KNeighborsRegressor(n_jobs=-1, n_neighbors=5)\n",
    "\n",
    "outcomes =cross_validate_sklearn(knr, x_train, y_train ,x_test, kf, scale=True, verbose=True)\n",
    "\n",
    "knr_cv=outcomes[0]\n",
    "knr_train_pred=outcomes[1]\n",
    "knr_test_pred=outcomes[2]\n",
    "\n",
    "knr_train_pred_df=pd.DataFrame(columns=['visitors'], data=knr_train_pred)\n",
    "knr_test_pred_df=pd.DataFrame(columns=['visitors'], data=knr_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 RMSLE score is 0.526898\n",
      "fold cv 1 RMSLE score is 0.524095\n",
      "fold cv 2 RMSLE score is 0.518715\n",
      "fold cv 3 RMSLE score is 0.522726\n",
      "fold cv 4 RMSLE score is 0.518273\n",
      "cv RMSLE score is 0.522155\n",
      "it takes 15778.621 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "# Cluster 6 with scaling\n",
    "knr = KNeighborsRegressor(n_jobs=-1, n_neighbors=6)\n",
    "\n",
    "outcomes =cross_validate_sklearn(knr, x_train, y_train ,x_test, kf, scale=True, verbose=True)\n",
    "\n",
    "knr_cv=outcomes[0]\n",
    "knr_train_pred=outcomes[1]\n",
    "knr_test_pred=outcomes[2]\n",
    "\n",
    "knr_train_pred_df=pd.DataFrame(columns=['visitors'], data=knr_train_pred)\n",
    "knr_test_pred_df=pd.DataFrame(columns=['visitors'], data=knr_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 RMSLE score is 0.523658\n",
      "fold cv 1 RMSLE score is 0.521276\n",
      "fold cv 2 RMSLE score is 0.515578\n",
      "fold cv 3 RMSLE score is 0.519137\n",
      "fold cv 4 RMSLE score is 0.514863\n",
      "cv RMSLE score is 0.518917\n",
      "it takes 3800.235 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "# Cluster 7 with scaling\n",
    "knr = KNeighborsRegressor(n_jobs=-1, n_neighbors=7)\n",
    "\n",
    "outcomes =cross_validate_sklearn(knr, x_train, y_train ,x_test, kf, scale=True, verbose=True)\n",
    "\n",
    "knr_cv=outcomes[0]\n",
    "knr_train_pred=outcomes[1]\n",
    "knr_test_pred=outcomes[2]\n",
    "\n",
    "knr_train_pred_df=pd.DataFrame(columns=['visitors'], data=knr_train_pred)\n",
    "knr_test_pred_df=pd.DataFrame(columns=['visitors'], data=knr_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "scoring_fnc=make_scorer(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "knr = KNeighborsRegressor(n_jobs=-1, n_neighbors=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=uniform, score=0.666111905261168, total=  12.0s\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=uniform \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=uniform, score=0.6925398526999745, total= 5.3min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=uniform \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  6.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=uniform, score=0.6800161775155649, total= 4.9min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=uniform, score=0.6701719564017073, total= 4.8min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=uniform, score=0.65008721096032, total= 4.8min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=distance, score=0.6662332412701135, total=  12.1s\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=distance, score=0.6930461823960394, total= 4.8min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=distance, score=0.6802101553086287, total= 4.8min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=distance, score=0.6703357319221299, total= 4.8min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=5, weights=distance, score=0.6502691314493592, total= 4.6min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=uniform, score=0.6581497111869924, total=  11.0s\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=uniform, score=0.683424969287187, total= 4.7min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=uniform, score=0.6738511111448842, total= 4.5min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=uniform, score=0.6616614927998953, total= 4.5min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=uniform, score=0.6414108991851677, total= 4.8min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=distance, score=0.6581994858781856, total=  11.0s\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=distance, score=0.6838063249803255, total= 4.5min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=distance, score=0.6737890442282652, total= 4.4min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=distance, score=0.6617472023258462, total= 4.6min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=6, weights=distance, score=0.6415867026151049, total= 4.6min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=uniform, score=0.6533679132265625, total=  11.3s\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=uniform, score=0.6776837780065854, total= 4.6min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=uniform, score=0.6691349405476941, total=55.5min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=uniform, score=0.655920480449406, total= 4.6min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=uniform, score=0.6346457711002957, total= 4.6min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=distance, score=0.6532230420713726, total=  11.1s\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=distance, score=0.6778805911026867, total= 4.5min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=distance, score=0.6688853439592565, total= 4.5min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=distance, score=0.6558363348983444, total= 4.6min\n",
      "[CV] algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=2, n_jobs=-1, n_neighbors=7, weights=distance, score=0.6347825211688155, total= 4.7min\n",
      "[CV] algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=uniform, score=0.666111905261168, total=  12.5s\n",
      "[CV] algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=uniform, score=0.6925398526999745, total= 4.8min\n",
      "[CV] algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=uniform, score=0.6800161775155649, total= 4.9min\n",
      "[CV] algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=uniform, score=0.6701719564017073, total= 5.2min\n",
      "[CV] algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=uniform, score=0.65008721096032, total= 5.2min\n",
      "[CV] algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=distance, score=0.6662332412701135, total=  16.0s\n",
      "[CV] algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=distance, score=0.6930461823960394, total= 5.1min\n",
      "[CV] algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=distance, score=0.6802101553086287, total= 4.7min\n",
      "[CV] algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=distance, score=0.6703357319221299, total= 4.7min\n",
      "[CV] algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=distance \n",
      "[CV]  algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=5, weights=distance, score=0.6502691314493592, total= 4.8min\n",
      "[CV] algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=6, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=6, weights=uniform, score=0.6581497111869924, total=  11.4s\n",
      "[CV] algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=6, weights=uniform \n",
      "[CV]  algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=6, weights=uniform, score=0.683424969287187, total= 4.8min\n",
      "[CV] algorithm=auto, leaf_size=3, n_jobs=-1, n_neighbors=6, weights=uniform \n"
     ]
    }
   ],
   "source": [
    "#Hyper Parameters Set\n",
    "params = {'n_neighbors':[5,6,7],\n",
    "          'leaf_size':[2,3,5],\n",
    "          'weights':['uniform', 'distance'],\n",
    "          'algorithm':['auto', 'ball_tree','kd_tree','brute'],\n",
    "          'n_jobs':[-1]}\n",
    "#Making models with hyper parameters sets\n",
    "\n",
    "model1 = GridSearchCV(knr, param_grid=params, n_jobs=1,scoring=scoring_fnc,cv=5,verbose=3)\n",
    "#Learning\n",
    "model1.fit(train_input,np.log1p(y))\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model1.best_params_)\n",
    "#Prediction\n",
    "prediction=model1.predict(test_input)\n",
    "#importing the metrics module\n",
    "\n",
    "from sklearn import metrics\n",
    "#evaluation(Accuracy)\n",
    "#print(\"Accuracy:\",metrics.accuracy_score(prediction,test_y))\n",
    "#evaluation(Confusion Metrix)\n",
    "#print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Velification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 rmsle score is 0.497094\n",
      "fold cv 1 rmsle score is 0.501248\n",
      "fold cv 2 rmsle score is 0.496586\n",
      "cv rmsle score is 0.498314\n",
      "it takes 307.088 seconds to perform cross validation\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"objective\" : \"reg:linear\",\n",
    "    \"eval_metric\" : \"rmse\",\n",
    "    \"nthread\": 4,\n",
    "    \"seed\" : 0,\n",
    "    'silent': 1,\n",
    "    \"colsample_bytree\"  :  0.5555, \n",
    "    \"gamma\": 1.8385,\n",
    "    \"learning_rate\": 0.2968,\n",
    "    \"max_delta_step\": 8.3539,\n",
    "    \"max_depth\": 10,#9.6366,\n",
    "    \"max_features\": 0.6203,\n",
    "    \"min_child_weight\": 8.1246,\n",
    "    \"min_samples_split\": 16.2850,\n",
    "    \"n_estimators\": 14.1175,\n",
    "    \"subsample\": 0.7658,\n",
    "    \"seed\": 0,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "outcomes=cross_validate_xgb(xgb_params, train_input, y, test_input, kf, verbose_eval=False)\n",
    "\n",
    "xgb_cv=outcomes[0]\n",
    "xgb_train_pred=outcomes[1]\n",
    "xgb_test_pred=outcomes[2]\n",
    "\n",
    "xgb_train_pred_df=pd.DataFrame(columns=['visitors'], data=xgb_train_pred)\n",
    "xgb_test_pred_df=pd.DataFrame(columns=['visitors'], data=xgb_test_pred)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv1_knr_train_pred = knr_train_pred_df.copy()\n",
    "lv1_knr_train_pred.to_csv('lv1_knr_train_pred.csv', index=False)\n",
    "\n",
    "lv1_knr_test_pred = knr_test_pred_df.copy()\n",
    "lv1_knr_test_pred.to_csv('lv1_knr_test_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last work with weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../../../mltestdata/05_recruit/*.csv')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n",
    "    on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_merge['visitors'] = (sub_merge['visitors_x'] + sub_merge['visitors_y']* 1.1)/2\n",
    "sub_merge[['id', 'visitors']].to_csv('submission_rs_recruit_v11_xgbm_v14_02.csv', index=False)\n",
    "# LB 0.487"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
