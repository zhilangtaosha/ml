{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suzukiry/lab/March/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/suzukiry/lab/March/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/Users/suzukiry/lab/March/lib/python3.6/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Contributions from:\n",
    "DSEverything - Mean Mix - Math, Geo, Harmonic (LB 0.493) \n",
    "https://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493\n",
    "JdPaletto - Surprised Yet? - Part2 - (LB: 0.503)\n",
    "https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503\n",
    "hklee - weighted mean comparisons, LB 0.497, 1ST\n",
    "https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st\n",
    "\n",
    "Also all comments for changes, encouragement, and forked scripts rock\n",
    "\n",
    "Keep the Surprise Going\n",
    "\"\"\"\n",
    "\n",
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../../mltestdata/05_recruit/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "    }\n",
    "\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "#sure it can be compressed...\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n",
    "\n",
    "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n",
    "\n",
    "# NEW FEATURES FROM JMBULL\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "\n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for LightGBM ...\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nProcessing data for LightGBM ...\" )\n",
    "for c, dtype in zip(train.columns, train.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        train[c] = train[c].astype(np.float32)\n",
    "\n",
    "for c, dtype in zip(test.columns, test.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        test[c] = test[c].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "y_train=train['visitors']\n",
    "x_train=train.drop(drop_cols, axis=1)\n",
    "\n",
    "x_test=test.copy()\n",
    "x_test=x_test.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import time\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "localtrain, localval = train_test_split(train,test_size=0.3,random_state=2018)\n",
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "y_localtrain=localtrain['visitors']\n",
    "x_localtrain=localtrain.drop(drop_cols, axis=1)\n",
    "\n",
    "y_localval=localval['visitors']\n",
    "x_localval=localval.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation function\n",
    "\n",
    "def eval_rmsle(preds, true):\n",
    "    eval_rmsle = (np.sum((np.log1p(preds)-np.log1p(true))**2)/len(true))**0.5\n",
    "    return eval_rmsle\n",
    "\n",
    "#def eval_rmsle(y, pred):\n",
    "#    return mean_squared_error(y, pred)**0.5\n",
    "\n",
    "def auc_to_gini_norm(auc_score):\n",
    "    return 2*auc_score-1\n",
    "\n",
    "def eval_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation matrix \n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "RMSLE = make_scorer(eval_rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for comparing predictions and true data.\n",
    "\n",
    "def compare_result(preds, true):\n",
    "    compare = pd.DataFrame({\"test_id\": true.index,\n",
    "                           \"real_cost\": true,\n",
    "                           \"pred_cost\": preds})\n",
    "    compare = compare[[\"test_id\", \"real_cost\", \"pred_cost\"]].reset_index(drop=True)\n",
    "    \n",
    "    compare[\"error_percent_(%)\"] = np.abs(compare.real_cost - compare.pred_cost) / compare.real_cost * 100\n",
    "    \n",
    "    return compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM Regression and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# model_selection.GridSearchCV\n",
    "# =================================================\n",
    "\n",
    "# 1) Create model\n",
    "#set static parameters\n",
    "params = {'boosting_type': 'dart',\n",
    "          'max_depth' : 5,\n",
    "          'max_bin' : 500,\n",
    "          'learning_rate': 0.1,  # 0.618580\n",
    "          'num_leaves': 22,\n",
    "         }\n",
    "\n",
    "#define XGB Grid Search model\n",
    "model = LGBMRegressor(boosting_type=params['boosting_type']\n",
    "                       )\n",
    "#set parameters\n",
    "gridParams = {\n",
    "    'colsample_bytree' :[0.9, 1.0], #0.6, 0.64, 0.85 /0.8, \n",
    "    'learning_rate': [0.075,0.1], # ,0.005, 0.06/0.05,\n",
    "    'max_depth' :[2,5,10], #2,3,4\n",
    "    'n_estimators': [8,24], #,100\n",
    "    #'num_leaves': [10,20], #2,4,5,6, 100\n",
    "    'objective': ['binary','regression'],\n",
    "    'reg_alpha' : [0.1,0.5], #,1.2\n",
    "    'reg_lambda' : [1.3,1.5], #1, 1.2,1.4\n",
    "    'subsample' :[0.7,0.9], #0.75 \n",
    "}\n",
    "\n",
    "# 3) Run GridSearch\n",
    "grid = GridSearchCV(model, gridParams, verbose=1, cv=5, n_jobs=-1, scoring=RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1920 out of 1920 | elapsed: 15.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 950.6239559650421 seconds\n"
     ]
    }
   ],
   "source": [
    "#grid search experiment\n",
    "start = time.time()\n",
    "\n",
    "grid.fit(x_localtrain, y_localtrain)\n",
    "\n",
    "end = time.time()\n",
    "duration = end-start\n",
    "print (\"It takes {} seconds\".format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.561887739802\n",
      "colsample_bytree: 1.0\n",
      "learning_rate: 0.1\n",
      "max_depth: 10\n",
      "n_estimators: 24\n",
      "objective: 'regression'\n",
      "reg_alpha: 0.5\n",
      "reg_lambda: 1.3\n",
      "subsample: 0.9\n"
     ]
    }
   ],
   "source": [
    "#get/show the best parameters\n",
    "best_parameters, score, _ = min(grid.grid_scores_, key=lambda x: x[1])\n",
    "print('score:', score)\n",
    "\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM RMSLE is : 0.5120897445327701\n",
      "It takes 1.2458980083465576 seconds\n"
     ]
    }
   ],
   "source": [
    "#use best model to predict\n",
    "start = time.time()\n",
    "lgb_regressor = LGBMRegressor(\n",
    "    colsample_bytree=best_parameters[\"colsample_bytree\"],\n",
    "    learning_rate=best_parameters[\"learning_rate\"],\n",
    "    max_depth=best_parameters[\"max_depth\"],\n",
    "    n_estimators=best_parameters[\"n_estimators\"],\n",
    "    objective=best_parameters[\"objective\"],\n",
    "    reg_alpha=best_parameters[\"reg_alpha\"],\n",
    "    reg_lambda=best_parameters[\"reg_lambda\"],\n",
    "    subsample=best_parameters[\"subsample\"]\n",
    ")\n",
    "\n",
    "y_localtrain_log = np.log1p(y_localtrain)\n",
    "\n",
    "model = lgb_regressor.fit(x_localtrain, y_localtrain_log)\n",
    "y_localvalprediction1 = model.predict(x_localval)\n",
    "\n",
    "y_localvalprediction = np.expm1(y_localvalprediction1)\n",
    "\n",
    "rmsle_lgb = eval_rmsle(y_localvalprediction, y_localval)\n",
    "print (\"LGBM RMSLE is : {}\".format(rmsle_lgb))\n",
    "\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print (\"It takes {} seconds\".format(duration))\n",
    "\n",
    "compare_lgb = compare_result(preds=y_localvalprediction, true=y_localval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cross validation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "\n",
    "### Kfold or Stratified Kfold\n",
    "kf=KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "#kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "\n",
    "### Criterion function\n",
    "def auc_to_gini_norm(auc_score):\n",
    "    return 2*auc_score-1\n",
    "\n",
    "def eval_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def eval_rmsle(y, pred):\n",
    "    return mean_squared_error(y, pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modified cross-validation function copied from Yifan's cool kernel. \n",
    "### Source: https://www.kaggle.com/yifanxie/porto-seguro-tutorial-end-to-end-ensemble\n",
    "\n",
    "def cross_validate_sklearn(clf, x_train, y_train, x_test, kf, scale=False, verbose=True):\n",
    "    start_time=time.time()\n",
    "    \n",
    "    # initialise the size of out-of-fold train an test prediction\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "\n",
    "    # use the kfold object to generate the required folds\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        # generate training folds and validation fold\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[test_index, :]\n",
    "        y_train_kf, y_val_kf = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        # perform scaling if required i.e. for linear algorithms\n",
    "        if scale:\n",
    "            scaler = StandardScaler().fit(x_train_kf.values)\n",
    "            x_train_kf_values = scaler.transform(x_train_kf.values)\n",
    "            x_val_kf_values = scaler.transform(x_val_kf.values)\n",
    "            x_test_values = scaler.transform(x_test.values)\n",
    "        else:\n",
    "            x_train_kf_values = x_train_kf.values\n",
    "            x_val_kf_values = x_val_kf.values\n",
    "            x_test_values = x_test.values\n",
    "        \n",
    "        ###################################################\n",
    "        # Please update for your own criterion \n",
    "        ###################################################\n",
    "        # fit the input classifier and perform prediction.\n",
    "        #clf.fit(x_train_kf_values, y_train_kf.values)\n",
    "        clf.fit(x_train_kf_values, np.log1p(y_train_kf.values))\n",
    "\n",
    "        ###################################################\n",
    "        # Please update for your own criterion \n",
    "        ###################################################\n",
    "        #val_pred=clf.predict_proba(x_val_kf_values)[:,1]\n",
    "        val_pred=clf.predict(x_val_kf_values)\n",
    "\n",
    "        train_pred[test_index] += val_pred\n",
    "\n",
    "        ###################################################\n",
    "        # Please update for your own criterion \n",
    "        ###################################################\n",
    "        #y_test_preds = clf.predict_proba(x_test_values)[:,1]\n",
    "        y_test_preds = clf.predict(x_test_values)\n",
    "        test_pred += y_test_preds\n",
    "\n",
    "        ###################################################\n",
    "        # Please update for your own criterion \n",
    "        ###################################################\n",
    "        #fold_auc = roc_auc_score(y_val_kf.values, val_pred)\n",
    "        #fold_gini_norm = auc_to_gini_norm(fold_auc)\n",
    "        fold_rmsle = eval_rmsle(np.log1p(y_val_kf.values), val_pred)\n",
    "\n",
    "        if verbose:\n",
    "            ###################################################\n",
    "            # Please update for your own criterion\n",
    "            ###################################################\n",
    "\n",
    "            #print('fold cv {} AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(i, fold_auc, fold_gini_norm))\n",
    "            print('fold cv {} RMSLE score is {:.6f}'.format(i, fold_rmsle))\n",
    "\n",
    "    test_pred /= kf.n_splits\n",
    "\n",
    "    ###################################################\n",
    "    # Please update for your own criterion\n",
    "    ###################################################\n",
    "    #cv_auc = roc_auc_score(y_train, train_pred)\n",
    "    #cv_gini_norm = auc_to_gini_norm(cv_auc)\n",
    "    cv_rmsle = eval_rmsle(np.log1p(y_train), train_pred)\n",
    "        \n",
    "    #cv_score = [cv_auc, cv_gini_norm]\n",
    "    cv_score = [cv_rmsle]\n",
    "    if verbose:\n",
    "\n",
    "        ###################################################\n",
    "        # Please update for your own criterion\n",
    "        ###################################################\n",
    "        #print('cv AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(cv_auc, cv_gini_norm))\n",
    "        print('cv RMSLE score is {:.6f}'.format(cv_rmsle))\n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    "    return cv_score, train_pred, test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 RMSLE score is 0.519012\n",
      "fold cv 1 RMSLE score is 0.519113\n",
      "fold cv 2 RMSLE score is 0.516890\n",
      "fold cv 3 RMSLE score is 0.515128\n",
      "fold cv 4 RMSLE score is 0.514956\n",
      "cv RMSLE score is 0.517023\n",
      "it takes 1131.644 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(criterion='mse',\n",
    "                            max_depth=5,\n",
    "                            n_estimators=500,\n",
    "                            min_samples_leaf=5,\n",
    "                            min_samples_split=2\n",
    "                           )\n",
    "\n",
    "outcomes =cross_validate_sklearn(rfr, x_train, y_train ,x_test, kf, scale=False, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
