{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test xgboost with Bayesian Optimsation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contributions from:\n",
    "DSEverything - Mean Mix - Math, Geo, Harmonic (LB 0.493) \n",
    "https://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493\n",
    "JdPaletto - Surprised Yet? - Part2 - (LB: 0.503)\n",
    "https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503\n",
    "hklee - weighted mean comparisons, LB 0.497, 1ST\n",
    "https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st\n",
    "\n",
    "Also all comments for changes, encouragement, and forked scripts rock\n",
    "\n",
    "Keep the Surprise Going\n",
    "\"\"\"\n",
    "\n",
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../../mltestdata/05_recruit/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "    }\n",
    "\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "#sure it can be compressed...\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n",
    "\n",
    "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n",
    "\n",
    "# NEW FEATURES FROM JMBULL\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "\n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, dtype in zip(train.columns, train.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        train[c] = train[c].astype(np.float32)\n",
    "\n",
    "for c, dtype in zip(test.columns, test.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        test[c] = test[c].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Golden week flag and Post Golden week flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = [train, test]\n",
    "gw_list = ['2016-04-29','2016-04-30','2016-05-01','2016-05-02','2016-05-03','2016-05-04','2016-05-05','2017-04-29','2017-04-30','2017-05-01','2017-05-02','2017-05-03','2017-05-04','2017-05-05']\n",
    "post_gw_list=['2016-05-06']\n",
    "train['gw_flg'] = 0\n",
    "train['post_gw_flg'] = 0\n",
    "test['gw_flg'] = 0\n",
    "test['post_gw_flg'] = 0\n",
    "update_gw_list = [[\"0\" for i in range(3)] for j in range(len(gw_list))]\n",
    "update_post_gw_list = [[\"0\" for i in range(3)] for j in range(len(post_gw_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "for index, gw_date in enumerate(gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_gw_list[index][col_i]=int(temp_figure)\n",
    "        \n",
    "    #print(\"{}  {}  {}\".format(update_list[index][0],update_list[index][1],update_list[index][2]))\n",
    "    \n",
    "for index, gw_date in enumerate(post_gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_post_gw_list[index][col_i]=int(temp_figure)\n",
    "\n",
    "for dataset in combine:\n",
    "    for index in range(len(update_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_gw_list[index][0],update_gw_list[index][1],update_gw_list[index][2]), 'gw_flg'] = 1\n",
    "        \n",
    "for dataset in combine:\n",
    "    for index in range(len(update_post_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_post_gw_list[index][0],update_post_gw_list[index][1],update_post_gw_list[index][2]), 'post_gw_flg'] = 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "y_train=train['visitors']\n",
    "x_train=train.drop(drop_cols, axis=1)\n",
    "\n",
    "x_test=test.copy()\n",
    "x_test=x_test.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.visitors\n",
    "train_input = train.copy()\n",
    "test_input = test.copy()\n",
    "\n",
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "train_input=train_input.drop(drop_cols, axis=1)\n",
    "test_input=test_input.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation function\n",
    "\n",
    "def rmsle(preds, true):\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n",
    "    return float(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation matrix \n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "RMSLE = make_scorer(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for comparing predictions and true data.\n",
    "def compare_result(preds, true):\n",
    "    compare = pd.DataFrame({\"test_id\": true.index,\n",
    "                           \"real_cost\": true,\n",
    "                           \"pred_cost\": preds})\n",
    "    compare = compare[[\"test_id\", \"real_cost\", \"pred_cost\"]].reset_index(drop=True)\n",
    "    \n",
    "    compare[\"error_percent_(%)\"] = np.abs(compare.real_cost - compare.pred_cost) / compare.real_cost * 100\n",
    "    \n",
    "    return compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_xgb(params, x_train, y_train, x_test, kf,  verbose=True, verbose_eval=50, scoreonly=False):\n",
    "    start_time=time.time()\n",
    "    nround=[]\n",
    "    # the prediction matrix need to contains 3 columns, one for the probability of each class\n",
    "    #train_pred = np.zeros((x_train.shape[0],3))\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "    \n",
    "    # self-defined eval metric\n",
    "    # f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "    # binary error\n",
    "    def feval_rmsle(preds, train_data):\n",
    "        preds = np.expm1(preds)\n",
    "        true = np.expm1(train_data.get_label())\n",
    "        #return 'rmsle', rmsle(true, preds), False\n",
    "\n",
    "        return 'rmsle', rmsle(preds, true), False\n",
    "\n",
    "    # use the k-fold object to enumerate indexes for each training and validation fold\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "\n",
    "        #y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n",
    "        y_train_kf, y_val_kf = np.log1p(y_train[train_index]), np.log1p(y_train[val_index])\n",
    "        x_test_kf=x_test.copy()\n",
    "        \n",
    "        d_train = xgboost.DMatrix(x_train_kf, y_train_kf)\n",
    "        d_val=xgboost.DMatrix(x_val_kf, y_val_kf)\n",
    "        d_test = xgboost.DMatrix(x_test_kf)\n",
    "        \n",
    "        watchlist= [(d_train, \"train\"), (d_val, 'val')]\n",
    "        bst = xgboost.train(params=params, \n",
    "                            dtrain=d_train, \n",
    "                            num_boost_round=8000, \n",
    "                            early_stopping_rounds=100,\n",
    "                            evals=watchlist, \n",
    "                            verbose_eval=verbose_eval)        \n",
    "        \n",
    "#        y_val_kf_preds=bst.predict(d_val, ntree_limit=bst.best_ntree_limit)\n",
    "        y_val_kf_preds=np.expm1(bst.predict(d_val, ntree_limit=bst.best_ntree_limit))\n",
    "        nround.append(bst.best_ntree_limit)\n",
    "        \n",
    "        train_pred[val_index] += y_val_kf_preds\n",
    "#        test_pred += np.expm1((bst.predict(x_test, ntree_limit=bst.best_ntree_limit)))\n",
    "        test_pred += np.expm1(bst.predict(d_test))\n",
    "        \n",
    "        \n",
    "        #fold_cv = log_loss(y_val_kf.values, y_val_kf_preds)\n",
    "        #fold_rmsle = rmsle(np.expm1(train_pred[val_index]),np.expm1(y_val_kf.values))\n",
    "        fold_rmsle = rmsle(train_pred[val_index],np.expm1(y_val_kf.values))\n",
    "        fold_cv = fold_rmsle\n",
    "        \n",
    "        if verbose:\n",
    "            print('fold cv {} rmsle score is {:.6f}'.format(i, fold_cv))\n",
    "\n",
    "    test_pred = test_pred / kf.n_splits\n",
    "    #cv_score = log_loss(y_train, train_pred)\n",
    "    #cv_score = rmsle(np.expm1(train_pred), y_train)\n",
    "    cv_score = rmsle(train_pred, y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print('cv rmsle score is {:.6f}'.format(cv_score))    \n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    " \n",
    "    if scoreonly:\n",
    "        #return cv_score # for the purpose of bayesian optimisation, we only need to return the CV score\n",
    "        return cv_score\n",
    "    else:\n",
    "        return (cv_score,train_pred,test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do 3 fold CV here so that we save some running time on Kaggle Kernel\n",
    "kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[0]\ttrain-rmse:2.32524\tval-rmse:2.32558\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.54\tval-rmse:0.5398\n",
      "[100]\ttrain-rmse:0.503041\tval-rmse:0.504951\n",
      "[150]\ttrain-rmse:0.49913\tval-rmse:0.502178\n",
      "[200]\ttrain-rmse:0.496437\tval-rmse:0.500558\n",
      "[250]\ttrain-rmse:0.494607\tval-rmse:0.4996\n",
      "[300]\ttrain-rmse:0.492879\tval-rmse:0.498688\n",
      "[350]\ttrain-rmse:0.491506\tval-rmse:0.49803\n",
      "[400]\ttrain-rmse:0.490117\tval-rmse:0.497413\n",
      "[450]\ttrain-rmse:0.489177\tval-rmse:0.496986\n",
      "[500]\ttrain-rmse:0.488209\tval-rmse:0.49654\n",
      "[550]\ttrain-rmse:0.487309\tval-rmse:0.496165\n",
      "[600]\ttrain-rmse:0.486394\tval-rmse:0.495819\n",
      "[650]\ttrain-rmse:0.485371\tval-rmse:0.495441\n",
      "[700]\ttrain-rmse:0.484624\tval-rmse:0.495213\n",
      "[750]\ttrain-rmse:0.483789\tval-rmse:0.494917\n",
      "[800]\ttrain-rmse:0.482926\tval-rmse:0.494607\n",
      "[850]\ttrain-rmse:0.482153\tval-rmse:0.49425\n",
      "[900]\ttrain-rmse:0.481474\tval-rmse:0.494048\n",
      "[950]\ttrain-rmse:0.48088\tval-rmse:0.493874\n",
      "[1000]\ttrain-rmse:0.480192\tval-rmse:0.493671\n",
      "[1050]\ttrain-rmse:0.479578\tval-rmse:0.493509\n",
      "[1100]\ttrain-rmse:0.479002\tval-rmse:0.493341\n",
      "[1150]\ttrain-rmse:0.478251\tval-rmse:0.493071\n",
      "[1200]\ttrain-rmse:0.477687\tval-rmse:0.492893\n",
      "[1250]\ttrain-rmse:0.477083\tval-rmse:0.492713\n",
      "[1300]\ttrain-rmse:0.476469\tval-rmse:0.492552\n",
      "[1350]\ttrain-rmse:0.47581\tval-rmse:0.492411\n",
      "[1400]\ttrain-rmse:0.475317\tval-rmse:0.492324\n",
      "[1450]\ttrain-rmse:0.474598\tval-rmse:0.492076\n",
      "[1500]\ttrain-rmse:0.473966\tval-rmse:0.491864\n",
      "[1550]\ttrain-rmse:0.473453\tval-rmse:0.49178\n",
      "[1600]\ttrain-rmse:0.472919\tval-rmse:0.491665\n",
      "[1650]\ttrain-rmse:0.472314\tval-rmse:0.491519\n",
      "[1700]\ttrain-rmse:0.471813\tval-rmse:0.491392\n",
      "[1750]\ttrain-rmse:0.471335\tval-rmse:0.491269\n",
      "[1800]\ttrain-rmse:0.470832\tval-rmse:0.49115\n",
      "[1850]\ttrain-rmse:0.470218\tval-rmse:0.491017\n",
      "[1900]\ttrain-rmse:0.469733\tval-rmse:0.490947\n",
      "[1950]\ttrain-rmse:0.469141\tval-rmse:0.490803\n",
      "[2000]\ttrain-rmse:0.468507\tval-rmse:0.490625\n",
      "[2050]\ttrain-rmse:0.467944\tval-rmse:0.49055\n",
      "[2100]\ttrain-rmse:0.467372\tval-rmse:0.490386\n",
      "[2150]\ttrain-rmse:0.466796\tval-rmse:0.490223\n",
      "[2200]\ttrain-rmse:0.466214\tval-rmse:0.490091\n",
      "[2250]\ttrain-rmse:0.465694\tval-rmse:0.489981\n",
      "[2300]\ttrain-rmse:0.465132\tval-rmse:0.489836\n",
      "[2350]\ttrain-rmse:0.464684\tval-rmse:0.489753\n",
      "[2400]\ttrain-rmse:0.46414\tval-rmse:0.489619\n",
      "[2450]\ttrain-rmse:0.463596\tval-rmse:0.489543\n",
      "[2500]\ttrain-rmse:0.46309\tval-rmse:0.489476\n",
      "[2550]\ttrain-rmse:0.462608\tval-rmse:0.489402\n",
      "[2600]\ttrain-rmse:0.462081\tval-rmse:0.489362\n",
      "[2650]\ttrain-rmse:0.461609\tval-rmse:0.489262\n",
      "[2700]\ttrain-rmse:0.461163\tval-rmse:0.48919\n",
      "[2750]\ttrain-rmse:0.460717\tval-rmse:0.489164\n",
      "[2800]\ttrain-rmse:0.460249\tval-rmse:0.489099\n",
      "[2850]\ttrain-rmse:0.459747\tval-rmse:0.489031\n",
      "[2900]\ttrain-rmse:0.459201\tval-rmse:0.488921\n",
      "[2950]\ttrain-rmse:0.458753\tval-rmse:0.488892\n",
      "[3000]\ttrain-rmse:0.458288\tval-rmse:0.488887\n",
      "[3050]\ttrain-rmse:0.457892\tval-rmse:0.488859\n",
      "[3100]\ttrain-rmse:0.457398\tval-rmse:0.488719\n",
      "[3150]\ttrain-rmse:0.456898\tval-rmse:0.488673\n",
      "[3200]\ttrain-rmse:0.456476\tval-rmse:0.488656\n",
      "[3250]\ttrain-rmse:0.456016\tval-rmse:0.488604\n",
      "[3300]\ttrain-rmse:0.455542\tval-rmse:0.488545\n",
      "[3350]\ttrain-rmse:0.455061\tval-rmse:0.488523\n",
      "[3400]\ttrain-rmse:0.45465\tval-rmse:0.488506\n",
      "[3450]\ttrain-rmse:0.45427\tval-rmse:0.488505\n",
      "[3500]\ttrain-rmse:0.45386\tval-rmse:0.488475\n",
      "[3550]\ttrain-rmse:0.453413\tval-rmse:0.488429\n",
      "[3600]\ttrain-rmse:0.453055\tval-rmse:0.488417\n",
      "[3650]\ttrain-rmse:0.452643\tval-rmse:0.488393\n",
      "[3700]\ttrain-rmse:0.452306\tval-rmse:0.488399\n",
      "[3750]\ttrain-rmse:0.451946\tval-rmse:0.488341\n",
      "[3800]\ttrain-rmse:0.45153\tval-rmse:0.488303\n",
      "[3850]\ttrain-rmse:0.451183\tval-rmse:0.48831\n",
      "[3900]\ttrain-rmse:0.450806\tval-rmse:0.488257\n",
      "[3950]\ttrain-rmse:0.450355\tval-rmse:0.488253\n",
      "[4000]\ttrain-rmse:0.449992\tval-rmse:0.488255\n",
      "[4050]\ttrain-rmse:0.449652\tval-rmse:0.488229\n",
      "[4100]\ttrain-rmse:0.449199\tval-rmse:0.488188\n",
      "[4150]\ttrain-rmse:0.44883\tval-rmse:0.488162\n",
      "[4200]\ttrain-rmse:0.448519\tval-rmse:0.488112\n",
      "[4250]\ttrain-rmse:0.448139\tval-rmse:0.48809\n",
      "[4300]\ttrain-rmse:0.447725\tval-rmse:0.488061\n",
      "[4350]\ttrain-rmse:0.447387\tval-rmse:0.488049\n",
      "[4400]\ttrain-rmse:0.447033\tval-rmse:0.488023\n",
      "[4450]\ttrain-rmse:0.446675\tval-rmse:0.488002\n",
      "[4500]\ttrain-rmse:0.446343\tval-rmse:0.488016\n",
      "[4550]\ttrain-rmse:0.445983\tval-rmse:0.487973\n",
      "[4600]\ttrain-rmse:0.445627\tval-rmse:0.487943\n",
      "[4650]\ttrain-rmse:0.445354\tval-rmse:0.487941\n",
      "[4700]\ttrain-rmse:0.445022\tval-rmse:0.487926\n",
      "[4750]\ttrain-rmse:0.444687\tval-rmse:0.487905\n",
      "[4800]\ttrain-rmse:0.444358\tval-rmse:0.487907\n",
      "[4850]\ttrain-rmse:0.444024\tval-rmse:0.487922\n",
      "Stopping. Best iteration:\n",
      "[4758]\ttrain-rmse:0.444632\tval-rmse:0.487897\n",
      "\n",
      "[0]\ttrain-rmse:2.32335\tval-rmse:2.32577\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.537014\tval-rmse:0.54362\n",
      "[100]\ttrain-rmse:0.500873\tval-rmse:0.509013\n",
      "[150]\ttrain-rmse:0.497057\tval-rmse:0.506363\n",
      "[200]\ttrain-rmse:0.494651\tval-rmse:0.504948\n",
      "[250]\ttrain-rmse:0.492795\tval-rmse:0.503931\n",
      "[300]\ttrain-rmse:0.491099\tval-rmse:0.502987\n",
      "[350]\ttrain-rmse:0.489637\tval-rmse:0.502258\n",
      "[400]\ttrain-rmse:0.488732\tval-rmse:0.501881\n",
      "[450]\ttrain-rmse:0.487781\tval-rmse:0.501465\n",
      "[500]\ttrain-rmse:0.487233\tval-rmse:0.501207\n",
      "[550]\ttrain-rmse:0.486805\tval-rmse:0.501035\n",
      "[600]\ttrain-rmse:0.48616\tval-rmse:0.500789\n",
      "[650]\ttrain-rmse:0.485733\tval-rmse:0.500623\n",
      "[700]\ttrain-rmse:0.485184\tval-rmse:0.500433\n",
      "[750]\ttrain-rmse:0.484547\tval-rmse:0.500214\n",
      "[800]\ttrain-rmse:0.484103\tval-rmse:0.500061\n",
      "[850]\ttrain-rmse:0.483796\tval-rmse:0.499959\n",
      "[900]\ttrain-rmse:0.483399\tval-rmse:0.499815\n",
      "[950]\ttrain-rmse:0.482935\tval-rmse:0.499641\n",
      "[1000]\ttrain-rmse:0.482359\tval-rmse:0.499402\n",
      "[1050]\ttrain-rmse:0.481871\tval-rmse:0.49925\n",
      "[1100]\ttrain-rmse:0.481558\tval-rmse:0.49915\n",
      "[1150]\ttrain-rmse:0.480938\tval-rmse:0.498893\n",
      "[1200]\ttrain-rmse:0.480466\tval-rmse:0.498723\n",
      "[1250]\ttrain-rmse:0.479974\tval-rmse:0.498469\n",
      "[1300]\ttrain-rmse:0.479581\tval-rmse:0.49837\n",
      "[1350]\ttrain-rmse:0.479199\tval-rmse:0.498229\n",
      "[1400]\ttrain-rmse:0.478856\tval-rmse:0.498138\n",
      "[1450]\ttrain-rmse:0.478374\tval-rmse:0.497939\n",
      "[1500]\ttrain-rmse:0.477913\tval-rmse:0.49781\n",
      "[1550]\ttrain-rmse:0.477561\tval-rmse:0.497683\n",
      "[1600]\ttrain-rmse:0.477243\tval-rmse:0.49763\n",
      "[1650]\ttrain-rmse:0.476935\tval-rmse:0.497518\n",
      "[1700]\ttrain-rmse:0.476671\tval-rmse:0.497435\n",
      "[1750]\ttrain-rmse:0.47627\tval-rmse:0.497268\n",
      "[1800]\ttrain-rmse:0.475949\tval-rmse:0.497191\n",
      "[1850]\ttrain-rmse:0.475524\tval-rmse:0.497074\n",
      "[1900]\ttrain-rmse:0.47527\tval-rmse:0.497014\n",
      "[1950]\ttrain-rmse:0.474888\tval-rmse:0.496959\n",
      "[2000]\ttrain-rmse:0.474601\tval-rmse:0.496911\n",
      "[2050]\ttrain-rmse:0.47425\tval-rmse:0.496829\n",
      "[2100]\ttrain-rmse:0.473928\tval-rmse:0.496777\n",
      "[2150]\ttrain-rmse:0.473566\tval-rmse:0.496655\n",
      "[2200]\ttrain-rmse:0.473308\tval-rmse:0.496617\n",
      "[2250]\ttrain-rmse:0.472987\tval-rmse:0.496569\n",
      "[2300]\ttrain-rmse:0.472703\tval-rmse:0.496507\n",
      "[2350]\ttrain-rmse:0.472352\tval-rmse:0.496442\n",
      "[2400]\ttrain-rmse:0.472109\tval-rmse:0.496422\n",
      "[2450]\ttrain-rmse:0.471787\tval-rmse:0.496329\n",
      "[2500]\ttrain-rmse:0.471499\tval-rmse:0.49624\n",
      "[2550]\ttrain-rmse:0.471243\tval-rmse:0.496211\n",
      "[2600]\ttrain-rmse:0.471022\tval-rmse:0.496126\n",
      "[2650]\ttrain-rmse:0.470689\tval-rmse:0.495998\n",
      "[2700]\ttrain-rmse:0.470436\tval-rmse:0.495939\n",
      "[2750]\ttrain-rmse:0.470143\tval-rmse:0.49587\n",
      "[2800]\ttrain-rmse:0.469939\tval-rmse:0.49584\n",
      "[2850]\ttrain-rmse:0.469661\tval-rmse:0.495815\n",
      "[2900]\ttrain-rmse:0.469437\tval-rmse:0.495784\n",
      "[2950]\ttrain-rmse:0.469184\tval-rmse:0.495721\n",
      "[3000]\ttrain-rmse:0.46883\tval-rmse:0.495566\n",
      "[3050]\ttrain-rmse:0.468515\tval-rmse:0.495475\n",
      "[3100]\ttrain-rmse:0.468229\tval-rmse:0.495467\n",
      "[3150]\ttrain-rmse:0.468024\tval-rmse:0.495442\n",
      "[3200]\ttrain-rmse:0.467757\tval-rmse:0.49535\n",
      "[3250]\ttrain-rmse:0.467554\tval-rmse:0.495305\n",
      "[3300]\ttrain-rmse:0.46729\tval-rmse:0.495241\n",
      "[3350]\ttrain-rmse:0.467135\tval-rmse:0.495219\n",
      "[3400]\ttrain-rmse:0.466874\tval-rmse:0.495161\n",
      "[3450]\ttrain-rmse:0.466684\tval-rmse:0.495127\n",
      "[3500]\ttrain-rmse:0.466432\tval-rmse:0.495112\n",
      "[3550]\ttrain-rmse:0.466231\tval-rmse:0.495065\n",
      "[3600]\ttrain-rmse:0.465923\tval-rmse:0.494994\n",
      "[3650]\ttrain-rmse:0.465711\tval-rmse:0.494985\n",
      "[3700]\ttrain-rmse:0.465534\tval-rmse:0.494981\n",
      "[3750]\ttrain-rmse:0.465374\tval-rmse:0.494961\n",
      "[3800]\ttrain-rmse:0.465155\tval-rmse:0.494908\n",
      "[3850]\ttrain-rmse:0.46497\tval-rmse:0.494891\n",
      "[3900]\ttrain-rmse:0.464778\tval-rmse:0.494819\n",
      "[3950]\ttrain-rmse:0.464506\tval-rmse:0.494752\n",
      "[4000]\ttrain-rmse:0.464281\tval-rmse:0.494724\n",
      "[4050]\ttrain-rmse:0.4641\tval-rmse:0.494698\n",
      "[4100]\ttrain-rmse:0.463915\tval-rmse:0.494672\n",
      "[4150]\ttrain-rmse:0.463739\tval-rmse:0.494628\n",
      "[4200]\ttrain-rmse:0.463608\tval-rmse:0.494622\n",
      "[4250]\ttrain-rmse:0.463479\tval-rmse:0.494621\n",
      "[4300]\ttrain-rmse:0.463224\tval-rmse:0.494616\n",
      "[4350]\ttrain-rmse:0.462977\tval-rmse:0.494555\n",
      "[4400]\ttrain-rmse:0.462832\tval-rmse:0.49452\n",
      "[4450]\ttrain-rmse:0.462618\tval-rmse:0.494494\n",
      "[4500]\ttrain-rmse:0.462413\tval-rmse:0.494446\n",
      "[4550]\ttrain-rmse:0.46223\tval-rmse:0.494425\n",
      "[4600]\ttrain-rmse:0.462053\tval-rmse:0.494422\n",
      "[4650]\ttrain-rmse:0.461924\tval-rmse:0.494413\n",
      "[4700]\ttrain-rmse:0.461773\tval-rmse:0.494387\n",
      "[4750]\ttrain-rmse:0.461592\tval-rmse:0.494363\n",
      "[4800]\ttrain-rmse:0.46135\tval-rmse:0.494317\n",
      "[4850]\ttrain-rmse:0.461152\tval-rmse:0.494308\n",
      "[4900]\ttrain-rmse:0.460985\tval-rmse:0.494287\n",
      "[4950]\ttrain-rmse:0.460851\tval-rmse:0.494272\n",
      "[5000]\ttrain-rmse:0.460681\tval-rmse:0.494229\n",
      "[5050]\ttrain-rmse:0.460528\tval-rmse:0.494203\n",
      "[5100]\ttrain-rmse:0.46031\tval-rmse:0.494144\n",
      "[5150]\ttrain-rmse:0.460164\tval-rmse:0.494112\n",
      "[5200]\ttrain-rmse:0.460002\tval-rmse:0.494043\n",
      "[5250]\ttrain-rmse:0.459771\tval-rmse:0.494018\n",
      "[5300]\ttrain-rmse:0.459644\tval-rmse:0.493998\n",
      "[5350]\ttrain-rmse:0.459475\tval-rmse:0.493967\n",
      "[5400]\ttrain-rmse:0.459364\tval-rmse:0.493968\n",
      "[5450]\ttrain-rmse:0.459128\tval-rmse:0.493918\n",
      "[5500]\ttrain-rmse:0.458974\tval-rmse:0.493852\n",
      "[5550]\ttrain-rmse:0.458735\tval-rmse:0.493812\n",
      "[5600]\ttrain-rmse:0.458589\tval-rmse:0.493808\n",
      "[5650]\ttrain-rmse:0.458459\tval-rmse:0.493827\n",
      "Stopping. Best iteration:\n",
      "[5565]\ttrain-rmse:0.458702\tval-rmse:0.493803\n",
      "\n",
      "[0]\ttrain-rmse:2.32499\tval-rmse:2.32226\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.541293\tval-rmse:0.539269\n",
      "[100]\ttrain-rmse:0.504149\tval-rmse:0.502423\n",
      "[150]\ttrain-rmse:0.501219\tval-rmse:0.500137\n",
      "[200]\ttrain-rmse:0.499547\tval-rmse:0.498969\n",
      "[250]\ttrain-rmse:0.498718\tval-rmse:0.498454\n",
      "[300]\ttrain-rmse:0.497734\tval-rmse:0.497863\n",
      "[350]\ttrain-rmse:0.496999\tval-rmse:0.497439\n",
      "[400]\ttrain-rmse:0.496176\tval-rmse:0.497071\n",
      "[450]\ttrain-rmse:0.495551\tval-rmse:0.496726\n",
      "[500]\ttrain-rmse:0.494674\tval-rmse:0.4963\n",
      "[550]\ttrain-rmse:0.494046\tval-rmse:0.496075\n",
      "[600]\ttrain-rmse:0.493368\tval-rmse:0.495757\n",
      "[650]\ttrain-rmse:0.492752\tval-rmse:0.495521\n",
      "[700]\ttrain-rmse:0.492153\tval-rmse:0.495294\n",
      "[750]\ttrain-rmse:0.491603\tval-rmse:0.495033\n",
      "[800]\ttrain-rmse:0.491037\tval-rmse:0.494764\n",
      "[850]\ttrain-rmse:0.490379\tval-rmse:0.494451\n",
      "[900]\ttrain-rmse:0.489482\tval-rmse:0.494006\n",
      "[950]\ttrain-rmse:0.488897\tval-rmse:0.493806\n",
      "[1000]\ttrain-rmse:0.488204\tval-rmse:0.493548\n",
      "[1050]\ttrain-rmse:0.487574\tval-rmse:0.493359\n",
      "[1100]\ttrain-rmse:0.48703\tval-rmse:0.493171\n",
      "[1150]\ttrain-rmse:0.486223\tval-rmse:0.492841\n",
      "[1200]\ttrain-rmse:0.485585\tval-rmse:0.492647\n",
      "[1250]\ttrain-rmse:0.485102\tval-rmse:0.492455\n",
      "[1300]\ttrain-rmse:0.48461\tval-rmse:0.492308\n",
      "[1350]\ttrain-rmse:0.484103\tval-rmse:0.492093\n",
      "[1400]\ttrain-rmse:0.483523\tval-rmse:0.491867\n",
      "[1450]\ttrain-rmse:0.483055\tval-rmse:0.491705\n",
      "[1500]\ttrain-rmse:0.482723\tval-rmse:0.491584\n",
      "[1550]\ttrain-rmse:0.48229\tval-rmse:0.491443\n",
      "[1600]\ttrain-rmse:0.481863\tval-rmse:0.491328\n",
      "[1650]\ttrain-rmse:0.48154\tval-rmse:0.491266\n",
      "[1700]\ttrain-rmse:0.481094\tval-rmse:0.491141\n",
      "[1750]\ttrain-rmse:0.480782\tval-rmse:0.491029\n",
      "[1800]\ttrain-rmse:0.480382\tval-rmse:0.490901\n",
      "[1850]\ttrain-rmse:0.480002\tval-rmse:0.490792\n",
      "[1900]\ttrain-rmse:0.479683\tval-rmse:0.490725\n",
      "[1950]\ttrain-rmse:0.479351\tval-rmse:0.490647\n",
      "[2000]\ttrain-rmse:0.478956\tval-rmse:0.490537\n",
      "[2050]\ttrain-rmse:0.47864\tval-rmse:0.490448\n",
      "[2100]\ttrain-rmse:0.47824\tval-rmse:0.490259\n",
      "[2150]\ttrain-rmse:0.47795\tval-rmse:0.490166\n",
      "[2200]\ttrain-rmse:0.477608\tval-rmse:0.490098\n",
      "[2250]\ttrain-rmse:0.47731\tval-rmse:0.49007\n",
      "[2300]\ttrain-rmse:0.477077\tval-rmse:0.490052\n",
      "[2350]\ttrain-rmse:0.476718\tval-rmse:0.489993\n",
      "[2400]\ttrain-rmse:0.476397\tval-rmse:0.489915\n",
      "[2450]\ttrain-rmse:0.476233\tval-rmse:0.489901\n",
      "[2500]\ttrain-rmse:0.47595\tval-rmse:0.489827\n",
      "[2550]\ttrain-rmse:0.475759\tval-rmse:0.489787\n",
      "[2600]\ttrain-rmse:0.475443\tval-rmse:0.489739\n",
      "[2650]\ttrain-rmse:0.475103\tval-rmse:0.489699\n",
      "[2700]\ttrain-rmse:0.474839\tval-rmse:0.489657\n",
      "[2750]\ttrain-rmse:0.474566\tval-rmse:0.489631\n",
      "[2800]\ttrain-rmse:0.474287\tval-rmse:0.489572\n",
      "[2850]\ttrain-rmse:0.47408\tval-rmse:0.489525\n",
      "[2900]\ttrain-rmse:0.473802\tval-rmse:0.489474\n",
      "[2950]\ttrain-rmse:0.473536\tval-rmse:0.489402\n",
      "[3000]\ttrain-rmse:0.473302\tval-rmse:0.489378\n",
      "[3050]\ttrain-rmse:0.473025\tval-rmse:0.489334\n",
      "[3100]\ttrain-rmse:0.472753\tval-rmse:0.489257\n",
      "[3150]\ttrain-rmse:0.472503\tval-rmse:0.489239\n",
      "[3200]\ttrain-rmse:0.472315\tval-rmse:0.489235\n",
      "[3250]\ttrain-rmse:0.47211\tval-rmse:0.489225\n",
      "[3300]\ttrain-rmse:0.471876\tval-rmse:0.489182\n",
      "[3350]\ttrain-rmse:0.471669\tval-rmse:0.489138\n",
      "[3400]\ttrain-rmse:0.471352\tval-rmse:0.489082\n",
      "[3450]\ttrain-rmse:0.471131\tval-rmse:0.489042\n",
      "[3500]\ttrain-rmse:0.470817\tval-rmse:0.488965\n",
      "[3550]\ttrain-rmse:0.470601\tval-rmse:0.488909\n",
      "[3600]\ttrain-rmse:0.47034\tval-rmse:0.488881\n",
      "[3650]\ttrain-rmse:0.469976\tval-rmse:0.488772\n",
      "[3700]\ttrain-rmse:0.469619\tval-rmse:0.488723\n",
      "[3750]\ttrain-rmse:0.469359\tval-rmse:0.488698\n",
      "[3800]\ttrain-rmse:0.468947\tval-rmse:0.488599\n",
      "[3850]\ttrain-rmse:0.46859\tval-rmse:0.488487\n",
      "[3900]\ttrain-rmse:0.468167\tval-rmse:0.488364\n",
      "[3950]\ttrain-rmse:0.467845\tval-rmse:0.488305\n",
      "[4000]\ttrain-rmse:0.467563\tval-rmse:0.488268\n",
      "[4050]\ttrain-rmse:0.467236\tval-rmse:0.488251\n",
      "[4100]\ttrain-rmse:0.466938\tval-rmse:0.488202\n",
      "[4150]\ttrain-rmse:0.466598\tval-rmse:0.488098\n",
      "[4200]\ttrain-rmse:0.466305\tval-rmse:0.488003\n",
      "[4250]\ttrain-rmse:0.466042\tval-rmse:0.487949\n",
      "[4300]\ttrain-rmse:0.465718\tval-rmse:0.487838\n",
      "[4350]\ttrain-rmse:0.46551\tval-rmse:0.487796\n",
      "[4400]\ttrain-rmse:0.465247\tval-rmse:0.487789\n",
      "[4450]\ttrain-rmse:0.464962\tval-rmse:0.487716\n",
      "[4500]\ttrain-rmse:0.46454\tval-rmse:0.487621\n",
      "[4550]\ttrain-rmse:0.464246\tval-rmse:0.487567\n",
      "[4600]\ttrain-rmse:0.463993\tval-rmse:0.487525\n",
      "[4650]\ttrain-rmse:0.463655\tval-rmse:0.487468\n",
      "[4700]\ttrain-rmse:0.463388\tval-rmse:0.487399\n",
      "[4750]\ttrain-rmse:0.463086\tval-rmse:0.487306\n",
      "[4800]\ttrain-rmse:0.462806\tval-rmse:0.487275\n",
      "[4850]\ttrain-rmse:0.462424\tval-rmse:0.487202\n",
      "[4900]\ttrain-rmse:0.462118\tval-rmse:0.487189\n",
      "[4950]\ttrain-rmse:0.461758\tval-rmse:0.487083\n",
      "[5000]\ttrain-rmse:0.461425\tval-rmse:0.487013\n",
      "[5050]\ttrain-rmse:0.460995\tval-rmse:0.486898\n",
      "[5100]\ttrain-rmse:0.460675\tval-rmse:0.486836\n",
      "[5150]\ttrain-rmse:0.460305\tval-rmse:0.486804\n",
      "[5200]\ttrain-rmse:0.459914\tval-rmse:0.486716\n",
      "[5250]\ttrain-rmse:0.45953\tval-rmse:0.486662\n",
      "[5300]\ttrain-rmse:0.459186\tval-rmse:0.486625\n",
      "[5350]\ttrain-rmse:0.458817\tval-rmse:0.486523\n",
      "[5400]\ttrain-rmse:0.458479\tval-rmse:0.486442\n",
      "[5450]\ttrain-rmse:0.458144\tval-rmse:0.486411\n",
      "[5500]\ttrain-rmse:0.457741\tval-rmse:0.486338\n",
      "[5550]\ttrain-rmse:0.457437\tval-rmse:0.486252\n",
      "[5600]\ttrain-rmse:0.457064\tval-rmse:0.486239\n",
      "[5650]\ttrain-rmse:0.456689\tval-rmse:0.486156\n",
      "[5700]\ttrain-rmse:0.456293\tval-rmse:0.486093\n",
      "[5750]\ttrain-rmse:0.455861\tval-rmse:0.48602\n",
      "[5800]\ttrain-rmse:0.455515\tval-rmse:0.486007\n",
      "[5850]\ttrain-rmse:0.455089\tval-rmse:0.485908\n",
      "[5900]\ttrain-rmse:0.454755\tval-rmse:0.485852\n",
      "[5950]\ttrain-rmse:0.454371\tval-rmse:0.485771\n",
      "[6000]\ttrain-rmse:0.454076\tval-rmse:0.485774\n",
      "[6050]\ttrain-rmse:0.453622\tval-rmse:0.485686\n",
      "[6100]\ttrain-rmse:0.453288\tval-rmse:0.485629\n",
      "[6150]\ttrain-rmse:0.452861\tval-rmse:0.485599\n",
      "[6200]\ttrain-rmse:0.452431\tval-rmse:0.4855\n",
      "[6250]\ttrain-rmse:0.452038\tval-rmse:0.485452\n",
      "[6300]\ttrain-rmse:0.451712\tval-rmse:0.485402\n",
      "[6350]\ttrain-rmse:0.451357\tval-rmse:0.48536\n",
      "[6400]\ttrain-rmse:0.451023\tval-rmse:0.48533\n",
      "[6450]\ttrain-rmse:0.450703\tval-rmse:0.485313\n",
      "[6500]\ttrain-rmse:0.45032\tval-rmse:0.485252\n",
      "[6550]\ttrain-rmse:0.450027\tval-rmse:0.485209\n",
      "[6600]\ttrain-rmse:0.449695\tval-rmse:0.485164\n",
      "[6650]\ttrain-rmse:0.449394\tval-rmse:0.485152\n",
      "[6700]\ttrain-rmse:0.449033\tval-rmse:0.48512\n",
      "[6750]\ttrain-rmse:0.448704\tval-rmse:0.485085\n",
      "[6800]\ttrain-rmse:0.448349\tval-rmse:0.485064\n",
      "[6850]\ttrain-rmse:0.448034\tval-rmse:0.485025\n",
      "[6900]\ttrain-rmse:0.447757\tval-rmse:0.485006\n",
      "[6950]\ttrain-rmse:0.447405\tval-rmse:0.485006\n",
      "[7000]\ttrain-rmse:0.447063\tval-rmse:0.484977\n",
      "[7050]\ttrain-rmse:0.446756\tval-rmse:0.484987\n",
      "Stopping. Best iteration:\n",
      "[6987]\ttrain-rmse:0.447157\tval-rmse:0.484963\n",
      "\n",
      "cv score is 0.488903\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"objective\" : \"reg:linear\",\n",
    "    #\"num_class\" : 3,\n",
    "    #\"tree_method\" : \"hist\",\n",
    "    \"eval_metric\" : \"rmse\",\n",
    "    \"nthread\": 4,\n",
    "    \"seed\" : 0,\n",
    "    'silent': 1,\n",
    "\n",
    "    \"eta\":0.05,  # default 0.3\n",
    "    \"max_depth\" : 5, # default 6\n",
    "    \"subsample\" : 0.8, # default 1\n",
    "    \"colsample_bytree\" : 0.6, # default 1\n",
    "    \"gamma\": 0.5\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "cv_score =cross_validate_xgb(xgb_params, train_input, y, test_input, kf, verbose=False, verbose_eval=50, scoreonly=True)\n",
    "\n",
    "print('cv score is {:.6f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimsation - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'max_depth':(4,10),\n",
    "        'learning_rate':(0.05,0.3),\n",
    "        'subsample': (0.4, 1),\n",
    "        'colsample_bytree': (0.4, 1),\n",
    "        'gamma': (0.001, 10.0),\n",
    "        'min_child_weight': (0, 20),\n",
    "        'max_delta_step': (0, 10),\n",
    "        'n_estimators': (10, 25),\n",
    "        'min_samples_split': (2, 20),\n",
    "        'max_features': (0.1, 0.999)\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(xgb_wrapper)\n",
    "def xgbcv_func(max_depth, learning_rate, subsample, \n",
    "               colsample_bytree, gamma, min_child_weight, \n",
    "               max_delta_step, n_estimators, \n",
    "               min_samples_split, max_features,nthread=4, seed=0):\n",
    "    params = {\n",
    "        \"objective\" : \"reg:linear\",\n",
    "        #\"num_class\" : 3,\n",
    "        #\"tree_method\" : \"hist\",\n",
    "        \"eval_metric\" : \"rmse\",\n",
    "        \"nthread\": nthread,\n",
    "        \"seed\" : 0,\n",
    "        'silent': 1,\n",
    "\n",
    "        \"eta\":learning_rate,  # default 0.3\n",
    "        \"max_depth\" : int(max_depth), # default 6\n",
    "        \"subsample\" : subsample, # default 1\n",
    "        \"colsample_bytree\" : colsample_bytree, # default 1\n",
    "\n",
    "        'gamma': gamma,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'max_delta_step': max_delta_step,\n",
    "        'n_estimators': n_estimators,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'max_features': max_features    \n",
    "\n",
    "    }\n",
    "    \n",
    "    # for a more ideal out-of-fold model prediction for this dataset, we use 10-fold CV\n",
    "    kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "    \n",
    "    # we will disable all the verbose setting in this functional call, so that we don't have too much information \n",
    "    # to read during the bayesian optimisation process.\n",
    "    return 1-cross_validate_xgb(params, train_input, y, test_input, kf, verbose=False, verbose_eval=False, scoreonly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bo=BayesianOptimization(xgbcv_func, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   learning_rate |   max_delta_step |   max_depth |   max_features |   min_child_weight |   min_samples_split |   n_estimators |   subsample | \n",
      "    1 | 19m27s | \u001b[35m   0.50939\u001b[0m | \u001b[32m            0.4982\u001b[0m | \u001b[32m   1.4984\u001b[0m | \u001b[32m         0.2771\u001b[0m | \u001b[32m          0.1868\u001b[0m | \u001b[32m     5.3416\u001b[0m | \u001b[32m        0.2608\u001b[0m | \u001b[32m           17.7966\u001b[0m | \u001b[32m            11.8738\u001b[0m | \u001b[32m       14.8328\u001b[0m | \u001b[32m     0.6453\u001b[0m | \n",
      "    2 | 36m04s | \u001b[35m   0.51026\u001b[0m | \u001b[32m            0.4166\u001b[0m | \u001b[32m   4.4879\u001b[0m | \u001b[32m         0.2051\u001b[0m | \u001b[32m          2.7165\u001b[0m | \u001b[32m     9.9061\u001b[0m | \u001b[32m        0.9401\u001b[0m | \u001b[32m            6.1321\u001b[0m | \u001b[32m             9.0535\u001b[0m | \u001b[32m       24.6231\u001b[0m | \u001b[32m     0.7013\u001b[0m | \n",
      "    3 | 16m52s |    0.50960 |             0.6286 |    2.1666 |          0.2696 |           3.0992 |      7.1065 |         0.4736 |            14.7268 |              6.0517 |        21.9034 |      0.5107 | \n",
      "    4 | 80m16s | \u001b[35m   0.51261\u001b[0m | \u001b[32m            0.8196\u001b[0m | \u001b[32m   4.1489\u001b[0m | \u001b[32m         0.1306\u001b[0m | \u001b[32m          2.1016\u001b[0m | \u001b[32m     7.4076\u001b[0m | \u001b[32m        0.1343\u001b[0m | \u001b[32m           17.9129\u001b[0m | \u001b[32m            12.5687\u001b[0m | \u001b[32m       21.2492\u001b[0m | \u001b[32m     0.5597\u001b[0m | \n",
      "    5 | 100m10s | \u001b[35m   0.51354\u001b[0m | \u001b[32m            0.9007\u001b[0m | \u001b[32m   4.3275\u001b[0m | \u001b[32m         0.0939\u001b[0m | \u001b[32m          5.0237\u001b[0m | \u001b[32m     8.3645\u001b[0m | \u001b[32m        0.3502\u001b[0m | \u001b[32m            7.3256\u001b[0m | \u001b[32m             9.4378\u001b[0m | \u001b[32m       11.7140\u001b[0m | \u001b[32m     0.5878\u001b[0m | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   learning_rate |   max_delta_step |   max_depth |   max_features |   min_child_weight |   min_samples_split |   n_estimators |   subsample | \n",
      "    6 | 08m10s |    0.49653 |             0.8614 |    9.0799 |          0.0958 |           9.8815 |      4.1602 |         0.9007 |             1.9372 |             19.5759 |        18.6439 |      0.9807 | \n",
      "    7 | 63m51s |    0.50487 |             0.5919 |    9.2286 |          0.0717 |           0.6590 |      9.5525 |         0.6499 |            19.5953 |              2.3529 |        10.4610 |      0.8001 | \n",
      "    8 | 23m14s |    0.51243 |             0.8425 |    1.5456 |          0.2625 |           9.5008 |      9.6101 |         0.8688 |            19.1720 |             19.7286 |        12.1921 |      0.8501 | \n",
      "    9 | 15m34s |    0.51216 |             0.8725 |    1.3178 |          0.1221 |           0.8864 |      9.0174 |         0.2523 |             1.4514 |             19.1039 |        11.3255 |      0.4752 | \n",
      "   10 | 22m06s |    0.51262 |             0.8968 |    1.2959 |          0.2681 |           9.0987 |      9.5647 |         0.1238 |             1.1941 |              2.5406 |        11.0970 |      0.9197 | \n",
      "   11 | 12m30s |    0.51150 |             0.4127 |    0.2082 |          0.0843 |           9.8100 |      9.9225 |         0.1173 |            14.1375 |             18.1245 |        23.7979 |      0.6832 | \n",
      "   12 | 27m34s | \u001b[35m   0.51358\u001b[0m | \u001b[32m            0.9688\u001b[0m | \u001b[32m   0.9831\u001b[0m | \u001b[32m         0.0771\u001b[0m | \u001b[32m          0.3471\u001b[0m | \u001b[32m     9.8630\u001b[0m | \u001b[32m        0.5540\u001b[0m | \u001b[32m           18.8820\u001b[0m | \u001b[32m            17.4223\u001b[0m | \u001b[32m       17.3135\u001b[0m | \u001b[32m     0.4372\u001b[0m | \n",
      "   13 | 78m12s |    0.50655 |             0.9840 |    8.7176 |          0.1656 |           0.9407 |      9.9420 |         0.1517 |            14.4686 |             19.1549 |        10.1785 |      0.5321 | \n",
      "   14 | 10m14s |    0.51047 |             0.8961 |    0.3883 |          0.1718 |           9.2637 |      9.9890 |         0.1763 |            18.9426 |              3.7894 |        10.6765 |      0.5045 | \n",
      "   15 | 09m15s |    0.51042 |             0.9962 |    1.2669 |          0.2464 |           0.5466 |      9.4075 |         0.2929 |             0.4141 |              3.0629 |        10.9351 |      0.5830 | \n",
      "   16 | 09m44s |    0.50785 |             0.9710 |    0.9512 |          0.2515 |           1.4269 |      9.7302 |         0.3807 |            10.7123 |             13.1899 |        16.6878 |      0.9988 | \n",
      "   17 | 27m39s |    0.50090 |             0.4848 |    9.5744 |          0.2888 |           6.4265 |      9.9584 |         0.2627 |             0.9355 |              3.9881 |        11.8491 |      0.4091 | \n",
      "   18 | 21m55s |    0.50895 |             0.8753 |    0.0704 |          0.1638 |           8.0847 |      4.1878 |         0.8980 |             0.0501 |              9.8805 |        11.3367 |      0.5600 | \n",
      "   19 | 161m14s |    0.49851 |             0.6211 |    9.9141 |          0.2016 |           9.8241 |      5.2228 |         0.9736 |            19.6948 |             19.0438 |        23.8174 |      0.4625 | \n",
      "   20 | 08m05s |    0.50885 |             0.5012 |    0.6943 |          0.1347 |           9.3333 |      9.9443 |         0.1530 |             2.6989 |             18.3343 |        11.5815 |      0.4003 | \n"
     ]
    }
   ],
   "source": [
    "xgb_bo.maximize(init_points=5, n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Maximum value: 0.513582\n",
      "Best parameters:  {'max_depth': 9.8629978540813301, 'learning_rate': 0.077111470871203, 'subsample': 0.43718276627682534, 'colsample_bytree': 0.96878133311737136, 'gamma': 0.98310097303364974, 'min_child_weight': 18.881955805794195, 'max_delta_step': 0.34713865069617045, 'n_estimators': 17.313468718375105, 'min_samples_split': 17.422269470760753, 'max_features': 0.55401408287086862}\n"
     ]
    }
   ],
   "source": [
    "print('-'*30)\n",
    "print('Maximum value: %f' % xgb_bo.res['max']['max_val'])\n",
    "print('Best parameters: ', xgb_bo.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Velification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting xgboost...\n",
      "fold cv 0 rmsle score is 0.488750\n",
      "fold cv 1 rmsle score is 0.491456\n",
      "fold cv 2 rmsle score is 0.485613\n",
      "cv rmsle score is 0.488613\n",
      "it takes 627.981 seconds to perform cross validation\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    'max_depth': 10,#9.8629978540813301, \n",
    "    'learning_rate': 0.077111470871203, \n",
    "    'subsample': 0.43718276627682534, \n",
    "    'colsample_bytree': 0.96878133311737136, \n",
    "    'gamma': 0.98310097303364974, \n",
    "    'min_child_weight': 18.881955805794195, \n",
    "    'max_delta_step': 0.34713865069617045, \n",
    "    'n_estimators': 17.313468718375105, \n",
    "    'min_samples_split': 17.422269470760753, \n",
    "    'max_features': 0.55401408287086862\n",
    "}\n",
    "print(\"Starting xgboost...\")\n",
    "outcomes=cross_validate_xgb(xgb_params, train_input, y, test_input, kf, verbose_eval=False)\n",
    "\n",
    "xgb_cv=outcomes[0]\n",
    "xgb_train_pred=outcomes[1]\n",
    "xgb_test_pred=outcomes[2]\n",
    "\n",
    "xgb_train_pred_df=pd.DataFrame(columns=['visitors'], data=xgb_train_pred)\n",
    "xgb_test_pred_df=pd.DataFrame(columns=['visitors'], data=xgb_test_pred)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv1_xgb_train_pred = xgb_train_pred_df.copy()\n",
    "lv1_xgb_train_pred.to_csv('lv1_xgb_train_pred.csv', index=False)\n",
    "\n",
    "lv1_xgb_test_pred = xgb_test_pred_df.copy()\n",
    "lv1_xgb_test_pred.to_csv('lv1_xgb_test_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['visitors'] = xgb_test_pred_df.values\n",
    "sub = test[['id','visitors']].copy()\n",
    "#sub.to_csv('submission_rs_recruit_v14_xgb_linear_feature_engineering_01.csv', index=False)\n",
    "#print('Good luck :)')\n",
    "\n",
    "#xgboost\n",
    "#Bopt\n",
    "#LB NOT SUBMITTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last work with weight# Consider weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../../../mltestdata/05_recruit/*.csv')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n",
    "    on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_merge['visitors'] = (sub_merge['visitors_x'] + sub_merge['visitors_y']* 1.1)/2\n",
    "sub_merge[['id', 'visitors']].to_csv('submission_rs_recruit_v14_xgb_linear_feature_engineering_01.csv', index=False)\n",
    "\n",
    "# gw_flag\n",
    "# xgb\n",
    "# Bopt\n",
    "# weight\n",
    "# LB 0.486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
