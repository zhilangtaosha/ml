{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import time\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contributions from:\n",
    "DSEverything - Mean Mix - Math, Geo, Harmonic (LB 0.493) \n",
    "https://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493\n",
    "JdPaletto - Surprised Yet? - Part2 - (LB: 0.503)\n",
    "https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503\n",
    "hklee - weighted mean comparisons, LB 0.497, 1ST\n",
    "https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st\n",
    "\n",
    "Also all comments for changes, encouragement, and forked scripts rock\n",
    "\n",
    "Keep the Surprise Going\n",
    "\"\"\"\n",
    "\n",
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../../mltestdata/05_recruit/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "    }\n",
    "\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "#sure it can be compressed...\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n",
    "\n",
    "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n",
    "\n",
    "# NEW FEATURES FROM JMBULL\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "\n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for LightGBM ...\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nProcessing data for LightGBM ...\" )\n",
    "for c, dtype in zip(train.columns, train.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        train[c] = train[c].astype(np.float32)\n",
    "\n",
    "for c, dtype in zip(test.columns, test.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        test[c] = test[c].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "y_train=train['visitors']\n",
    "x_train=train.drop(drop_cols, axis=1)\n",
    "\n",
    "x_test=test.copy()\n",
    "x_test=x_test.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.visitors\n",
    "train_input = train.copy()\n",
    "test_input = test.copy()\n",
    "\n",
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "train_input=train_input.drop(drop_cols, axis=1)\n",
    "test_input=test_input.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "localtrain, localval = train_test_split(train,test_size=0.3,random_state=2018)\n",
    "\n",
    "y_localtrain=localtrain['visitors']\n",
    "x_localtrain=localtrain.drop(drop_cols, axis=1)\n",
    "\n",
    "y_localval=localval['visitors']\n",
    "x_localval=localval.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation function\n",
    "\n",
    "def rmsle(preds, true):\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n",
    "    return rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation matrix \n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "RMSLE = make_scorer(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for comparing predictions and true data.\n",
    "def compare_result(preds, true):\n",
    "    compare = pd.DataFrame({\"test_id\": true.index,\n",
    "                           \"real_cost\": true,\n",
    "                           \"pred_cost\": preds})\n",
    "    compare = compare[[\"test_id\", \"real_cost\", \"pred_cost\"]].reset_index(drop=True)\n",
    "    \n",
    "    compare[\"error_percent_(%)\"] = np.abs(compare.real_cost - compare.pred_cost) / compare.real_cost * 100\n",
    "    \n",
    "    return compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_lgb(params, x_train, y_train, kf, verbose=True, verbose_eval=50,df_input=True):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "\n",
    "    # self-defined eval metric\n",
    "    # f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "    # binary error\n",
    "    def feval_rmsle(preds, train_data):\n",
    "        #print(\"preds    \"+str(preds[0:5]))\n",
    "        #print(\"train_data   \"+str(train_data.get_label()[0:5]))\n",
    "        #preds = np.expm1(preds)\n",
    "        #true = np.expm1(train_data.get_label())\n",
    "        true = train_data.get_label()\n",
    "        return 'my rmsle', rmsle(preds,true), False\n",
    "\n",
    "    # use the k-fold object to enumerate indexes for each training and validation fold\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)): # folds 1, 2 ,3 ,4, 5\n",
    "        # example: training from 1,2,3,4; validation from 5\n",
    "\n",
    "        if df_input:\n",
    "            x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "        else:\n",
    "            x_train_kf, x_val_kf = x_train[train_index], x_train[val_index]\n",
    "            \n",
    "        y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n",
    "        #y_train_kf, y_val_kf = np.log1p(y_train[train_index]), np.log1p(y_train[val_index])\n",
    "\n",
    "        lgb_train = lgb.Dataset(x_train_kf, y_train_kf)\n",
    "        lgb_val = lgb.Dataset(x_val_kf, y_val_kf, reference=lgb_train)\n",
    "        \n",
    "        watchlist= [lgb_train, lgb_val]\n",
    "\n",
    "        gbm = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=8000,\n",
    "                        #valid_sets=lgb_val,\n",
    "                        valid_sets=watchlist, \n",
    "                        early_stopping_rounds=100,\n",
    "                        verbose_eval=verbose_eval,\n",
    "                        feval=feval_rmsle\n",
    "                       )\n",
    "\n",
    "        ###print(\"gbm.predict  \"+str(gbm.predict(x_val_kf)))\n",
    "        ###print(\"gbm.predict  \"+str(np.expm1(gbm.predict(x_val_kf))))\n",
    "\n",
    "        #val_pred = np.expm1(gbm.predict(x_val_kf, num_iteration=gbm.best_iteration))\n",
    "        #val_pred = np.expm1(gbm.predict(x_val_kf))\n",
    "        val_pred = gbm.predict(x_val_kf, num_iteration=gbm.best_iteration)\n",
    "\n",
    "        train_pred[val_index] += val_pred\n",
    "        #print(\"\\nBefore fold_rmsle\")\n",
    "        #print(\"gbm.predict  \"+str(gbm.predict(x_val_kf, num_iteration=gbm.best_iteration)))\n",
    "        #print(\"train_pred[val_index]  \"+str(train_pred[val_index][0:5]))\n",
    "\n",
    "        ###print(\"np.expm1(y_val_kf.values)  \"+str(np.expm1(y_val_kf.values)[0:5]))\n",
    "        ###print(\"y_val_kf.values  \"+str(y_val_kf.values[0:5]))\n",
    " \n",
    "        #fold_rmsle = eval_rmsle(val_pred, y_val_kf.values)\n",
    "        #fold_rmsle = rmsle(val_pred, np.expm1(y_val_kf.values))\n",
    "        #fold_rmsle = eval_rmsle(train_pred[val_index], np.expm1(y_val_kf.values))\n",
    "        fold_rmsle = rmsle(val_pred, y_val_kf.values)\n",
    "        #Yifan: fold_rmsle = rmsle(np.expm1(y_val_kf.values), train_pred[val_index])\n",
    "\n",
    "        if verbose:\n",
    "            print('fold cv {} RMSLE score is {:.6f}\\n'.format(i, fold_rmsle))\n",
    " \n",
    "    #cv_rmsle = eval_rmsle(train_pred, np.log1p(y_train))\n",
    "    #cv_rmsle = rmsle(train_pred, y_train)\n",
    "    cv_rmsle = rmsle(train_pred, y_train)\n",
    "\n",
    "    if verbose:\n",
    "        print('cv RMSLE score is {:.6f}'.format(cv_rmsle))\n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    "\n",
    "    return cv_rmsle\n",
    "#    return cv_rmsle, train_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\ttraining's my rmsle: 0.519235\tvalid_1's my rmsle: 0.525025\n",
      "[100]\ttraining's my rmsle: 0.513931\tvalid_1's my rmsle: 0.519803\n",
      "[150]\ttraining's my rmsle: 0.513553\tvalid_1's my rmsle: 0.519514\n",
      "[200]\ttraining's my rmsle: 0.517118\tvalid_1's my rmsle: 0.523332\n",
      "Early stopping, best iteration is:\n",
      "[136]\ttraining's my rmsle: 0.510945\tvalid_1's my rmsle: 0.516755\n",
      "fold cv 0 RMSLE score is 0.599870\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\ttraining's my rmsle: 0.52042\tvalid_1's my rmsle: 0.522396\n",
      "[100]\ttraining's my rmsle: 0.514981\tvalid_1's my rmsle: 0.516974\n",
      "[150]\ttraining's my rmsle: 0.514943\tvalid_1's my rmsle: 0.517211\n",
      "[200]\ttraining's my rmsle: 0.51871\tvalid_1's my rmsle: 0.521434\n",
      "Early stopping, best iteration is:\n",
      "[136]\ttraining's my rmsle: 0.512434\tvalid_1's my rmsle: 0.514343\n",
      "fold cv 1 RMSLE score is 0.596215\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\ttraining's my rmsle: 0.52124\tvalid_1's my rmsle: 0.520336\n",
      "[100]\ttraining's my rmsle: 0.515764\tvalid_1's my rmsle: 0.515129\n",
      "[150]\ttraining's my rmsle: 0.515522\tvalid_1's my rmsle: 0.515097\n",
      "[200]\ttraining's my rmsle: 0.519055\tvalid_1's my rmsle: 0.518901\n",
      "[250]\ttraining's my rmsle: 0.518529\tvalid_1's my rmsle: 0.518764\n",
      "Early stopping, best iteration is:\n",
      "[153]\ttraining's my rmsle: 0.512911\tvalid_1's my rmsle: 0.512749\n",
      "fold cv 2 RMSLE score is 0.571349\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\ttraining's my rmsle: 0.520943\tvalid_1's my rmsle: 0.521994\n",
      "[100]\ttraining's my rmsle: 0.51534\tvalid_1's my rmsle: 0.516663\n",
      "[150]\ttraining's my rmsle: 0.515025\tvalid_1's my rmsle: 0.516476\n",
      "[200]\ttraining's my rmsle: 0.518729\tvalid_1's my rmsle: 0.520478\n",
      "[250]\ttraining's my rmsle: 0.518191\tvalid_1's my rmsle: 0.520265\n",
      "Early stopping, best iteration is:\n",
      "[153]\ttraining's my rmsle: 0.512357\tvalid_1's my rmsle: 0.513945\n",
      "fold cv 3 RMSLE score is 0.571622\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\ttraining's my rmsle: 0.521264\tvalid_1's my rmsle: 0.519301\n",
      "[100]\ttraining's my rmsle: 0.515888\tvalid_1's my rmsle: 0.51414\n",
      "[150]\ttraining's my rmsle: 0.515874\tvalid_1's my rmsle: 0.514108\n",
      "[200]\ttraining's my rmsle: 0.519322\tvalid_1's my rmsle: 0.517651\n",
      "Early stopping, best iteration is:\n",
      "[136]\ttraining's my rmsle: 0.513122\tvalid_1's my rmsle: 0.511647\n",
      "fold cv 4 RMSLE score is 0.596341\n",
      "\n",
      "cv RMSLE score is 0.587228\n",
      "it takes 107.390 seconds to perform cross validation\n",
      "cv score is 0.587228\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "    'boosting_type': 'dart',\n",
    "    'max_depth' : 5,\n",
    "    'max_bin' : 500,\n",
    "    'learning_rate': 0.1,  # 0.618580\n",
    "    'num_leaves': 22,\n",
    "#    'metric': 'RMSE'\n",
    "}\n",
    "\n",
    "# only do 5 fold CV here so that we save some running time on Kaggle Kernel\n",
    "kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "cv_score =cross_validate_lgb(lgb_params, train_input, y, kf, verbose=True, verbose_eval=50,df_input=True)\n",
    "\n",
    "print('cv score is {:.6f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Start training...\n",
    "preds    [ 21.15093175  20.87278066  22.22167965  21.48732884  20.02349571]\n",
    "train_data   0    25\n",
    "1    32\n",
    "2    29\n",
    "3    22\n",
    "4     6\n",
    "Name: visitors, dtype: int64\n",
    "preds    [ 22.22167965  20.47679023  20.87278066  22.22167965  21.48732884]\n",
    "train_data   8     18\n",
    "11    11\n",
    "17    12\n",
    "18    45\n",
    "19    15\n",
    "Name: visitors, dtype: int64\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "preds    [ 21.24943     20.97127891  23.37412446  21.9696604   19.39810651]\n",
    "train_data   0    25\n",
    "1    32\n",
    "2    29\n",
    "3    22\n",
    "4     6\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Start training...\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmse: 0.582389\n",
    "[100]\tvalid_0's rmse: 0.588296\n",
    "Early stopping, best iteration is:\n",
    "[45]\tvalid_0's rmse: 0.515019\n",
    "gbm.predict  [ 1.42741231  1.16785115  1.19904827 ...,  0.59878107  0.69817213\n",
    "  0.66574006]\n",
    "gbm.predict  [ 3.16789999  2.21507649  2.31695857 ...,  0.81989912  1.01007519\n",
    "  0.9459301 ]\n",
    "np.expm1(y_val_kf.values)  [ 18.  11.  12.  45.  15.]\n",
    "y_val_kf.values  [ 2.94443898  2.48490665  2.56494936  3.8286414   2.77258872]\n",
    "fold cv 0 RMSLE score is 1.784524\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmse: 0.578917\n",
    "[100]\tvalid_0's rmse: 0.584797\n",
    "Early stopping, best iteration is:\n",
    "[45]\tvalid_0's rmse: 0.51222\n",
    "gbm.predict  [ 1.0005289   1.1695398   1.18349952 ...,  0.67099202  0.68324909\n",
    "  0.68324909]\n",
    "gbm.predict  [ 1.71971992  2.22051023  2.26578291 ...,  0.95617693  0.98030146\n",
    "  0.98030146]\n",
    "np.expm1(y_val_kf.values)  [  6.   9.  21.  26.   6.]\n",
    "y_val_kf.values  [ 1.94591015  2.30258509  3.09104245  3.29583687  1.94591015]\n",
    "fold cv 1 RMSLE score is 1.780887\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmse: 0.578507\n",
    "[100]\tvalid_0's rmse: 0.584762\n",
    "Early stopping, best iteration is:\n",
    "[45]\tvalid_0's rmse: 0.509925\n",
    "gbm.predict  [ 1.29143688  1.17206736  1.42941167 ...,  0.67207122  0.66017064\n",
    "  0.67207122]\n",
    "gbm.predict  [ 2.6380102   2.22866054  3.17624146 ...,  0.95828917  0.93512252\n",
    "  0.95828917]\n",
    "np.expm1(y_val_kf.values)  [ 25.  32.  29.  31.  26.]\n",
    "y_val_kf.values  [ 3.25809654  3.49650756  3.40119738  3.4657359   3.29583687]\n",
    "fold cv 2 RMSLE score is 1.781198\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmse: 0.578578\n",
    "[100]\tvalid_0's rmse: 0.584745\n",
    "Early stopping, best iteration is:\n",
    "[45]\tvalid_0's rmse: 0.511196\n",
    "gbm.predict  [ 1.31281624  1.28247999  1.16827171 ...,  0.69396583  0.65322372\n",
    "  0.68092431]\n",
    "gbm.predict  [ 2.7166259   2.60557041  2.21642891 ...,  1.00163798  0.92172597\n",
    "  0.97570305]\n",
    "np.expm1(y_val_kf.values)  [ 22.  24.  21.  15.  23.]\n",
    "y_val_kf.values  [ 3.13549422  3.21887582  3.09104245  2.77258872  3.17805383]\n",
    "fold cv 3 RMSLE score is 1.779868\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmse: 0.576969\n",
    "[100]\tvalid_0's rmse: 0.583153\n",
    "Early stopping, best iteration is:\n",
    "[45]\tvalid_0's rmse: 0.508495\n",
    "gbm.predict  [ 1.42674843  1.43787536  1.00130438 ...,  0.66073807  0.67613772\n",
    "  0.66073807]\n",
    "gbm.predict  [ 3.16513392  3.21173788  1.72182982 ...,  0.93622088  0.96626876\n",
    "  0.93622088]\n",
    "np.expm1(y_val_kf.values)  [ 32.  51.  29.  30.  37.]\n",
    "y_val_kf.values  [ 3.49650756  3.95124372  3.40119738  3.4339872   3.63758616]\n",
    "fold cv 4 RMSLE score is 1.780247\n",
    "\n",
    "cv RMSLE score is 1.781348\n",
    "it takes 38.238 seconds to perform cross validation\n",
    "cv score is 1.781348\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Start training...\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmsle: 0.525025\n",
    "[100]\tvalid_0's rmsle: 0.519803\n",
    "[150]\tvalid_0's rmsle: 0.519514\n",
    "[200]\tvalid_0's rmsle: 0.523332\n",
    "Early stopping, best iteration is:\n",
    "[136]\tvalid_0's rmsle: 0.516755\n",
    "fold cv 0 RMSLE score is 0.599870\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmsle: 0.522396\n",
    "[100]\tvalid_0's rmsle: 0.516974\n",
    "[150]\tvalid_0's rmsle: 0.517211\n",
    "[200]\tvalid_0's rmsle: 0.521434\n",
    "Early stopping, best iteration is:\n",
    "[136]\tvalid_0's rmsle: 0.514343\n",
    "fold cv 1 RMSLE score is 0.596215\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmsle: 0.520336\n",
    "[100]\tvalid_0's rmsle: 0.515129\n",
    "[150]\tvalid_0's rmsle: 0.515097\n",
    "[200]\tvalid_0's rmsle: 0.518901\n",
    "[250]\tvalid_0's rmsle: 0.518764\n",
    "Early stopping, best iteration is:\n",
    "[153]\tvalid_0's rmsle: 0.512749\n",
    "fold cv 2 RMSLE score is 0.571349\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmsle: 0.521994\n",
    "[100]\tvalid_0's rmsle: 0.516663\n",
    "[150]\tvalid_0's rmsle: 0.516476\n",
    "[200]\tvalid_0's rmsle: 0.520478\n",
    "[250]\tvalid_0's rmsle: 0.520265\n",
    "Early stopping, best iteration is:\n",
    "[153]\tvalid_0's rmsle: 0.513945\n",
    "fold cv 3 RMSLE score is 0.571622\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmsle: 0.519301\n",
    "[100]\tvalid_0's rmsle: 0.51414\n",
    "[150]\tvalid_0's rmsle: 0.514108\n",
    "[200]\tvalid_0's rmsle: 0.517651\n",
    "Early stopping, best iteration is:\n",
    "[138]\tvalid_0's rmsle: 0.511618\n",
    "fold cv 4 RMSLE score is 0.591054\n",
    "\n",
    "cv RMSLE score is 0.586159\n",
    "it takes 74.017 seconds to perform cross validation\n",
    "cv score is 0.586159\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimsation - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'num_leaves':(7,100),#(7,4095),\n",
    "    'max_depth':(2,63),\n",
    "    'learning_rate':(0.05,0.3),\n",
    "    #'scale_pos_weight':(1,10000),\n",
    "    'min_sum_hessian_in_leaf':(2,30),\n",
    "    'subsample':(0.4,1.0),\n",
    "    'colsample_bytree':(0.4,1.0),\n",
    "    #'feature_fraction':(0.0,1.0),\n",
    "    #'bagging_fraction':(0.0,1.0),\n",
    "    #'bagging_freq':(0,2),\n",
    "    #'lambda_l1':(0.0,1.0),\n",
    "    #'lambda_l2':(0.0,1.0),\n",
    "    #'n_estimators':(2,30), \n",
    "    #'reg_lambda':(0.0,2.0),\n",
    "    #'min_gain_to_split':(0.0,1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(lgb_wrapper)\n",
    "#def lgbcv_func(max_depth, learning_rate, subsample, colsample_bytree, nthread=4, seed=0):\n",
    "def lgbcv_func(num_leaves, max_depth, learning_rate,\n",
    "               #scale_pos_weight, \n",
    "               min_sum_hessian_in_leaf, \n",
    "               subsample, \n",
    "               colsample_bytree,\n",
    "               #feature_fraction, bagging_fraction, \n",
    "               #bagging_freq, lambda_l1, lambda_l2,\n",
    "               #n_estimators,reg_lambda,min_gain_to_split,\n",
    "               nthread=4):\n",
    "\n",
    "    params = {\n",
    "        'objective' : \"regression\",\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'dart',\n",
    "                \n",
    "        'num_leaves': int(num_leaves),\n",
    "        'max_depth': int(max_depth), \n",
    "        'learning_rate': float(learning_rate),\n",
    "        #'scale_pos_weight':scale_pos_weight,\n",
    "        'min_sum_hessian_in_leaf':float(min_sum_hessian_in_leaf), \n",
    "        'subsample':subsample,\n",
    "        'colsample_bytree':colsample_bytree,\n",
    "        #'feature_fraction':feature_fraction, \n",
    "        #'bagging_fraction':bagging_fraction,\n",
    "        #'bagging_freq':bagging_freq, \n",
    "        #'lambda_l1':lambda_l1, \n",
    "        #'lambda_l2':lambda_l2,\n",
    "        #'n_estimators':n_estimators,\n",
    "        #'reg_lambda':reg_lambda,\n",
    "        #'min_gain_to_split':min_gain_to_split       \n",
    "        #'metric': 'RMSE'\n",
    "    }\n",
    "    \n",
    "    # for a more ideal out-of-fold model prediction for this dataset, we use 10-fold CV\n",
    "    kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)\n",
    "    \n",
    "    # we will disable all the verbose setting in this functional call, so that we don't have too much information \n",
    "    # to read during the bayesian optimisation process.\n",
    "    return 1-np.float32(cross_validate_lgb(params, train_input, y, kf, verbose=False, verbose_eval=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_input.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_bo=BayesianOptimization(lgbcv_func, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_sum_hessian_in_leaf |   num_leaves |   subsample | \n",
      "    1 | 01m40s | \u001b[35m   0.45014\u001b[0m | \u001b[32m            0.6501\u001b[0m | \u001b[32m         0.0625\u001b[0m | \u001b[32m    27.1274\u001b[0m | \u001b[32m                   7.7231\u001b[0m | \u001b[32m     77.8175\u001b[0m | \u001b[32m     0.7037\u001b[0m | \n",
      "    2 | 01m10s |    0.42097 |             0.9993 |          0.1017 |      5.5520 |                    8.7725 |      85.3658 |      0.6568 | \n",
      "    3 | 01m05s |    0.43846 |             0.5943 |          0.1574 |     24.3220 |                   18.5864 |      48.1857 |      0.7388 | \n",
      "    4 | 01m08s |    0.44734 |             0.4117 |          0.1593 |     16.1724 |                   24.2836 |      53.7343 |      0.4401 | \n",
      "    5 | 01m13s |    0.43483 |             0.7806 |          0.1647 |     39.2492 |                   13.1549 |      89.1572 |      0.6872 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_sum_hessian_in_leaf |   num_leaves |   subsample | \n",
      "    6 | 01m12s |    0.44584 |             0.4000 |          0.0500 |      2.0000 |                    2.0000 |       7.0000 |      0.4000 | \n",
      "    7 | 01m12s |    0.44600 |             0.4000 |          0.0500 |      2.0000 |                   30.0000 |       7.0000 |      0.4000 | \n",
      "    8 | 03m00s | \u001b[35m   0.48536\u001b[0m | \u001b[32m            0.4000\u001b[0m | \u001b[32m         0.0500\u001b[0m | \u001b[32m    63.0000\u001b[0m | \u001b[32m                   2.0000\u001b[0m | \u001b[32m     78.5501\u001b[0m | \u001b[32m     0.4000\u001b[0m | \n",
      "    9 | 01m21s |    0.45653 |             0.4000 |          0.0500 |     63.0000 |                    2.0000 |       7.0000 |      0.4000 | \n",
      "   10 | 01m17s |    0.43126 |             0.6888 |          0.1203 |     62.9495 |                   29.4166 |      59.5846 |      0.6414 | \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-067a6f427821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlgb_bo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;31m# Append most recently generated values to X and Y arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mobserve_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# measure the target function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-6df50ae1a8eb>\u001b[0m in \u001b[0;36mlgbcv_func\u001b[0;34m(num_leaves, max_depth, learning_rate, min_sum_hessian_in_leaf, subsample, colsample_bytree, nthread)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# we will disable all the verbose setting in this functional call, so that we don't have too much information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# to read during the bayesian optimisation process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_validate_lgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-7bd05017e88e>\u001b[0m in \u001b[0;36mcross_validate_lgb\u001b[0;34m(params, x_train, y_train, kf, verbose, verbose_eval, df_input)\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                         \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval_rmsle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                        )\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks_after_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36meval_valid\u001b[0;34m(self, feval)\u001b[0m\n\u001b[1;32m   1625\u001b[0m             \u001b[0mList\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m         \"\"\"\n\u001b[0;32m-> 1627\u001b[0;31m         return [item for i in range_(1, self.__num_dataset)\n\u001b[0m\u001b[1;32m   1628\u001b[0m                 for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n\u001b[1;32m   1629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1626\u001b[0m         \"\"\"\n\u001b[1;32m   1627\u001b[0m         return [item for i in range_(1, self.__num_dataset)\n\u001b[0;32m-> 1628\u001b[0;31m                 for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n\u001b[0m\u001b[1;32m   1629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__inner_eval\u001b[0;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[1;32m   1888\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m                 \u001b[0mcur_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m             \u001b[0mfeval_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__inner_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0meval_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_higher_better\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeval_ret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-7bd05017e88e>\u001b[0m in \u001b[0;36mfeval_rmsle\u001b[0;34m(preds, train_data)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#true = np.expm1(train_data.get_label())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m'my rmsle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmsle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# use the k-fold object to enumerate indexes for each training and validation fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0675d28aab3b>\u001b[0m in \u001b[0;36mrmsle\u001b[0;34m(preds, true)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrmsle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mrmsle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrmsle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \"\"\"\n\u001b[1;32m    237\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 238\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    239\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n\u001b[1;32m    240\u001b[0m                                weights=sample_weight)\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    420\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     41\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     42\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 43\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "lgb_bo.maximize(init_points=5, n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.17995806067430897,\n",
       " 'bagging_freq': 1.3430342823014347,\n",
       " 'feature_fraction': 0.51232300441712064,\n",
       " 'lambda_l1': 0.54056564353676506,\n",
       " 'max_depth': 20.348482273206898,\n",
       " 'min_sum_hessian_in_leaf': 29.260145258218444,\n",
       " 'num_leaves': 15.619747584846419}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_bo.res['max']['max_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "{'bagging_fraction': 0.17995806067430897,\n",
    " 'bagging_freq': 1.3430342823014347,\n",
    " 'feature_fraction': 0.51232300441712064,\n",
    " 'lambda_l1': 0.54056564353676506,\n",
    " 'max_depth': 20.348482273206898,\n",
    " 'min_sum_hessian_in_leaf': 29.260145258218444,\n",
    " 'num_leaves': 15.619747584846419}\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Final Results\n",
      "Maximum value: 0.422800\n",
      "Best parameters:  {'num_leaves': 15.619747584846419, 'min_sum_hessian_in_leaf': 29.260145258218444, 'max_depth': 20.348482273206898, 'feature_fraction': 0.51232300441712064, 'bagging_fraction': 0.17995806067430897, 'bagging_freq': 1.3430342823014347, 'lambda_l1': 0.54056564353676506}\n"
     ]
    }
   ],
   "source": [
    "print('-'*30)\n",
    "print('Final Results')\n",
    "print('Maximum value: %f' % lgb_bo.res['max']['max_val'])\n",
    "print('Best parameters: ', lgb_bo.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "------------------------------\n",
    "Final Results\n",
    "Maximum value: 0.422800\n",
    "Best parameters:  {'num_leaves': 15.619747584846419, 'min_sum_hessian_in_leaf': 29.260145258218444, 'max_depth': 20.348482273206898, 'feature_fraction': 0.51232300441712064, 'bagging_fraction': 0.17995806067430897, 'bagging_freq': 1.3430342823014347, 'lambda_l1': 0.54056564353676506}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective' : \"regression\",\n",
    "    'learning_rate': 0.1,       \n",
    "    'task': 'train',\n",
    "    'boosting_type': 'dart',\n",
    "    'num_leaves': 16,#15.619747584846419, \n",
    "    'min_sum_hessian_in_leaf': 29.260145258218444, \n",
    "    'max_depth': 20,#20.348482273206898,\n",
    "    'feature_fraction': 0.51232300441712064, \n",
    "    'bagging_fraction': 0.17995806067430897, \n",
    "    'bagging_freq': 1, #1.3430342823014347, \n",
    "    'lambda_l1': 0.54056564353676506\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\ttraining's rmsle: 0.580172\tvalid_1's rmsle: 0.579043\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's rmsle: 0.511956\tvalid_1's rmsle: 0.512371\n",
      "fold cv 0 RMSLE score is 0.577125\n",
      "\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\ttraining's rmsle: 0.578437\tvalid_1's rmsle: 0.584288\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's rmsle: 0.510283\tvalid_1's rmsle: 0.516068\n",
      "fold cv 1 RMSLE score is 0.575701\n",
      "\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\ttraining's rmsle: 0.580905\tvalid_1's rmsle: 0.578431\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's rmsle: 0.513352\tvalid_1's rmsle: 0.509707\n",
      "fold cv 2 RMSLE score is 0.573614\n",
      "\n",
      "cv RMSLE score is 0.575483\n",
      "it takes 13.225 seconds to perform cross validation\n",
      "cv score is 0.575483\n"
     ]
    }
   ],
   "source": [
    "# only do 3 fold CV here so that we save some running time on Kaggle Kernel\n",
    "kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "lgb_cv_score, lgb_train_pred =cross_validate_lgb(lgb_params, train_input, y, kf, verbose=True, verbose_eval=50,df_input=True)\n",
    "\n",
    "print('cv score is {:.6f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.69600288,  5.47287828,  8.57551433, ...,  1.95046062,\n",
       "        2.10087429,  2.00777837])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = lgb.train(lgb_params,\n",
    "                ,\n",
    "                num_boost_round=4000,\n",
    "                valid_sets=lgb_val,\n",
    "                early_stopping_rounds=30,\n",
    "                verbose_eval=verbose_eval)\n",
    "\n",
    "\n",
    "lgb_model = lgb.train(lgb_params, train_set=d_train, num_boost_round=rounds, \n",
    "                          valid_sets=watchlist, verbose_eval=1000, early_stopping_rounds = 300)\n",
    "test_pred = lgb_model.predict(X_v)\n",
    "\n",
    "params, train_set=d_train, num_boost_round=7500, valid_sets=watchlist, \\\n",
    "    early_stopping_rounds=500, verbose_eval=500\n",
    "\n",
    "    \n",
    "xgr_g = xgb.XGBRegressor(**grid_xgb.best_params_)\n",
    "xgr_g.fit(X_train, y_train)\n",
    "y_pred_gs = xgr_g.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
