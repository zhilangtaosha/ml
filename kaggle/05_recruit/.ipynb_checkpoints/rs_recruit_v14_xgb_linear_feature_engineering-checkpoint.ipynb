{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test xgboost with Bayesian Optimsation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contributions from:\n",
    "DSEverything - Mean Mix - Math, Geo, Harmonic (LB 0.493) \n",
    "https://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493\n",
    "JdPaletto - Surprised Yet? - Part2 - (LB: 0.503)\n",
    "https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503\n",
    "hklee - weighted mean comparisons, LB 0.497, 1ST\n",
    "https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st\n",
    "\n",
    "Also all comments for changes, encouragement, and forked scripts rock\n",
    "\n",
    "Keep the Surprise Going\n",
    "\"\"\"\n",
    "\n",
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../../mltestdata/05_recruit/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "    }\n",
    "\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "#sure it can be compressed...\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n",
    "\n",
    "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n",
    "\n",
    "# NEW FEATURES FROM JMBULL\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "\n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, dtype in zip(train.columns, train.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        train[c] = train[c].astype(np.float32)\n",
    "\n",
    "for c, dtype in zip(test.columns, test.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        test[c] = test[c].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Golden week flag and Post Golden week flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = [train, test]\n",
    "gw_list = ['2016-04-29','2016-04-30','2016-05-01','2016-05-02','2016-05-03','2016-05-04','2016-05-05','2017-04-29','2017-04-30','2017-05-01','2017-05-02','2017-05-03','2017-05-04','2017-05-05']\n",
    "post_gw_list=['2016-05-06']\n",
    "train['gw_flg'] = 0\n",
    "train['post_gw_flg'] = 0\n",
    "test['gw_flg'] = 0\n",
    "test['post_gw_flg'] = 0\n",
    "update_gw_list = [[\"0\" for i in range(3)] for j in range(len(gw_list))]\n",
    "update_post_gw_list = [[\"0\" for i in range(3)] for j in range(len(post_gw_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "for index, gw_date in enumerate(gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_gw_list[index][col_i]=int(temp_figure)\n",
    "        \n",
    "    #print(\"{}  {}  {}\".format(update_list[index][0],update_list[index][1],update_list[index][2]))\n",
    "    \n",
    "for index, gw_date in enumerate(post_gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_post_gw_list[index][col_i]=int(temp_figure)\n",
    "\n",
    "for dataset in combine:\n",
    "    for index in range(len(update_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_gw_list[index][0],update_gw_list[index][1],update_gw_list[index][2]), 'gw_flg'] = 1\n",
    "        \n",
    "for dataset in combine:\n",
    "    for index in range(len(update_post_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_post_gw_list[index][0],update_post_gw_list[index][1],update_post_gw_list[index][2]), 'post_gw_flg'] = 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "y_train=train['visitors']\n",
    "x_train=train.drop(drop_cols, axis=1)\n",
    "\n",
    "x_test=test.copy()\n",
    "x_test=x_test.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.visitors\n",
    "train_input = train.copy()\n",
    "test_input = test.copy()\n",
    "\n",
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "train_input=train_input.drop(drop_cols, axis=1)\n",
    "test_input=test_input.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation function\n",
    "\n",
    "def rmsle(preds, true):\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n",
    "    return float(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation matrix \n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "RMSLE = make_scorer(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for comparing predictions and true data.\n",
    "def compare_result(preds, true):\n",
    "    compare = pd.DataFrame({\"test_id\": true.index,\n",
    "                           \"real_cost\": true,\n",
    "                           \"pred_cost\": preds})\n",
    "    compare = compare[[\"test_id\", \"real_cost\", \"pred_cost\"]].reset_index(drop=True)\n",
    "    \n",
    "    compare[\"error_percent_(%)\"] = np.abs(compare.real_cost - compare.pred_cost) / compare.real_cost * 100\n",
    "    \n",
    "    return compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_xgb(params, x_train, y_train, x_test, kf,  verbose=True, verbose_eval=50, scoreonly=False):\n",
    "    start_time=time.time()\n",
    "    nround=[]\n",
    "    # the prediction matrix need to contains 3 columns, one for the probability of each class\n",
    "    #train_pred = np.zeros((x_train.shape[0],3))\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "    \n",
    "    # self-defined eval metric\n",
    "    # f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "    # binary error\n",
    "    def feval_rmsle(preds, train_data):\n",
    "        preds = np.expm1(preds)\n",
    "        true = np.expm1(train_data.get_label())\n",
    "        #return 'rmsle', rmsle(true, preds), False\n",
    "\n",
    "        return 'rmsle', rmsle(preds, true), False\n",
    "\n",
    "    # use the k-fold object to enumerate indexes for each training and validation fold\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "\n",
    "        #y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n",
    "        y_train_kf, y_val_kf = np.log1p(y_train[train_index]), np.log1p(y_train[val_index])\n",
    "        x_test_kf=x_test.copy()\n",
    "        \n",
    "        d_train = xgboost.DMatrix(x_train_kf, y_train_kf)\n",
    "        d_val=xgboost.DMatrix(x_val_kf, y_val_kf)\n",
    "        d_test = xgboost.DMatrix(x_test_kf)\n",
    "        \n",
    "        watchlist= [(d_train, \"train\"), (d_val, 'val')]\n",
    "        bst = xgboost.train(params=params, \n",
    "                            dtrain=d_train, \n",
    "                            num_boost_round=8000, \n",
    "                            early_stopping_rounds=100,\n",
    "                            evals=watchlist, \n",
    "                            verbose_eval=verbose_eval)        \n",
    "        \n",
    "        y_val_kf_preds=bst.predict(d_val, ntree_limit=bst.best_ntree_limit)\n",
    "        nround.append(bst.best_ntree_limit)\n",
    "        \n",
    "        train_pred[val_index] += y_val_kf_preds\n",
    "#        test_pred += np.expm1((bst.predict(x_test, ntree_limit=bst.best_ntree_limit)))\n",
    "        test_pred += np.expm1(bst.predict(d_test))\n",
    "        \n",
    "        \n",
    "        #fold_cv = log_loss(y_val_kf.values, y_val_kf_preds)\n",
    "        fold_rmsle = rmsle(np.expm1(train_pred[val_index]),np.expm1(y_val_kf.values))\n",
    "        fold_cv = fold_rmsle\n",
    "        \n",
    "        if verbose:\n",
    "            print('fold cv {} rmsle score is {:.6f}'.format(i, fold_cv))\n",
    "\n",
    "    test_pred = test_pred / kf.n_splits\n",
    "    #cv_score = log_loss(y_train, train_pred)\n",
    "    cv_score = rmsle(np.expm1(train_pred), y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print('cv rmsle score is {:.6f}'.format(cv_score))    \n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    " \n",
    "    if scoreonly:\n",
    "        #return cv_score # for the purpose of bayesian optimisation, we only need to return the CV score\n",
    "        return cv_score\n",
    "    else:\n",
    "        return (cv_score,train_pred,test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do 3 fold CV here so that we save some running time on Kaggle Kernel\n",
    "kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[0]\ttrain-rmse:2.32524\tval-rmse:2.32558\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.54\tval-rmse:0.5398\n",
      "[100]\ttrain-rmse:0.503041\tval-rmse:0.504951\n",
      "[150]\ttrain-rmse:0.49913\tval-rmse:0.502178\n",
      "[200]\ttrain-rmse:0.496437\tval-rmse:0.500558\n",
      "[250]\ttrain-rmse:0.494607\tval-rmse:0.4996\n",
      "[300]\ttrain-rmse:0.492879\tval-rmse:0.498688\n",
      "[350]\ttrain-rmse:0.491506\tval-rmse:0.49803\n",
      "[400]\ttrain-rmse:0.490117\tval-rmse:0.497413\n",
      "[450]\ttrain-rmse:0.489177\tval-rmse:0.496986\n",
      "[500]\ttrain-rmse:0.488209\tval-rmse:0.49654\n",
      "[550]\ttrain-rmse:0.487309\tval-rmse:0.496165\n",
      "[600]\ttrain-rmse:0.486394\tval-rmse:0.495819\n",
      "[650]\ttrain-rmse:0.485371\tval-rmse:0.495441\n",
      "[700]\ttrain-rmse:0.484624\tval-rmse:0.495213\n",
      "[750]\ttrain-rmse:0.483789\tval-rmse:0.494917\n",
      "[800]\ttrain-rmse:0.482926\tval-rmse:0.494607\n",
      "[850]\ttrain-rmse:0.482153\tval-rmse:0.49425\n",
      "[900]\ttrain-rmse:0.481474\tval-rmse:0.494048\n",
      "[950]\ttrain-rmse:0.48088\tval-rmse:0.493874\n",
      "[1000]\ttrain-rmse:0.480192\tval-rmse:0.493671\n",
      "[1050]\ttrain-rmse:0.479578\tval-rmse:0.493509\n",
      "[1100]\ttrain-rmse:0.479002\tval-rmse:0.493341\n",
      "[1150]\ttrain-rmse:0.478251\tval-rmse:0.493071\n",
      "[1200]\ttrain-rmse:0.477687\tval-rmse:0.492893\n",
      "[1250]\ttrain-rmse:0.477083\tval-rmse:0.492713\n",
      "[1300]\ttrain-rmse:0.476469\tval-rmse:0.492552\n",
      "[1350]\ttrain-rmse:0.47581\tval-rmse:0.492411\n",
      "[1400]\ttrain-rmse:0.475317\tval-rmse:0.492324\n",
      "[1450]\ttrain-rmse:0.474598\tval-rmse:0.492076\n",
      "[1500]\ttrain-rmse:0.473966\tval-rmse:0.491864\n",
      "[1550]\ttrain-rmse:0.473453\tval-rmse:0.49178\n",
      "[1600]\ttrain-rmse:0.472919\tval-rmse:0.491665\n",
      "[1650]\ttrain-rmse:0.472314\tval-rmse:0.491519\n",
      "[1700]\ttrain-rmse:0.471813\tval-rmse:0.491392\n",
      "[1750]\ttrain-rmse:0.471335\tval-rmse:0.491269\n",
      "[1800]\ttrain-rmse:0.470832\tval-rmse:0.49115\n",
      "[1850]\ttrain-rmse:0.470218\tval-rmse:0.491017\n",
      "[1900]\ttrain-rmse:0.469733\tval-rmse:0.490947\n",
      "[1950]\ttrain-rmse:0.469141\tval-rmse:0.490803\n",
      "[2000]\ttrain-rmse:0.468507\tval-rmse:0.490625\n",
      "[2050]\ttrain-rmse:0.467944\tval-rmse:0.49055\n",
      "[2100]\ttrain-rmse:0.467372\tval-rmse:0.490386\n",
      "[2150]\ttrain-rmse:0.466796\tval-rmse:0.490223\n",
      "[2200]\ttrain-rmse:0.466214\tval-rmse:0.490091\n",
      "[2250]\ttrain-rmse:0.465694\tval-rmse:0.489981\n",
      "[2300]\ttrain-rmse:0.465132\tval-rmse:0.489836\n",
      "[2350]\ttrain-rmse:0.464684\tval-rmse:0.489753\n",
      "[2400]\ttrain-rmse:0.46414\tval-rmse:0.489619\n",
      "[2450]\ttrain-rmse:0.463596\tval-rmse:0.489543\n",
      "[2500]\ttrain-rmse:0.46309\tval-rmse:0.489476\n",
      "[2550]\ttrain-rmse:0.462608\tval-rmse:0.489402\n",
      "[2600]\ttrain-rmse:0.462081\tval-rmse:0.489362\n",
      "[2650]\ttrain-rmse:0.461609\tval-rmse:0.489262\n",
      "[2700]\ttrain-rmse:0.461163\tval-rmse:0.48919\n",
      "[2750]\ttrain-rmse:0.460717\tval-rmse:0.489164\n",
      "[2800]\ttrain-rmse:0.460249\tval-rmse:0.489099\n",
      "[2850]\ttrain-rmse:0.459747\tval-rmse:0.489031\n",
      "[2900]\ttrain-rmse:0.459201\tval-rmse:0.488921\n",
      "[2950]\ttrain-rmse:0.458753\tval-rmse:0.488892\n",
      "[3000]\ttrain-rmse:0.458288\tval-rmse:0.488887\n",
      "[3050]\ttrain-rmse:0.457892\tval-rmse:0.488859\n",
      "[3100]\ttrain-rmse:0.457398\tval-rmse:0.488719\n",
      "[3150]\ttrain-rmse:0.456898\tval-rmse:0.488673\n",
      "[3200]\ttrain-rmse:0.456476\tval-rmse:0.488656\n",
      "[3250]\ttrain-rmse:0.456016\tval-rmse:0.488604\n",
      "[3300]\ttrain-rmse:0.455542\tval-rmse:0.488545\n",
      "[3350]\ttrain-rmse:0.455061\tval-rmse:0.488523\n",
      "[3400]\ttrain-rmse:0.45465\tval-rmse:0.488506\n",
      "[3450]\ttrain-rmse:0.45427\tval-rmse:0.488505\n",
      "[3500]\ttrain-rmse:0.45386\tval-rmse:0.488475\n",
      "[3550]\ttrain-rmse:0.453413\tval-rmse:0.488429\n",
      "[3600]\ttrain-rmse:0.453055\tval-rmse:0.488417\n",
      "[3650]\ttrain-rmse:0.452643\tval-rmse:0.488393\n",
      "[3700]\ttrain-rmse:0.452306\tval-rmse:0.488399\n",
      "[3750]\ttrain-rmse:0.451946\tval-rmse:0.488341\n",
      "[3800]\ttrain-rmse:0.45153\tval-rmse:0.488303\n",
      "[3850]\ttrain-rmse:0.451183\tval-rmse:0.48831\n",
      "[3900]\ttrain-rmse:0.450806\tval-rmse:0.488257\n",
      "[3950]\ttrain-rmse:0.450355\tval-rmse:0.488253\n",
      "[4000]\ttrain-rmse:0.449992\tval-rmse:0.488255\n",
      "[4050]\ttrain-rmse:0.449652\tval-rmse:0.488229\n",
      "[4100]\ttrain-rmse:0.449199\tval-rmse:0.488188\n",
      "[4150]\ttrain-rmse:0.44883\tval-rmse:0.488162\n",
      "[4200]\ttrain-rmse:0.448519\tval-rmse:0.488112\n",
      "[4250]\ttrain-rmse:0.448139\tval-rmse:0.48809\n",
      "[4300]\ttrain-rmse:0.447725\tval-rmse:0.488061\n",
      "[4350]\ttrain-rmse:0.447387\tval-rmse:0.488049\n",
      "[4400]\ttrain-rmse:0.447033\tval-rmse:0.488023\n",
      "[4450]\ttrain-rmse:0.446675\tval-rmse:0.488002\n",
      "[4500]\ttrain-rmse:0.446343\tval-rmse:0.488016\n",
      "[4550]\ttrain-rmse:0.445983\tval-rmse:0.487973\n",
      "[4600]\ttrain-rmse:0.445627\tval-rmse:0.487943\n",
      "[4650]\ttrain-rmse:0.445354\tval-rmse:0.487941\n",
      "[4700]\ttrain-rmse:0.445022\tval-rmse:0.487926\n",
      "[4750]\ttrain-rmse:0.444687\tval-rmse:0.487905\n",
      "[4800]\ttrain-rmse:0.444358\tval-rmse:0.487907\n",
      "[4850]\ttrain-rmse:0.444024\tval-rmse:0.487922\n",
      "Stopping. Best iteration:\n",
      "[4758]\ttrain-rmse:0.444632\tval-rmse:0.487897\n",
      "\n",
      "[0]\ttrain-rmse:2.32335\tval-rmse:2.32577\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.537014\tval-rmse:0.54362\n",
      "[100]\ttrain-rmse:0.500873\tval-rmse:0.509013\n",
      "[150]\ttrain-rmse:0.497057\tval-rmse:0.506363\n",
      "[200]\ttrain-rmse:0.494651\tval-rmse:0.504948\n",
      "[250]\ttrain-rmse:0.492795\tval-rmse:0.503931\n",
      "[300]\ttrain-rmse:0.491099\tval-rmse:0.502987\n",
      "[350]\ttrain-rmse:0.489637\tval-rmse:0.502258\n",
      "[400]\ttrain-rmse:0.488732\tval-rmse:0.501881\n",
      "[450]\ttrain-rmse:0.487781\tval-rmse:0.501465\n",
      "[500]\ttrain-rmse:0.487233\tval-rmse:0.501207\n",
      "[550]\ttrain-rmse:0.486805\tval-rmse:0.501035\n",
      "[600]\ttrain-rmse:0.48616\tval-rmse:0.500789\n",
      "[650]\ttrain-rmse:0.485733\tval-rmse:0.500623\n",
      "[700]\ttrain-rmse:0.485184\tval-rmse:0.500433\n",
      "[750]\ttrain-rmse:0.484547\tval-rmse:0.500214\n",
      "[800]\ttrain-rmse:0.484103\tval-rmse:0.500061\n",
      "[850]\ttrain-rmse:0.483796\tval-rmse:0.499959\n",
      "[900]\ttrain-rmse:0.483399\tval-rmse:0.499815\n",
      "[950]\ttrain-rmse:0.482935\tval-rmse:0.499641\n",
      "[1000]\ttrain-rmse:0.482359\tval-rmse:0.499402\n",
      "[1050]\ttrain-rmse:0.481871\tval-rmse:0.49925\n",
      "[1100]\ttrain-rmse:0.481558\tval-rmse:0.49915\n",
      "[1150]\ttrain-rmse:0.480938\tval-rmse:0.498893\n",
      "[1200]\ttrain-rmse:0.480466\tval-rmse:0.498723\n",
      "[1250]\ttrain-rmse:0.479974\tval-rmse:0.498469\n",
      "[1300]\ttrain-rmse:0.479581\tval-rmse:0.49837\n",
      "[1350]\ttrain-rmse:0.479199\tval-rmse:0.498229\n",
      "[1400]\ttrain-rmse:0.478856\tval-rmse:0.498138\n",
      "[1450]\ttrain-rmse:0.478374\tval-rmse:0.497939\n",
      "[1500]\ttrain-rmse:0.477913\tval-rmse:0.49781\n",
      "[1550]\ttrain-rmse:0.477561\tval-rmse:0.497683\n",
      "[1600]\ttrain-rmse:0.477243\tval-rmse:0.49763\n",
      "[1650]\ttrain-rmse:0.476935\tval-rmse:0.497518\n",
      "[1700]\ttrain-rmse:0.476671\tval-rmse:0.497435\n",
      "[1750]\ttrain-rmse:0.47627\tval-rmse:0.497268\n",
      "[1800]\ttrain-rmse:0.475949\tval-rmse:0.497191\n",
      "[1850]\ttrain-rmse:0.475524\tval-rmse:0.497074\n",
      "[1900]\ttrain-rmse:0.47527\tval-rmse:0.497014\n",
      "[1950]\ttrain-rmse:0.474888\tval-rmse:0.496959\n",
      "[2000]\ttrain-rmse:0.474601\tval-rmse:0.496911\n",
      "[2050]\ttrain-rmse:0.47425\tval-rmse:0.496829\n",
      "[2100]\ttrain-rmse:0.473928\tval-rmse:0.496777\n",
      "[2150]\ttrain-rmse:0.473566\tval-rmse:0.496655\n",
      "[2200]\ttrain-rmse:0.473308\tval-rmse:0.496617\n",
      "[2250]\ttrain-rmse:0.472987\tval-rmse:0.496569\n",
      "[2300]\ttrain-rmse:0.472703\tval-rmse:0.496507\n",
      "[2350]\ttrain-rmse:0.472352\tval-rmse:0.496442\n",
      "[2400]\ttrain-rmse:0.472109\tval-rmse:0.496422\n",
      "[2450]\ttrain-rmse:0.471787\tval-rmse:0.496329\n",
      "[2500]\ttrain-rmse:0.471499\tval-rmse:0.49624\n",
      "[2550]\ttrain-rmse:0.471243\tval-rmse:0.496211\n",
      "[2600]\ttrain-rmse:0.471022\tval-rmse:0.496126\n",
      "[2650]\ttrain-rmse:0.470689\tval-rmse:0.495998\n",
      "[2700]\ttrain-rmse:0.470436\tval-rmse:0.495939\n",
      "[2750]\ttrain-rmse:0.470143\tval-rmse:0.49587\n",
      "[2800]\ttrain-rmse:0.469939\tval-rmse:0.49584\n",
      "[2850]\ttrain-rmse:0.469661\tval-rmse:0.495815\n",
      "[2900]\ttrain-rmse:0.469437\tval-rmse:0.495784\n",
      "[2950]\ttrain-rmse:0.469184\tval-rmse:0.495721\n",
      "[3000]\ttrain-rmse:0.46883\tval-rmse:0.495566\n",
      "[3050]\ttrain-rmse:0.468515\tval-rmse:0.495475\n",
      "[3100]\ttrain-rmse:0.468229\tval-rmse:0.495467\n",
      "[3150]\ttrain-rmse:0.468024\tval-rmse:0.495442\n",
      "[3200]\ttrain-rmse:0.467757\tval-rmse:0.49535\n",
      "[3250]\ttrain-rmse:0.467554\tval-rmse:0.495305\n",
      "[3300]\ttrain-rmse:0.46729\tval-rmse:0.495241\n",
      "[3350]\ttrain-rmse:0.467135\tval-rmse:0.495219\n",
      "[3400]\ttrain-rmse:0.466874\tval-rmse:0.495161\n",
      "[3450]\ttrain-rmse:0.466684\tval-rmse:0.495127\n",
      "[3500]\ttrain-rmse:0.466432\tval-rmse:0.495112\n",
      "[3550]\ttrain-rmse:0.466231\tval-rmse:0.495065\n",
      "[3600]\ttrain-rmse:0.465923\tval-rmse:0.494994\n",
      "[3650]\ttrain-rmse:0.465711\tval-rmse:0.494985\n",
      "[3700]\ttrain-rmse:0.465534\tval-rmse:0.494981\n",
      "[3750]\ttrain-rmse:0.465374\tval-rmse:0.494961\n",
      "[3800]\ttrain-rmse:0.465155\tval-rmse:0.494908\n",
      "[3850]\ttrain-rmse:0.46497\tval-rmse:0.494891\n",
      "[3900]\ttrain-rmse:0.464778\tval-rmse:0.494819\n",
      "[3950]\ttrain-rmse:0.464506\tval-rmse:0.494752\n",
      "[4000]\ttrain-rmse:0.464281\tval-rmse:0.494724\n",
      "[4050]\ttrain-rmse:0.4641\tval-rmse:0.494698\n",
      "[4100]\ttrain-rmse:0.463915\tval-rmse:0.494672\n",
      "[4150]\ttrain-rmse:0.463739\tval-rmse:0.494628\n",
      "[4200]\ttrain-rmse:0.463608\tval-rmse:0.494622\n",
      "[4250]\ttrain-rmse:0.463479\tval-rmse:0.494621\n",
      "[4300]\ttrain-rmse:0.463224\tval-rmse:0.494616\n",
      "[4350]\ttrain-rmse:0.462977\tval-rmse:0.494555\n",
      "[4400]\ttrain-rmse:0.462832\tval-rmse:0.49452\n",
      "[4450]\ttrain-rmse:0.462618\tval-rmse:0.494494\n",
      "[4500]\ttrain-rmse:0.462413\tval-rmse:0.494446\n",
      "[4550]\ttrain-rmse:0.46223\tval-rmse:0.494425\n",
      "[4600]\ttrain-rmse:0.462053\tval-rmse:0.494422\n",
      "[4650]\ttrain-rmse:0.461924\tval-rmse:0.494413\n",
      "[4700]\ttrain-rmse:0.461773\tval-rmse:0.494387\n",
      "[4750]\ttrain-rmse:0.461592\tval-rmse:0.494363\n",
      "[4800]\ttrain-rmse:0.46135\tval-rmse:0.494317\n",
      "[4850]\ttrain-rmse:0.461152\tval-rmse:0.494308\n",
      "[4900]\ttrain-rmse:0.460985\tval-rmse:0.494287\n",
      "[4950]\ttrain-rmse:0.460851\tval-rmse:0.494272\n",
      "[5000]\ttrain-rmse:0.460681\tval-rmse:0.494229\n",
      "[5050]\ttrain-rmse:0.460528\tval-rmse:0.494203\n",
      "[5100]\ttrain-rmse:0.46031\tval-rmse:0.494144\n",
      "[5150]\ttrain-rmse:0.460164\tval-rmse:0.494112\n",
      "[5200]\ttrain-rmse:0.460002\tval-rmse:0.494043\n",
      "[5250]\ttrain-rmse:0.459771\tval-rmse:0.494018\n",
      "[5300]\ttrain-rmse:0.459644\tval-rmse:0.493998\n",
      "[5350]\ttrain-rmse:0.459475\tval-rmse:0.493967\n",
      "[5400]\ttrain-rmse:0.459364\tval-rmse:0.493968\n",
      "[5450]\ttrain-rmse:0.459128\tval-rmse:0.493918\n",
      "[5500]\ttrain-rmse:0.458974\tval-rmse:0.493852\n",
      "[5550]\ttrain-rmse:0.458735\tval-rmse:0.493812\n",
      "[5600]\ttrain-rmse:0.458589\tval-rmse:0.493808\n",
      "[5650]\ttrain-rmse:0.458459\tval-rmse:0.493827\n",
      "Stopping. Best iteration:\n",
      "[5565]\ttrain-rmse:0.458702\tval-rmse:0.493803\n",
      "\n",
      "[0]\ttrain-rmse:2.32499\tval-rmse:2.32226\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.541293\tval-rmse:0.539269\n",
      "[100]\ttrain-rmse:0.504149\tval-rmse:0.502423\n",
      "[150]\ttrain-rmse:0.501219\tval-rmse:0.500137\n",
      "[200]\ttrain-rmse:0.499547\tval-rmse:0.498969\n",
      "[250]\ttrain-rmse:0.498718\tval-rmse:0.498454\n",
      "[300]\ttrain-rmse:0.497734\tval-rmse:0.497863\n",
      "[350]\ttrain-rmse:0.496999\tval-rmse:0.497439\n",
      "[400]\ttrain-rmse:0.496176\tval-rmse:0.497071\n",
      "[450]\ttrain-rmse:0.495551\tval-rmse:0.496726\n",
      "[500]\ttrain-rmse:0.494674\tval-rmse:0.4963\n",
      "[550]\ttrain-rmse:0.494046\tval-rmse:0.496075\n",
      "[600]\ttrain-rmse:0.493368\tval-rmse:0.495757\n",
      "[650]\ttrain-rmse:0.492752\tval-rmse:0.495521\n",
      "[700]\ttrain-rmse:0.492153\tval-rmse:0.495294\n",
      "[750]\ttrain-rmse:0.491603\tval-rmse:0.495033\n",
      "[800]\ttrain-rmse:0.491037\tval-rmse:0.494764\n",
      "[850]\ttrain-rmse:0.490379\tval-rmse:0.494451\n",
      "[900]\ttrain-rmse:0.489482\tval-rmse:0.494006\n",
      "[950]\ttrain-rmse:0.488897\tval-rmse:0.493806\n",
      "[1000]\ttrain-rmse:0.488204\tval-rmse:0.493548\n",
      "[1050]\ttrain-rmse:0.487574\tval-rmse:0.493359\n",
      "[1100]\ttrain-rmse:0.48703\tval-rmse:0.493171\n",
      "[1150]\ttrain-rmse:0.486223\tval-rmse:0.492841\n",
      "[1200]\ttrain-rmse:0.485585\tval-rmse:0.492647\n",
      "[1250]\ttrain-rmse:0.485102\tval-rmse:0.492455\n",
      "[1300]\ttrain-rmse:0.48461\tval-rmse:0.492308\n",
      "[1350]\ttrain-rmse:0.484103\tval-rmse:0.492093\n",
      "[1400]\ttrain-rmse:0.483523\tval-rmse:0.491867\n",
      "[1450]\ttrain-rmse:0.483055\tval-rmse:0.491705\n",
      "[1500]\ttrain-rmse:0.482723\tval-rmse:0.491584\n",
      "[1550]\ttrain-rmse:0.48229\tval-rmse:0.491443\n",
      "[1600]\ttrain-rmse:0.481863\tval-rmse:0.491328\n",
      "[1650]\ttrain-rmse:0.48154\tval-rmse:0.491266\n",
      "[1700]\ttrain-rmse:0.481094\tval-rmse:0.491141\n",
      "[1750]\ttrain-rmse:0.480782\tval-rmse:0.491029\n",
      "[1800]\ttrain-rmse:0.480382\tval-rmse:0.490901\n",
      "[1850]\ttrain-rmse:0.480002\tval-rmse:0.490792\n",
      "[1900]\ttrain-rmse:0.479683\tval-rmse:0.490725\n",
      "[1950]\ttrain-rmse:0.479351\tval-rmse:0.490647\n",
      "[2000]\ttrain-rmse:0.478956\tval-rmse:0.490537\n",
      "[2050]\ttrain-rmse:0.47864\tval-rmse:0.490448\n",
      "[2100]\ttrain-rmse:0.47824\tval-rmse:0.490259\n",
      "[2150]\ttrain-rmse:0.47795\tval-rmse:0.490166\n",
      "[2200]\ttrain-rmse:0.477608\tval-rmse:0.490098\n",
      "[2250]\ttrain-rmse:0.47731\tval-rmse:0.49007\n",
      "[2300]\ttrain-rmse:0.477077\tval-rmse:0.490052\n",
      "[2350]\ttrain-rmse:0.476718\tval-rmse:0.489993\n",
      "[2400]\ttrain-rmse:0.476397\tval-rmse:0.489915\n",
      "[2450]\ttrain-rmse:0.476233\tval-rmse:0.489901\n",
      "[2500]\ttrain-rmse:0.47595\tval-rmse:0.489827\n",
      "[2550]\ttrain-rmse:0.475759\tval-rmse:0.489787\n",
      "[2600]\ttrain-rmse:0.475443\tval-rmse:0.489739\n",
      "[2650]\ttrain-rmse:0.475103\tval-rmse:0.489699\n",
      "[2700]\ttrain-rmse:0.474839\tval-rmse:0.489657\n",
      "[2750]\ttrain-rmse:0.474566\tval-rmse:0.489631\n",
      "[2800]\ttrain-rmse:0.474287\tval-rmse:0.489572\n",
      "[2850]\ttrain-rmse:0.47408\tval-rmse:0.489525\n",
      "[2900]\ttrain-rmse:0.473802\tval-rmse:0.489474\n",
      "[2950]\ttrain-rmse:0.473536\tval-rmse:0.489402\n",
      "[3000]\ttrain-rmse:0.473302\tval-rmse:0.489378\n",
      "[3050]\ttrain-rmse:0.473025\tval-rmse:0.489334\n",
      "[3100]\ttrain-rmse:0.472753\tval-rmse:0.489257\n",
      "[3150]\ttrain-rmse:0.472503\tval-rmse:0.489239\n",
      "[3200]\ttrain-rmse:0.472315\tval-rmse:0.489235\n",
      "[3250]\ttrain-rmse:0.47211\tval-rmse:0.489225\n",
      "[3300]\ttrain-rmse:0.471876\tval-rmse:0.489182\n",
      "[3350]\ttrain-rmse:0.471669\tval-rmse:0.489138\n",
      "[3400]\ttrain-rmse:0.471352\tval-rmse:0.489082\n",
      "[3450]\ttrain-rmse:0.471131\tval-rmse:0.489042\n",
      "[3500]\ttrain-rmse:0.470817\tval-rmse:0.488965\n",
      "[3550]\ttrain-rmse:0.470601\tval-rmse:0.488909\n",
      "[3600]\ttrain-rmse:0.47034\tval-rmse:0.488881\n",
      "[3650]\ttrain-rmse:0.469976\tval-rmse:0.488772\n",
      "[3700]\ttrain-rmse:0.469619\tval-rmse:0.488723\n",
      "[3750]\ttrain-rmse:0.469359\tval-rmse:0.488698\n",
      "[3800]\ttrain-rmse:0.468947\tval-rmse:0.488599\n",
      "[3850]\ttrain-rmse:0.46859\tval-rmse:0.488487\n",
      "[3900]\ttrain-rmse:0.468167\tval-rmse:0.488364\n",
      "[3950]\ttrain-rmse:0.467845\tval-rmse:0.488305\n",
      "[4000]\ttrain-rmse:0.467563\tval-rmse:0.488268\n",
      "[4050]\ttrain-rmse:0.467236\tval-rmse:0.488251\n",
      "[4100]\ttrain-rmse:0.466938\tval-rmse:0.488202\n",
      "[4150]\ttrain-rmse:0.466598\tval-rmse:0.488098\n",
      "[4200]\ttrain-rmse:0.466305\tval-rmse:0.488003\n",
      "[4250]\ttrain-rmse:0.466042\tval-rmse:0.487949\n",
      "[4300]\ttrain-rmse:0.465718\tval-rmse:0.487838\n",
      "[4350]\ttrain-rmse:0.46551\tval-rmse:0.487796\n",
      "[4400]\ttrain-rmse:0.465247\tval-rmse:0.487789\n",
      "[4450]\ttrain-rmse:0.464962\tval-rmse:0.487716\n",
      "[4500]\ttrain-rmse:0.46454\tval-rmse:0.487621\n",
      "[4550]\ttrain-rmse:0.464246\tval-rmse:0.487567\n",
      "[4600]\ttrain-rmse:0.463993\tval-rmse:0.487525\n",
      "[4650]\ttrain-rmse:0.463655\tval-rmse:0.487468\n",
      "[4700]\ttrain-rmse:0.463388\tval-rmse:0.487399\n",
      "[4750]\ttrain-rmse:0.463086\tval-rmse:0.487306\n",
      "[4800]\ttrain-rmse:0.462806\tval-rmse:0.487275\n",
      "[4850]\ttrain-rmse:0.462424\tval-rmse:0.487202\n",
      "[4900]\ttrain-rmse:0.462118\tval-rmse:0.487189\n",
      "[4950]\ttrain-rmse:0.461758\tval-rmse:0.487083\n",
      "[5000]\ttrain-rmse:0.461425\tval-rmse:0.487013\n",
      "[5050]\ttrain-rmse:0.460995\tval-rmse:0.486898\n",
      "[5100]\ttrain-rmse:0.460675\tval-rmse:0.486836\n",
      "[5150]\ttrain-rmse:0.460305\tval-rmse:0.486804\n",
      "[5200]\ttrain-rmse:0.459914\tval-rmse:0.486716\n",
      "[5250]\ttrain-rmse:0.45953\tval-rmse:0.486662\n",
      "[5300]\ttrain-rmse:0.459186\tval-rmse:0.486625\n",
      "[5350]\ttrain-rmse:0.458817\tval-rmse:0.486523\n",
      "[5400]\ttrain-rmse:0.458479\tval-rmse:0.486442\n",
      "[5450]\ttrain-rmse:0.458144\tval-rmse:0.486411\n",
      "[5500]\ttrain-rmse:0.457741\tval-rmse:0.486338\n",
      "[5550]\ttrain-rmse:0.457437\tval-rmse:0.486252\n",
      "[5600]\ttrain-rmse:0.457064\tval-rmse:0.486239\n",
      "[5650]\ttrain-rmse:0.456689\tval-rmse:0.486156\n",
      "[5700]\ttrain-rmse:0.456293\tval-rmse:0.486093\n",
      "[5750]\ttrain-rmse:0.455861\tval-rmse:0.48602\n",
      "[5800]\ttrain-rmse:0.455515\tval-rmse:0.486007\n",
      "[5850]\ttrain-rmse:0.455089\tval-rmse:0.485908\n",
      "[5900]\ttrain-rmse:0.454755\tval-rmse:0.485852\n",
      "[5950]\ttrain-rmse:0.454371\tval-rmse:0.485771\n",
      "[6000]\ttrain-rmse:0.454076\tval-rmse:0.485774\n",
      "[6050]\ttrain-rmse:0.453622\tval-rmse:0.485686\n",
      "[6100]\ttrain-rmse:0.453288\tval-rmse:0.485629\n",
      "[6150]\ttrain-rmse:0.452861\tval-rmse:0.485599\n",
      "[6200]\ttrain-rmse:0.452431\tval-rmse:0.4855\n",
      "[6250]\ttrain-rmse:0.452038\tval-rmse:0.485452\n",
      "[6300]\ttrain-rmse:0.451712\tval-rmse:0.485402\n",
      "[6350]\ttrain-rmse:0.451357\tval-rmse:0.48536\n",
      "[6400]\ttrain-rmse:0.451023\tval-rmse:0.48533\n",
      "[6450]\ttrain-rmse:0.450703\tval-rmse:0.485313\n",
      "[6500]\ttrain-rmse:0.45032\tval-rmse:0.485252\n",
      "[6550]\ttrain-rmse:0.450027\tval-rmse:0.485209\n",
      "[6600]\ttrain-rmse:0.449695\tval-rmse:0.485164\n",
      "[6650]\ttrain-rmse:0.449394\tval-rmse:0.485152\n",
      "[6700]\ttrain-rmse:0.449033\tval-rmse:0.48512\n",
      "[6750]\ttrain-rmse:0.448704\tval-rmse:0.485085\n",
      "[6800]\ttrain-rmse:0.448349\tval-rmse:0.485064\n",
      "[6850]\ttrain-rmse:0.448034\tval-rmse:0.485025\n",
      "[6900]\ttrain-rmse:0.447757\tval-rmse:0.485006\n",
      "[6950]\ttrain-rmse:0.447405\tval-rmse:0.485006\n",
      "[7000]\ttrain-rmse:0.447063\tval-rmse:0.484977\n",
      "[7050]\ttrain-rmse:0.446756\tval-rmse:0.484987\n",
      "Stopping. Best iteration:\n",
      "[6987]\ttrain-rmse:0.447157\tval-rmse:0.484963\n",
      "\n",
      "cv score is 0.488903\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"objective\" : \"reg:linear\",\n",
    "    #\"num_class\" : 3,\n",
    "    #\"tree_method\" : \"hist\",\n",
    "    \"eval_metric\" : \"rmse\",\n",
    "    \"nthread\": 4,\n",
    "    \"seed\" : 0,\n",
    "    'silent': 1,\n",
    "\n",
    "    \"eta\":0.05,  # default 0.3\n",
    "    \"max_depth\" : 5, # default 6\n",
    "    \"subsample\" : 0.8, # default 1\n",
    "    \"colsample_bytree\" : 0.6, # default 1\n",
    "    \"gamma\": 0.5\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "cv_score =cross_validate_xgb(xgb_params, train_input, y, test_input, kf, verbose=False, verbose_eval=50, scoreonly=True)\n",
    "\n",
    "print('cv score is {:.6f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimsation - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'max_depth':(4,10),\n",
    "        'learning_rate':(0.05,0.3),\n",
    "        'subsample': (0.4, 1),\n",
    "        'colsample_bytree': (0.4, 1),\n",
    "        'gamma': (0.001, 10.0),\n",
    "        'min_child_weight': (0, 20),\n",
    "        'max_delta_step': (0, 10),\n",
    "        'n_estimators': (10, 25),\n",
    "        'min_samples_split': (2, 20),\n",
    "        'max_features': (0.1, 0.999)\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(xgb_wrapper)\n",
    "def xgbcv_func(max_depth, learning_rate, subsample, \n",
    "               colsample_bytree, gamma, min_child_weight, \n",
    "               max_delta_step, n_estimators, \n",
    "               min_samples_split, max_features,nthread=4, seed=0):\n",
    "    params = {\n",
    "        \"objective\" : \"reg:linear\",\n",
    "        #\"num_class\" : 3,\n",
    "        #\"tree_method\" : \"hist\",\n",
    "        \"eval_metric\" : \"rmse\",\n",
    "        \"nthread\": nthread,\n",
    "        \"seed\" : 0,\n",
    "        'silent': 1,\n",
    "\n",
    "        \"eta\":learning_rate,  # default 0.3\n",
    "        \"max_depth\" : int(max_depth), # default 6\n",
    "        \"subsample\" : subsample, # default 1\n",
    "        \"colsample_bytree\" : colsample_bytree, # default 1\n",
    "\n",
    "        'gamma': gamma,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'max_delta_step': max_delta_step,\n",
    "        'n_estimators': n_estimators,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'max_features': max_features    \n",
    "\n",
    "    }\n",
    "    \n",
    "    # for a more ideal out-of-fold model prediction for this dataset, we use 10-fold CV\n",
    "    kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "    \n",
    "    # we will disable all the verbose setting in this functional call, so that we don't have too much information \n",
    "    # to read during the bayesian optimisation process.\n",
    "    return 1-cross_validate_xgb(params, train_input, y, test_input, kf, verbose=False, verbose_eval=False, scoreonly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bo=BayesianOptimization(xgbcv_func, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   learning_rate |   max_delta_step |   max_depth |   max_features |   min_child_weight |   min_samples_split |   n_estimators |   subsample | \n",
      "    1 | 30m42s | \u001b[35m   0.51366\u001b[0m | \u001b[32m            0.7611\u001b[0m | \u001b[32m   1.3094\u001b[0m | \u001b[32m         0.0853\u001b[0m | \u001b[32m          1.1432\u001b[0m | \u001b[32m     5.2611\u001b[0m | \u001b[32m        0.6137\u001b[0m | \u001b[32m            0.7916\u001b[0m | \u001b[32m             5.1929\u001b[0m | \u001b[32m       12.5610\u001b[0m | \u001b[32m     0.6471\u001b[0m | \n",
      "    2 | 22m04s |    0.50662 |             0.9664 |    3.6040 |          0.1771 |           0.7674 |      4.1586 |         0.8420 |             8.0657 |              5.6041 |        20.8680 |      0.4182 | \n",
      "    3 | 16m45s |    0.50627 |             0.5966 |    4.9819 |          0.2657 |           8.1701 |      5.0125 |         0.1827 |             7.3694 |              6.1190 |        15.5397 |      0.5667 | \n",
      "    4 | 15m15s |    0.50615 |             0.7010 |    4.8876 |          0.0555 |           5.1069 |      9.9668 |         0.9943 |             9.7238 |             15.2833 |        11.5916 |      0.9910 | \n",
      "    5 | 28m50s |    0.50641 |             0.6341 |    6.6831 |          0.0926 |           4.5208 |      7.1095 |         0.2670 |            13.3021 |             19.7116 |        16.4707 |      0.7669 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   learning_rate |   max_delta_step |   max_depth |   max_features |   min_child_weight |   min_samples_split |   n_estimators |   subsample | \n",
      "    6 | 03m36s |    0.49699 |             1.0000 |    0.0010 |          0.0500 |          10.0000 |      4.0000 |         0.8870 |             0.0000 |             20.0000 |        22.4970 |      1.0000 | \n",
      "    7 | 06m38s |    0.49489 |             0.5036 |    9.9687 |          0.2835 |           0.1127 |      4.4740 |         0.7094 |             0.0322 |             18.1089 |        12.4221 |      0.5023 | \n",
      "    8 | 09m26s |    0.50656 |             0.6678 |    0.0951 |          0.2821 |           0.8615 |      4.3378 |         0.7323 |            19.8805 |             16.1346 |        10.8690 |      0.9566 | \n",
      "    9 | 07m46s |    0.51138 |             0.7148 |    0.2300 |          0.1969 |           0.2297 |      9.9760 |         0.1015 |             0.3148 |              2.7562 |        23.8436 |      0.8221 | \n",
      "   10 | 03m37s |    0.50445 |             0.4201 |    0.0254 |          0.2698 |           9.8355 |      9.8983 |         0.1173 |            19.5989 |             17.2959 |        24.4228 |      0.5127 | \n",
      "   11 | 146m25s |    0.50856 |             0.8952 |    7.4374 |          0.0785 |           1.1708 |      9.4579 |         0.2864 |            19.9216 |              2.6506 |        10.4103 |      0.5218 | \n",
      "   12 | 05m51s |    0.50975 |             0.5190 |    0.5691 |          0.1768 |           0.5262 |      9.9261 |         0.2548 |            11.6698 |              2.6014 |        10.6812 |      0.6393 | \n",
      "   13 | 27m32s |    0.50451 |             0.7966 |    9.3431 |          0.2607 |           9.9185 |      7.0480 |         0.4694 |            19.7993 |             19.8878 |        10.0261 |      0.5943 | \n",
      "   14 | 19m31s |    0.50763 |             0.9501 |    7.6889 |          0.2595 |           1.0092 |      9.6483 |         0.2807 |             1.2338 |              2.5291 |        10.3349 |      0.8236 | \n",
      "   15 | 08m43s |    0.51185 |             0.9362 |    0.2907 |          0.1201 |           9.5648 |      8.4233 |         0.6069 |             0.4543 |              3.4509 |        10.2204 |      0.4933 | \n",
      "   16 | 19m11s |    0.50355 |             0.4631 |    9.2025 |          0.1558 |           0.4930 |      9.3983 |         0.3326 |            19.7492 |              7.0732 |        24.0770 |      0.9575 | \n",
      "   17 | 09m29s |    0.51145 |             0.9734 |    0.0300 |          0.1258 |           2.1612 |      8.0831 |         0.1536 |             0.6163 |             10.7682 |        10.8090 |      0.4585 | \n",
      "   18 | 92m29s |    0.50031 |             0.6113 |    9.6518 |          0.0914 |           0.1641 |      9.6397 |         0.2916 |            19.6358 |             16.5394 |        10.3128 |      0.4746 | \n",
      "   19 | 34m48s |    0.51128 |             0.8691 |    1.3650 |          0.1816 |           9.9914 |      4.2649 |         0.3471 |            19.1915 |              2.5496 |        10.9875 |      0.8184 | \n",
      "   20 | 39m57s |    0.51204 |             0.9775 |    0.0153 |          0.1531 |           0.1605 |      5.7657 |         0.3618 |             0.4623 |              2.0976 |        10.4289 |      0.6134 | \n",
      "   21 | 13m09s |    0.51077 |             0.4016 |    0.2407 |          0.1044 |           0.8549 |      8.5936 |         0.3986 |             0.0908 |             19.7509 |        23.7803 |      0.6775 | \n"
     ]
    }
   ],
   "source": [
    "xgb_bo.maximize(init_points=5, n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Final Results\n",
      "Maximum value: 0.486679\n",
      "Best parameters:  {'max_depth': 10.0, 'learning_rate': 0.05000000200182169, 'subsample': 1.0, 'colsample_bytree': 1.0, 'gamma': 0.001, 'min_child_weight': 20.0, 'max_delta_step': 0.0, 'n_estimators': 25.0, 'min_samples_split': 20.0, 'max_features': 0.10000000000000001}\n"
     ]
    }
   ],
   "source": [
    "print('-'*30)\n",
    "print('Maximum value: %f' % xgb_bo.res['max']['max_val'])\n",
    "print('Best parameters: ', xgb_bo.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Velification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Step |   Time |      Value |   colsample_bytree |     gamma |   learning_rate |   max_delta_step |   max_depth |   max_features |   min_child_weight |   min_samples_split |   n_estimators |   subsample | \n",
    "    1 | 30m42s |    0.51366 |             0.7611 |    1.3094 |          0.0853 |           1.1432 |      5.2611 |         0.6137 |             0.7916 |              5.1929 |        12.5610 |      0.6471 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 rmsle score is 0.488156\n",
      "fold cv 1 rmsle score is 0.490608\n",
      "fold cv 2 rmsle score is 0.484701\n",
      "cv rmsle score is 0.487829\n",
      "it takes 2269.535 seconds to perform cross validation\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"objective\" : \"reg:linear\",\n",
    "    \"eval_metric\" : \"rmse\",\n",
    "    \"nthread\": 4,\n",
    "    \"seed\" : 0,\n",
    "    'silent': 1,\n",
    "    \"colsample_bytree\"  :  0.7611, \n",
    "    \"gamma\": 1.3094,\n",
    "    \"learning_rate\": 0.0853,\n",
    "    \"max_delta_step\": 1.1432,\n",
    "    \"max_depth\": 5,#5.2611,\n",
    "    \"max_features\": 0.6137,\n",
    "    \"min_child_weight\": 0.7916,\n",
    "    \"min_samples_split\": 5.1929,\n",
    "    \"n_estimators\": 12.5610,\n",
    "    \"subsample\": 0.6471\n",
    "}\n",
    "print(\"Starting xgboost...\")\n",
    "outcomes=cross_validate_xgb(xgb_params, train_input, y, test_input, kf, verbose_eval=False)\n",
    "\n",
    "xgb_cv=outcomes[0]\n",
    "xgb_train_pred=outcomes[1]\n",
    "xgb_test_pred=outcomes[2]\n",
    "\n",
    "xgb_train_pred_df=pd.DataFrame(columns=['visitors'], data=xgb_train_pred)\n",
    "xgb_test_pred_df=pd.DataFrame(columns=['visitors'], data=xgb_test_pred)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['visitors'] = xgb_test_pred_df.values\n",
    "sub = test[['id','visitors']].copy()\n",
    "#sub.to_csv('submission_rs_recruit_v14_xgb_linear_feature_engineering_01.csv', index=False)\n",
    "#print('Good luck :)')\n",
    "\n",
    "#xgboost\n",
    "#Bopt\n",
    "#LB NOT SUBMITTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last work with weight# Consider weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../../../mltestdata/05_recruit/*.csv')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n",
    "    on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_merge['visitors'] = (sub_merge['visitors_x'] + sub_merge['visitors_y']* 1.1)/2\n",
    "sub_merge[['id', 'visitors']].to_csv('submission_rs_recruit_v14_xgb_linear_feature_engineering_01.csv', index=False)\n",
    "\n",
    "# gw_flag\n",
    "# xgb\n",
    "# Bopt\n",
    "# weight\n",
    "# LB 0.486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
