{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test H2O\n",
    "[Reference](https://www.kaggle.com/meli19/surprise-me-h2o-automl-version-ver5-lb-0-479/code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>9 hours 18 mins</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.16.0.4</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>12 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_suzukiry_vzadrg</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>0</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.2 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         9 hours 18 mins\n",
       "H2O cluster version:        3.16.0.4\n",
       "H2O cluster version age:    12 days\n",
       "H2O cluster name:           H2O_from_python_suzukiry_vzadrg\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    0\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.2 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Contributions from:\n",
    "DSEverything - Mean Mix - Math, Geo, Harmonic (LB 0.493) \n",
    "https://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493\n",
    "JdPaletto - Surprised Yet? - Part2 - (LB: 0.503)\n",
    "https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503\n",
    "hklee - weighted mean comparisons, LB 0.497, 1ST\n",
    "https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st\n",
    "\n",
    "Also all comments for changes, encouragement, and forked scripts rock\n",
    "\n",
    "Keep the Surprise Going\n",
    "\"\"\"\n",
    "\n",
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing done!\n"
     ]
    },
    {
     "ename": "H2OServerError",
     "evalue": "HTTP 500 Server Error:\n'<html>\\n<head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=ISO-8859-1\"/>\\n<title>Error 500 Server Error</title>\\n</head>\\n<body>\\n<h2>HTTP ERROR: 500</h2>\\n<p>Problem accessing /3/PostFile. Reason:\\n<pre>    Server Error</pre></p>\\n<hr /><i><small>Powered by Jetty://</small></i>\\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n</body>\\n</html>\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mH2OServerError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a5dac72658be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pre-processing done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m \u001b[0mhtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH2OFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0mhtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH2OFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/h2o/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, python_obj, destination_frame, header, separator, column_names, column_types, na_strings)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpython_obj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             self._upload_python_object(python_obj, destination_frame, header, separator,\n\u001b[0;32m--> 102\u001b[0;31m                                        column_names, column_types, na_strings)\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/h2o/frame.py\u001b[0m in \u001b[0;36m_upload_python_object\u001b[0;34m(self, python_obj, destination_frame, header, separator, column_names, column_types, na_strings)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mcsv_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_write\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mtmp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# close the streams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_upload_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# delete the tmp file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/h2o/frame.py\u001b[0m in \u001b[0;36m_upload_parse\u001b[0;34m(self, path, destination_frame, header, sep, column_names, column_types, na_strings)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_upload_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST /3/PostFile\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0mrawkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"destination_frame\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/h2o/h2o.py\u001b[0m in \u001b[0;36mapi\u001b[0;34m(endpoint, data, json, filename, save_to)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# type checks are performed in H2OConnection class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0m_check_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mh2oconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/h2o/backend/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, endpoint, data, json, filename, save_to)\u001b[0m\n\u001b[1;32m    400\u001b[0m                                     auth=self._auth, verify=self._verify_ssl_cert, proxies=self._proxies)\n\u001b[1;32m    401\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_end_transaction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/h2o/backend/connection.py\u001b[0m in \u001b[0;36m_process_response\u001b[0;34m(response, save_to)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;31m# Note that it is possible to receive valid H2OErrorV3 object in this case, however it merely means the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;31m# did not provide the correct status code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mH2OServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HTTP %d %s:\\n%r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mH2OServerError\u001b[0m: HTTP 500 Server Error:\n'<html>\\n<head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=ISO-8859-1\"/>\\n<title>Error 500 Server Error</title>\\n</head>\\n<body>\\n<h2>HTTP ERROR: 500</h2>\\n<p>Problem accessing /3/PostFile. Reason:\\n<pre>    Server Error</pre></p>\\n<hr /><i><small>Powered by Jetty://</small></i>\\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n                                                \\n</body>\\n</html>\\n'"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'tra': pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../../mltestdata/05_recruit/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "    }\n",
    "\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "# data['tra']['dom'] = data['tra']['visit_date'].dt.day\n",
    "data['tra']['doy'] = data['tra']['visit_date'].dt.dayofyear\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['week'] = data['tra']['visit_date'].dt.week\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "# data['tes']['dom'] = data['tes']['visit_date'].dt.day\n",
    "data['tes']['doy'] = data['tes']['visit_date'].dt.dayofyear\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['week'] = data['tes']['visit_date'].dt.week\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "#sure it can be compressed...\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "# tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "# stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(4):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name' +str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date'])\n",
    "\n",
    "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n",
    "\n",
    "# NEW FEATURES FROM JMBULL\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "\n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "# GW flag\n",
    "combine = [train, test]\n",
    "gw_list = ['2016-04-29','2016-04-30','2016-05-01','2016-05-02','2016-05-03','2016-05-04','2016-05-05','2017-04-29','2017-04-30','2017-05-01','2017-05-02','2017-05-03','2017-05-04','2017-05-05']\n",
    "post_gw_list=['2016-05-06']\n",
    "train['gw_flg'] = 0\n",
    "train['post_gw_flg'] = 0\n",
    "test['gw_flg'] = 0\n",
    "test['post_gw_flg'] = 0\n",
    "update_gw_list = [[\"0\" for i in range(3)] for j in range(len(gw_list))]\n",
    "update_post_gw_list = [[\"0\" for i in range(3)] for j in range(len(post_gw_list))]\n",
    "\n",
    "from datetime import date\n",
    "for index, gw_date in enumerate(gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_gw_list[index][col_i]=int(temp_figure)\n",
    "        \n",
    "    #print(\"{}  {}  {}\".format(update_list[index][0],update_list[index][1],update_list[index][2]))\n",
    "    \n",
    "for index, gw_date in enumerate(post_gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_post_gw_list[index][col_i]=int(temp_figure)\n",
    "\n",
    "for dataset in combine:\n",
    "    for index in range(len(update_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_gw_list[index][0],update_gw_list[index][1],update_gw_list[index][2]), 'gw_flg'] = 1\n",
    "        \n",
    "for dataset in combine:\n",
    "    for index in range(len(update_post_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_post_gw_list[index][0],update_post_gw_list[index][1],update_post_gw_list[index][2]), 'post_gw_flg'] = 1     \n",
    "\n",
    "train = train.fillna(-999)\n",
    "test = test.fillna(-999)\n",
    "\n",
    "train['visitors'] = np.log1p(train['visitors'].values)\n",
    "\n",
    "print('Pre-processing done!')\n",
    "\n",
    "htrain = h2o.H2OFrame(train)\n",
    "htest = h2o.H2OFrame(test)\n",
    "\n",
    "htrain.drop(['id', 'air_store_id', 'visit_date'])\n",
    "htest.drop(['id', 'air_store_id', 'visit_date'])\n",
    "\n",
    "x =htrain.columns\n",
    "y ='visitors'\n",
    "x.remove(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "def RMSLE(y_, pred):\n",
    "    return metrics.mean_squared_error(y_, pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting h2o autoML model!\n",
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "print('Starting h2o autoML model!')  \n",
    "\n",
    "#aml = H2OAutoML(max_runtime_secs = 8000)\n",
    "aml = H2OAutoML(max_runtime_secs = 3350)\n",
    "aml.train(x=x, y=y, training_frame=htrain, leaderboard_frame = htest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate predictions...\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "RMSLE H2O automl leader:  24.3894775844\n"
     ]
    }
   ],
   "source": [
    "print('Generate predictions...')\n",
    "htrain.drop(['visitors'])\n",
    "preds = aml.leader.predict(htrain)\n",
    "preds = preds.as_data_frame()\n",
    "print('RMSLE H2O automl leader: ', RMSLE(train_automl['visitors'].values, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "preds = aml.leader.predict(htest)\n",
    "preds = preds.as_data_frame()\n",
    "\n",
    "test['visitors'] = preds\n",
    "test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\n",
    "sub1 = test[['id','visitors']].copy()\n",
    "del train; del data; del htrain; del htest;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaderboard :  "
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                             </th><th style=\"text-align: right;\">  mean_residual_deviance</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">    mae</th><th style=\"text-align: right;\">    rmsle</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>StackedEnsemble_AllModels_0_AutoML_20180127_171601   </td><td style=\"text-align: right;\">                 7.66361</td><td style=\"text-align: right;\">2.76832</td><td style=\"text-align: right;\">2.6608 </td><td style=\"text-align: right;\">  1.29405</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180127_171601_model_2            </td><td style=\"text-align: right;\">                 7.66879</td><td style=\"text-align: right;\">2.76926</td><td style=\"text-align: right;\">2.66921</td><td style=\"text-align: right;\">  1.29592</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180127_171601_model_1            </td><td style=\"text-align: right;\">                 7.67258</td><td style=\"text-align: right;\">2.76994</td><td style=\"text-align: right;\">2.66953</td><td style=\"text-align: right;\">  1.29599</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180127_171601_model_0            </td><td style=\"text-align: right;\">                 7.67368</td><td style=\"text-align: right;\">2.77014</td><td style=\"text-align: right;\">2.66904</td><td style=\"text-align: right;\">  1.29594</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_0_AutoML_20180127_171601</td><td style=\"text-align: right;\">                 7.68574</td><td style=\"text-align: right;\">2.77232</td><td style=\"text-align: right;\">2.67007</td><td style=\"text-align: right;\">nan      </td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180127_171601_model_10           </td><td style=\"text-align: right;\">                 7.70084</td><td style=\"text-align: right;\">2.77504</td><td style=\"text-align: right;\">2.76247</td><td style=\"text-align: right;\">  1.32448</td></tr>\n",
       "<tr><td>DRF_0_AutoML_20180127_171601                         </td><td style=\"text-align: right;\">                 7.71299</td><td style=\"text-align: right;\">2.77723</td><td style=\"text-align: right;\">2.67763</td><td style=\"text-align: right;\">  1.2982 </td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180127_171601_model_3            </td><td style=\"text-align: right;\">                 7.72759</td><td style=\"text-align: right;\">2.77985</td><td style=\"text-align: right;\">2.68737</td><td style=\"text-align: right;\">  1.30108</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180127_171601_model_4            </td><td style=\"text-align: right;\">                 7.72809</td><td style=\"text-align: right;\">2.77994</td><td style=\"text-align: right;\">2.69803</td><td style=\"text-align: right;\">  1.3043 </td></tr>\n",
       "<tr><td>XRT_0_AutoML_20180127_171601                         </td><td style=\"text-align: right;\">                 7.7529 </td><td style=\"text-align: right;\">2.7844 </td><td style=\"text-align: right;\">2.69015</td><td style=\"text-align: right;\">  1.3018 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " H2O automl leader performace :  Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_0_AutoML_20180127_171601\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsRegressionGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.196135394689592\n",
      "RMSE: 0.44287175874014817\n",
      "MAE: 0.3248263679715371\n",
      "RMSLE: 0.13953665477829305\n",
      "R^2: 0.6993486552762503\n",
      "Mean Residual Deviance: 0.196135394689592\n",
      "Null degrees of freedom: 201535\n",
      "Residual degrees of freedom: 201529\n",
      "Null deviance: 131475.68969122303\n",
      "Residual deviance: 39528.34290416162\n",
      "AIC: 243658.31134907017\n",
      "\n",
      "ModelMetricsRegressionGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.22746901607569878\n",
      "RMSE: 0.4769371196244834\n",
      "MAE: 0.35049623829551546\n",
      "RMSLE: 0.14906579281667567\n",
      "R^2: 0.6514497531278864\n",
      "Mean Residual Deviance: 0.22746901607569878\n",
      "Null degrees of freedom: 50571\n",
      "Residual degrees of freedom: 50565\n",
      "Null deviance: 33004.18690112252\n",
      "Residual deviance: 11503.563080980239\n",
      "AIC: 68649.07286448657\n",
      "\n",
      "ModelMetricsRegressionGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.233583251279126\n",
      "RMSE: 0.48330451195817115\n",
      "MAE: 0.3538499512129127\n",
      "RMSLE: 0.1510221886341565\n",
      "R^2: 0.641945714524449\n",
      "Mean Residual Deviance: 0.233583251279126\n",
      "Null degrees of freedom: 201535\n",
      "Residual degrees of freedom: 201530\n",
      "Null deviance: 131477.11138260184\n",
      "Residual deviance: 47075.43412978994\n",
      "AIC: 278871.3697823143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../../../mltestdata/05_recruit/*.csv')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n",
    "    on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')\n",
    "\n",
    "sub_merge['visitors'] = 0.5*sub_merge['visitors_x'] + 0.5*sub_merge['visitors_y']*1.1\n",
    "sub_merge[['id', 'visitors']].to_csv('submission_automl_test.csv', index=False)\n",
    "\n",
    "print('Leaderboard : ', aml.leaderboard)\n",
    "\n",
    "print(' H2O automl leader performace : ', aml.leader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
