{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test LightGBM with Bayesian Optimsation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import time\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contributions from:\n",
    "DSEverything - Mean Mix - Math, Geo, Harmonic (LB 0.493) \n",
    "https://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493\n",
    "JdPaletto - Surprised Yet? - Part2 - (LB: 0.503)\n",
    "https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503\n",
    "hklee - weighted mean comparisons, LB 0.497, 1ST\n",
    "https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st\n",
    "\n",
    "Also all comments for changes, encouragement, and forked scripts rock\n",
    "\n",
    "Keep the Surprise Going\n",
    "\"\"\"\n",
    "\n",
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../../mltestdata/05_recruit/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "    }\n",
    "\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "#sure it can be compressed...\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n",
    "\n",
    "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n",
    "\n",
    "# NEW FEATURES FROM JMBULL\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "\n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, dtype in zip(train.columns, train.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        train[c] = train[c].astype(np.float32)\n",
    "\n",
    "for c, dtype in zip(test.columns, test.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        test[c] = test[c].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Golden week flag and Post Golden week flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = [train, test]\n",
    "gw_list = ['2016-04-29','2016-04-30','2016-05-01','2016-05-02','2016-05-03','2016-05-04','2016-05-05','2017-04-29','2017-04-30','2017-05-01','2017-05-02','2017-05-03','2017-05-04','2017-05-05']\n",
    "post_gw_list=['2016-05-06']\n",
    "train['gw_flg'] = 0\n",
    "train['post_gw_flg'] = 0\n",
    "test['gw_flg'] = 0\n",
    "test['post_gw_flg'] = 0\n",
    "update_gw_list = [[\"0\" for i in range(3)] for j in range(len(gw_list))]\n",
    "update_post_gw_list = [[\"0\" for i in range(3)] for j in range(len(post_gw_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "for index, gw_date in enumerate(gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_gw_list[index][col_i]=int(temp_figure)\n",
    "        \n",
    "    #print(\"{}  {}  {}\".format(update_list[index][0],update_list[index][1],update_list[index][2]))\n",
    "    \n",
    "for index, gw_date in enumerate(post_gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_post_gw_list[index][col_i]=int(temp_figure)\n",
    "\n",
    "for dataset in combine:\n",
    "    for index in range(len(update_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_gw_list[index][0],update_gw_list[index][1],update_gw_list[index][2]), 'gw_flg'] = 1\n",
    "        \n",
    "for dataset in combine:\n",
    "    for index in range(len(update_post_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_post_gw_list[index][0],update_post_gw_list[index][1],update_post_gw_list[index][2]), 'post_gw_flg'] = 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "y_train=train['visitors']\n",
    "x_train=train.drop(drop_cols, axis=1)\n",
    "\n",
    "x_test=test.copy()\n",
    "x_test=x_test.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.visitors\n",
    "train_input = train.copy()\n",
    "test_input = test.copy()\n",
    "\n",
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "train_input=train_input.drop(drop_cols, axis=1)\n",
    "test_input=test_input.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation function\n",
    "\n",
    "def rmsle(preds, true):\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n",
    "    return float(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation matrix \n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "RMSLE = make_scorer(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for comparing predictions and true data.\n",
    "def compare_result(preds, true):\n",
    "    compare = pd.DataFrame({\"test_id\": true.index,\n",
    "                           \"real_cost\": true,\n",
    "                           \"pred_cost\": preds})\n",
    "    compare = compare[[\"test_id\", \"real_cost\", \"pred_cost\"]].reset_index(drop=True)\n",
    "    \n",
    "    compare[\"error_percent_(%)\"] = np.abs(compare.real_cost - compare.pred_cost) / compare.real_cost * 100\n",
    "    \n",
    "    return compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_lgb(params, x_train, y_train, x_test, \n",
    "                        kf, \n",
    "                        cat_features=[],\n",
    "                        verbose=True, verbose_eval=100, nseeds=1, df_input=True,\n",
    "                        early_stopping=100, num_boost_round=8000, scoreonly=False):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "\n",
    "    # self-defined eval metric\n",
    "    # f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "    # binary error\n",
    "    def feval_rmsle(preds, train_data):\n",
    "        preds = np.expm1(preds)\n",
    "        true = np.expm1(train_data.get_label())\n",
    "\n",
    "        return 'rmsle', rmsle(preds, true), False\n",
    "       \n",
    "    if len(cat_features)==0: use_cat=False\n",
    "\n",
    "    # use the k-fold object to enumerate indexes for each training and validation fold\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)): # folds 1, 2 ,3 ,4, 5\n",
    "        # example: training from 1,2,3,4; validation from 5\n",
    "        if df_input:\n",
    "            x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "        else:\n",
    "            x_train_kf, x_val_kf = x_train[train_index], x_train[val_index]\n",
    "\n",
    "        y_train_kf, y_val_kf = np.log1p(y_train[train_index]), np.log1p(y_train[val_index])\n",
    "\n",
    "        for seed in range(nseeds):\n",
    "            params['feature_fraction_seed'] = seed\n",
    "            params['bagging_seed'] = seed\n",
    "\n",
    "            if use_cat:\n",
    "                lgb_train = lgb.Dataset(x_train_kf, y_train_kf, categorical_feature=cat_features)\n",
    "                lgb_val = lgb.Dataset(x_val_kf, y_val_kf, reference=lgb_train, categorical_feature=cat_features)\n",
    "\n",
    "            else:\n",
    "                lgb_train = lgb.Dataset(x_train_kf, y_train_kf)\n",
    "                lgb_val = lgb.Dataset(x_val_kf, y_val_kf, reference=lgb_train)\n",
    "\n",
    "            gbm = lgb.train(params,\n",
    "                            lgb_train,\n",
    "                            num_boost_round=num_boost_round,\n",
    "                            valid_sets=[lgb_val],\n",
    "                            early_stopping_rounds=early_stopping,\n",
    "                            feval=feval_rmsle,\n",
    "                            verbose_eval=verbose_eval)\n",
    "\n",
    "            val_pred = np.expm1(gbm.predict(x_val_kf, num_iteration=gbm.best_iteration))\n",
    "      \n",
    "            train_pred[val_index] += val_pred\n",
    "            test_pred += np.expm1((gbm.predict(x_test, num_iteration=gbm.best_iteration)))\n",
    "\n",
    "        train_pred[val_index] = val_pred/nseeds\n",
    "\n",
    "        #fold_rmsle = rmsle(np.expm1(y_val_kf.values), train_pred[val_index])\n",
    "        fold_rmsle = rmsle(train_pred[val_index],np.expm1(y_val_kf.values))\n",
    "        if verbose:\n",
    "            print('fold cv {} RMSLE score is {:.6f}'.format(i, fold_rmsle))\n",
    "\n",
    "    test_pred = test_pred / (nseeds * kf.n_splits)\n",
    "    #cv_score = rmsle(y_train, train_pred)\n",
    "    cv_score = rmsle(train_pred, y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print('cv RMSLE score is {:.6f}'.format(cv_score))\n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    "    #return cv_score, np.expm1(train_pred),test_pred\n",
    "    \n",
    "    if scoreonly:\n",
    "        return cv_score\n",
    "    else:\n",
    "        return cv_score, train_pred, test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do 5 fold CV here so that we save some running time on Kaggle Kernel\n",
    "kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)\n",
    "#kf=KFold(n_splits=3, shuffle=True, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's rmsle: 0.577244\n",
      "[100]\tvalid_0's rmsle: 0.583161\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's rmsle: 0.51107\n",
      "fold cv 0 RMSLE score is 1.779715\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's rmsle: 0.582782\n",
      "[100]\tvalid_0's rmsle: 0.588787\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's rmsle: 0.514722\n",
      "fold cv 1 RMSLE score is 1.783542\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's rmsle: 0.577298\n",
      "[100]\tvalid_0's rmsle: 0.583508\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's rmsle: 0.508429\n",
      "fold cv 2 RMSLE score is 1.780813\n",
      "cv RMSLE score is 1.781357\n",
      "it takes 46.408 seconds to perform cross validation\n",
      "cv score is 1.781357\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "    'boosting_type': 'dart',\n",
    "    'max_depth' : 5,\n",
    "    'max_bin' : 500,\n",
    "    'learning_rate': 0.1,  # 0.618580\n",
    "    'num_leaves': 22,\n",
    "    #'metric': 'RMSE'\n",
    "}\n",
    "\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "#cv_score =cross_validate_lgb(lgb_params, train_input, y, kf, verbose=True, verbose_eval=50,df_input=True)\n",
    "#cv_score =cross_validate_lgb_nofeval(lgb_params, train_input, y, kf, verbose=True, verbose_eval=50,df_input=True)\n",
    "cv_score =cross_validate_lgb(lgb_params, train_input, y, test_input, kf, verbose=True, verbose_eval=50,df_input=True,scoreonly=True)\n",
    "\n",
    "print('cv score is {:.6f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimsation - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'num_leaves':(7,4095),#(7,4095),\n",
    "    'max_depth':(2,63),\n",
    "    'learning_rate':(0.05,0.3),\n",
    "    'scale_pos_weight':(1,10000),\n",
    "    'min_sum_hessian_in_leaf':(2,30),\n",
    "    'subsample':(0.4,1.0),\n",
    "    'colsample_bytree':(0.4,1.0),\n",
    "    'feature_fraction':(0.1,0.9),\n",
    "    'bagging_fraction':(0.1,0.9),\n",
    "    'bagging_freq':(0,2),\n",
    "    'lambda_l1':(0.0,1.0),\n",
    "    'lambda_l2':(0.0,1.0),\n",
    "    'n_estimators':(2,30), \n",
    "    'reg_lambda':(0.0,2.0),\n",
    "    'min_gain_to_split':(0.0,1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(lgb_wrapper)\n",
    "#def lgbcv_func(max_depth, learning_rate, subsample, colsample_bytree, nthread=4, seed=0):\n",
    "def lgbcv_func(num_leaves, max_depth, learning_rate,\n",
    "               scale_pos_weight, \n",
    "               min_sum_hessian_in_leaf, \n",
    "               subsample, \n",
    "               colsample_bytree,\n",
    "               feature_fraction, bagging_fraction, \n",
    "               bagging_freq, lambda_l1, lambda_l2,\n",
    "               n_estimators,reg_lambda,min_gain_to_split,\n",
    "               nthread=4):\n",
    "\n",
    "    params = {\n",
    "        'objective' : \"regression\",\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'dart',\n",
    "                \n",
    "        'num_leaves': int(num_leaves),\n",
    "        'max_depth': int(max_depth), \n",
    "        'learning_rate': float(learning_rate),\n",
    "        'scale_pos_weight':scale_pos_weight,\n",
    "        'min_sum_hessian_in_leaf':float(min_sum_hessian_in_leaf), \n",
    "        'subsample':subsample,\n",
    "        'colsample_bytree':colsample_bytree,\n",
    "        'feature_fraction':feature_fraction, \n",
    "        'bagging_fraction':bagging_fraction,\n",
    "        'bagging_freq':int(bagging_freq), \n",
    "        'lambda_l1':lambda_l1, \n",
    "        'lambda_l2':lambda_l2,\n",
    "        'n_estimators':n_estimators,\n",
    "        'reg_lambda':reg_lambda,\n",
    "        'min_gain_to_split':min_gain_to_split       \n",
    "        #'metric': 'RMSE'\n",
    "    }\n",
    "    \n",
    "    # for a more ideal out-of-fold model prediction for this dataset, we use 10-fold CV\n",
    "    kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)\n",
    "    \n",
    "    # we will disable all the verbose setting in this functional call, so that we don't have too much information \n",
    "    # to read during the bayesian optimisation process.\n",
    "    return 1-cross_validate_lgb(params, train_input, y, test_input, kf, verbose=False, verbose_eval=False, scoreonly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_bo=BayesianOptimization(lgbcv_func, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   bagging_fraction |   bagging_freq |   colsample_bytree |   feature_fraction |   lambda_l1 |   lambda_l2 |   learning_rate |   max_depth |   min_gain_to_split |   min_sum_hessian_in_leaf |   n_estimators |   num_leaves |   reg_lambda |   scale_pos_weight |   subsample | \n",
      "    1 | 01m51s | \u001b[35m  -0.74229\u001b[0m | \u001b[32m            0.2810\u001b[0m | \u001b[32m        0.6125\u001b[0m | \u001b[32m            0.6848\u001b[0m | \u001b[32m            0.5255\u001b[0m | \u001b[32m     0.3627\u001b[0m | \u001b[32m     0.5175\u001b[0m | \u001b[32m         0.1646\u001b[0m | \u001b[32m    10.6387\u001b[0m | \u001b[32m             0.7193\u001b[0m | \u001b[32m                  27.8901\u001b[0m | \u001b[32m       28.3943\u001b[0m | \u001b[32m   1055.4091\u001b[0m | \u001b[32m      0.2843\u001b[0m | \u001b[32m         4704.8618\u001b[0m | \u001b[32m     0.8410\u001b[0m | \n",
      "    2 | 03m41s |   -0.77583 |             0.2905 |         1.0026 |             0.9072 |             0.7064 |      0.6714 |      0.9219 |          0.1483 |     37.0390 |              0.0013 |                   27.2142 |        27.9853 |    1015.4974 |       0.4028 |           669.3453 |      0.7605 | \n",
      "    3 | 06m05s | \u001b[35m   0.40111\u001b[0m | \u001b[32m            0.4440\u001b[0m | \u001b[32m        1.4840\u001b[0m | \u001b[32m            0.5399\u001b[0m | \u001b[32m            0.8035\u001b[0m | \u001b[32m     0.8079\u001b[0m | \u001b[32m     0.8392\u001b[0m | \u001b[32m         0.2812\u001b[0m | \u001b[32m    48.1343\u001b[0m | \u001b[32m             0.9464\u001b[0m | \u001b[32m                  12.4297\u001b[0m | \u001b[32m       28.7551\u001b[0m | \u001b[32m   2521.1964\u001b[0m | \u001b[32m      0.3330\u001b[0m | \u001b[32m         5656.3176\u001b[0m | \u001b[32m     0.5956\u001b[0m | \n",
      "    4 | 05m53s | \u001b[35m   0.43390\u001b[0m | \u001b[32m            0.2646\u001b[0m | \u001b[32m        1.4610\u001b[0m | \u001b[32m            0.6710\u001b[0m | \u001b[32m            0.6093\u001b[0m | \u001b[32m     0.6890\u001b[0m | \u001b[32m     0.5859\u001b[0m | \u001b[32m         0.1897\u001b[0m | \u001b[32m     3.7515\u001b[0m | \u001b[32m             0.9652\u001b[0m | \u001b[32m                  13.5790\u001b[0m | \u001b[32m       27.9406\u001b[0m | \u001b[32m   1176.1187\u001b[0m | \u001b[32m      1.0507\u001b[0m | \u001b[32m         7629.9892\u001b[0m | \u001b[32m     0.5054\u001b[0m | \n",
      "    5 | 00m51s |   -0.78184 |             0.7196 |         0.6488 |             0.4634 |             0.8442 |      0.9102 |      0.1547 |          0.1017 |      4.4507 |              0.5149 |                   19.5887 |        19.5481 |    2267.8214 |       0.6856 |          1710.6646 |      0.7671 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   bagging_fraction |   bagging_freq |   colsample_bytree |   feature_fraction |   lambda_l1 |   lambda_l2 |   learning_rate |   max_depth |   min_gain_to_split |   min_sum_hessian_in_leaf |   n_estimators |   num_leaves |   reg_lambda |   scale_pos_weight |   subsample | \n",
      "    6 | 02m36s |   -0.81362 |             0.9000 |         2.0000 |             1.0000 |             0.1000 |      1.0000 |      1.0000 |          0.0500 |     63.0000 |              0.0000 |                    2.0000 |         2.0000 |    4095.0000 |       0.0000 |         10000.0000 |      1.0000 | \n",
      "    7 | 01m05s |   -0.79450 |             0.9000 |         0.0000 |             0.4000 |             0.9000 |      0.0000 |      0.0000 |          0.0500 |     63.0000 |              0.0000 |                   30.0000 |         2.0000 |       7.0000 |       2.0000 |         10000.0000 |      1.0000 | \n",
      "    8 | 08m39s | \u001b[35m   0.47346\u001b[0m | \u001b[32m            0.1000\u001b[0m | \u001b[32m        0.0000\u001b[0m | \u001b[32m            0.4000\u001b[0m | \u001b[32m            0.9000\u001b[0m | \u001b[32m     0.0000\u001b[0m | \u001b[32m     0.0000\u001b[0m | \u001b[32m         0.3000\u001b[0m | \u001b[32m     2.0000\u001b[0m | \u001b[32m             0.0000\u001b[0m | \u001b[32m                  30.0000\u001b[0m | \u001b[32m        2.0000\u001b[0m | \u001b[32m   4095.0000\u001b[0m | \u001b[32m      2.0000\u001b[0m | \u001b[32m         6751.8602\u001b[0m | \u001b[32m     0.4000\u001b[0m | \n",
      "    9 | 05m15s |    0.34457 |             0.9000 |         2.0000 |             1.0000 |             0.9000 |      0.0000 |      1.0000 |          0.3000 |     63.0000 |              1.0000 |                    2.0000 |         2.0000 |    4095.0000 |       2.0000 |             1.0000 |      1.0000 | \n",
      "   10 | 05m55s |   -0.97410 |             0.1000 |         2.0000 |             1.0000 |             0.1000 |      0.0000 |      1.0000 |          0.2906 |     63.0000 |              0.0000 |                    2.0000 |         2.0000 |    4095.0000 |       2.0000 |          4289.6587 |      0.4000 | \n",
      "   11 | 01m30s |   -0.79116 |             0.1000 |         2.0000 |             0.4000 |             0.9000 |      1.0000 |      0.0000 |          0.0500 |     63.0000 |              1.0000 |                    2.0000 |        30.0000 |    2718.0273 |       0.0000 |          7414.4904 |      0.4000 | \n",
      "   12 | 02m27s |   -0.77696 |             0.8766 |         1.8070 |             0.6910 |             0.3561 |      0.2112 |      0.5078 |          0.1453 |     15.5648 |              0.2765 |                   10.8666 |        28.6877 |    1169.1556 |       0.3598 |          7633.5717 |      0.5426 | \n",
      "   13 | 08m39s |    0.43113 |             0.8778 |         0.3139 |             0.4750 |             0.7270 |      0.7854 |      0.7799 |          0.2033 |     61.0900 |              0.3344 |                   11.2304 |        29.6398 |    2373.9976 |       0.5263 |          4660.6267 |      0.9241 | \n",
      "   14 | 01m38s |   -0.77636 |             0.2019 |         1.9792 |             0.5200 |             0.7544 |      0.1078 |      0.4922 |          0.1614 |     44.4040 |              0.6424 |                    7.5718 |        26.7374 |    3271.3526 |       0.0019 |          2263.0731 |      0.6895 | \n",
      "   15 | 04m41s |   -0.77818 |             0.6156 |         0.0534 |             0.7817 |             0.3015 |      0.3201 |      0.2001 |          0.1170 |     38.9803 |              0.0124 |                   28.7698 |         3.1926 |    2996.0313 |       0.0545 |           848.3092 |      0.6310 | \n",
      "   16 | 10m05s |    0.45060 |             0.6476 |         1.6497 |             0.9644 |             0.4717 |      0.7581 |      0.5501 |          0.2160 |     10.0635 |              0.3887 |                   13.3642 |        27.4465 |    3965.0235 |       0.2263 |          5664.9355 |      0.7977 | \n",
      "   17 | 242m14s |   -0.77702 |             0.6113 |         1.3878 |             0.6372 |             0.3327 |      0.7407 |      0.8220 |          0.1478 |     45.4611 |              0.3190 |                   10.9153 |        24.0495 |    1986.0703 |       1.2944 |          9321.5410 |      0.6303 | \n",
      "   18 | 144m55s |    0.42901 |             0.2257 |         1.7821 |             0.5667 |             0.3531 |      0.1648 |      0.1550 |          0.1934 |     59.2831 |              0.7581 |                   20.6178 |        26.0084 |    2386.1518 |       0.5464 |          9719.4721 |      0.8528 | \n",
      "   19 | 02m33s |   -0.45336 |             0.2871 |         0.5325 |             0.7837 |             0.5142 |      0.1835 |      0.5722 |          0.1825 |     62.7414 |              0.5308 |                    5.7380 |        14.8303 |     933.2377 |       0.3879 |          6151.6033 |      0.4511 | \n",
      "   20 | 05m29s |    0.40268 |             0.4591 |         1.4517 |             0.7636 |             0.4264 |      0.7015 |      0.6485 |          0.2700 |     20.8461 |              0.1562 |                    8.5747 |        25.7756 |     237.0972 |       0.9586 |          9716.6725 |      0.8653 | \n"
     ]
    }
   ],
   "source": [
    "lgb_bo.maximize(init_points=5, n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Maximum value: 0.473458\n",
      "Best parameters:  {'num_leaves': 4095.0, 'max_depth': 2.0, 'learning_rate': 0.29999999999999999, 'scale_pos_weight': 6751.8601605304248, 'min_sum_hessian_in_leaf': 30.0, 'subsample': 0.40000000000000002, 'colsample_bytree': 0.40000000000000002, 'feature_fraction': 0.90000000000000002, 'bagging_fraction': 0.10000000000000001, 'bagging_freq': 0.0, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'n_estimators': 2.0, 'reg_lambda': 2.0, 'min_gain_to_split': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print('-'*30)\n",
    "print('Maximum value: %f' % lgb_bo.res['max']['max_val'])\n",
    "print('Best parameters: ', lgb_bo.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Velification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 RMSLE score is 0.498071\n",
      "fold cv 1 RMSLE score is 0.501691\n",
      "fold cv 2 RMSLE score is 0.494472\n",
      "cv RMSLE score is 0.498088\n",
      "it takes 457.688 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {'num_leaves': 4095, \n",
    "              'max_depth': 2, \n",
    "              'learning_rate': 0.29999999999999999, \n",
    "              'scale_pos_weight': 6751.8601605304248, \n",
    "              'min_sum_hessian_in_leaf': 30.0, \n",
    "              'subsample': 0.40000000000000002, \n",
    "              'colsample_bytree': 0.40000000000000002, \n",
    "              'feature_fraction': 0.90000000000000002, \n",
    "              'bagging_fraction': 0.10000000000000001, \n",
    "              #'bagging_freq': 0.0, \n",
    "              #'lambda_l1': 0.0, \n",
    "              #'lambda_l2': 0.0, \n",
    "              'n_estimators': 2.0, \n",
    "              'reg_lambda': 2.0, \n",
    "              'min_gain_to_split': 0.0} \n",
    "\n",
    "outcomes=cross_validate_lgb(lgb_params, train_input, y, test_input, kf, verbose_eval=False)\n",
    "\n",
    "lgb_cv=outcomes[0]\n",
    "lgb_train_pred=outcomes[1]\n",
    "lgb_test_pred=outcomes[2]\n",
    "\n",
    "lgb_train_pred_df=pd.DataFrame(columns=['visitors'], data=lgb_train_pred)\n",
    "lgb_test_pred_df=pd.DataFrame(columns=['visitors'], data=lgb_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv1_lgb_train_pred = lgb_train_pred_df.copy()\n",
    "lv1_lgb_train_pred.to_csv('lv1_lgb_train_pred.csv', index=False)\n",
    "\n",
    "lv1_lgb_test_pred = lgb_test_pred_df.copy()\n",
    "lv1_lgb_test_pred.to_csv('lv1_lgb_test_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lv1_lgb_test_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-99b25228c4f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'visitors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlv1_lgb_test_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'visitors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission_rs_recruit_v15_lgbm_feature_engineering_shinji_01.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lv1_lgb_test_pred' is not defined"
     ]
    }
   ],
   "source": [
    "test['visitors'] = lv1_lgb_test_pred.values\n",
    "sub = test[['id','visitors']].copy()\n",
    "sub.to_csv('submission_rs_recruit_v15_lgbm_feature_engineering_shinji_01.csv', index=False)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last work with weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../../../mltestdata/05_recruit/*.csv')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n",
    "    on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_merge['visitors'] = (sub_merge['visitors_x'] + sub_merge['visitors_y']* 1.1)/2\n",
    "sub_merge[['id', 'visitors']].to_csv('submission_rs_recruit_v15_lgbm_feature_engineering_02.csv', index=False)\n",
    "# LB Not submitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_automl = pd.read_csv('../../../mltestdata/05_recruit/submission_automl.csv')\n",
    "sub_lgbv15 = pd.read_csv('./submission_rs_recruit_v15_lgbm_feature_engineering_02.csv')\n",
    "sub_blending = pd.DataFrame()\n",
    "sub_blending['id'] = sub_lgbboptweight['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_automl = 0.8\n",
    "rate_lgbboptweight = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_blending['visitors'] = rate_automl*sub_automl['visitors']+rate_lgbboptweight*sub_lgbboptweight['visitors']\n",
    "sub_blending.to_csv('submission_rs_recruit_v11_lgbm_v04_03.csv', index=False)\n",
    "# LB 0.480"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
