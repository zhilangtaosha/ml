{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contributions from:\n",
    "DSEverything - Mean Mix - Math, Geo, Harmonic (LB 0.493) \n",
    "https://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493\n",
    "JdPaletto - Surprised Yet? - Part2 - (LB: 0.503)\n",
    "https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503\n",
    "hklee - weighted mean comparisons, LB 0.497, 1ST\n",
    "https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st\n",
    "\n",
    "Also all comments for changes, encouragement, and forked scripts rock\n",
    "\n",
    "Keep the Surprise Going\n",
    "\"\"\"\n",
    "\n",
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../../mltestdata/05_recruit/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "    }\n",
    "\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "#sure it can be compressed...\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n",
    "\n",
    "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n",
    "\n",
    "# NEW FEATURES FROM JMBULL\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "\n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, dtype in zip(train.columns, train.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        train[c] = train[c].astype(np.float32)\n",
    "\n",
    "for c, dtype in zip(test.columns, test.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        test[c] = test[c].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = [train, test]\n",
    "gw_list = ['2016-04-29','2016-04-30','2016-05-01','2016-05-02','2016-05-03','2016-05-04','2016-05-05','2017-04-29','2017-04-30','2017-05-01','2017-05-02','2017-05-03','2017-05-04','2017-05-05']\n",
    "post_gw_list=['2016-05-06']\n",
    "train['gw_flg'] = 0\n",
    "train['post_gw_flg'] = 0\n",
    "test['gw_flg'] = 0\n",
    "test['post_gw_flg'] = 0\n",
    "update_gw_list = [[\"0\" for i in range(3)] for j in range(len(gw_list))]\n",
    "update_post_gw_list = [[\"0\" for i in range(3)] for j in range(len(post_gw_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "for index, gw_date in enumerate(gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_gw_list[index][col_i]=int(temp_figure)\n",
    "        \n",
    "    #print(\"{}  {}  {}\".format(update_list[index][0],update_list[index][1],update_list[index][2]))\n",
    "    \n",
    "for index, gw_date in enumerate(post_gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_post_gw_list[index][col_i]=int(temp_figure)\n",
    "\n",
    "for dataset in combine:\n",
    "    for index in range(len(update_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_gw_list[index][0],update_gw_list[index][1],update_gw_list[index][2]), 'gw_flg'] = 1\n",
    "        \n",
    "for dataset in combine:\n",
    "    for index in range(len(update_post_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_post_gw_list[index][0],update_post_gw_list[index][1],update_post_gw_list[index][2]), 'post_gw_flg'] = 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "y_train=train['visitors']\n",
    "x_train=train.drop(drop_cols, axis=1)\n",
    "\n",
    "x_test=test.copy()\n",
    "x_test=x_test.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.visitors\n",
    "train_input = train.copy()\n",
    "test_input = test.copy()\n",
    "\n",
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "train_input=train_input.drop(drop_cols, axis=1)\n",
    "test_input=test_input.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation function\n",
    "\n",
    "def rmsle(preds, true):\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n",
    "    return float(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation matrix \n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "RMSLE = make_scorer(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_xgb(params, x_train, y_train, x_test, kf,  verbose=True, verbose_eval=50, scoreonly=False):\n",
    "    start_time=time.time()\n",
    "    nround=[]\n",
    "    # the prediction matrix need to contains 3 columns, one for the probability of each class\n",
    "    #train_pred = np.zeros((x_train.shape[0],3))\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "    \n",
    "    # self-defined eval metric\n",
    "    # f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "    # binary error\n",
    "    def feval_rmsle(preds, train_data):\n",
    "        preds = np.expm1(preds)\n",
    "        true = np.expm1(train_data.get_label())\n",
    "        #return 'rmsle', rmsle(true, preds), False\n",
    "\n",
    "        return 'rmsle', rmsle(preds, true), False\n",
    "\n",
    "    # use the k-fold object to enumerate indexes for each training and validation fold\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "\n",
    "        #y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n",
    "        y_train_kf, y_val_kf = np.log1p(y_train[train_index]), np.log1p(y_train[val_index])\n",
    "        x_test_kf=x_test.copy()\n",
    "        \n",
    "        d_train = xgboost.DMatrix(x_train_kf, y_train_kf)\n",
    "        d_val=xgboost.DMatrix(x_val_kf, y_val_kf)\n",
    "        d_test = xgboost.DMatrix(x_test_kf)\n",
    "        \n",
    "        watchlist= [(d_train, \"train\"), (d_val, 'val')]\n",
    "        bst = xgboost.train(params=params, \n",
    "                            dtrain=d_train, \n",
    "                            num_boost_round=8000, \n",
    "                            early_stopping_rounds=100,\n",
    "                            evals=watchlist, \n",
    "                            verbose_eval=verbose_eval)        \n",
    "        \n",
    "#        y_val_kf_preds=bst.predict(d_val, ntree_limit=bst.best_ntree_limit)\n",
    "        y_val_kf_preds=np.expm1(bst.predict(d_val, ntree_limit=bst.best_ntree_limit))\n",
    "        nround.append(bst.best_ntree_limit)\n",
    "        \n",
    "        train_pred[val_index] += y_val_kf_preds\n",
    "#        test_pred += np.expm1((bst.predict(x_test, ntree_limit=bst.best_ntree_limit)))\n",
    "        test_pred += np.expm1(bst.predict(d_test))\n",
    "        \n",
    "        \n",
    "        #fold_cv = log_loss(y_val_kf.values, y_val_kf_preds)\n",
    "        #fold_rmsle = rmsle(np.expm1(train_pred[val_index]),np.expm1(y_val_kf.values))\n",
    "        fold_rmsle = rmsle(train_pred[val_index],np.expm1(y_val_kf.values))\n",
    "        fold_cv = fold_rmsle\n",
    "        \n",
    "        if verbose:\n",
    "            print('fold cv {} rmsle score is {:.6f}'.format(i, fold_cv))\n",
    "\n",
    "    test_pred = test_pred / kf.n_splits\n",
    "    #cv_score = log_loss(y_train, train_pred)\n",
    "    #cv_score = rmsle(np.expm1(train_pred), y_train)\n",
    "    cv_score = rmsle(train_pred, y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print('cv rmsle score is {:.6f}'.format(cv_score))    \n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    " \n",
    "    if scoreonly:\n",
    "        #return cv_score # for the purpose of bayesian optimisation, we only need to return the CV score\n",
    "        return cv_score\n",
    "    else:\n",
    "        return (cv_score,train_pred,test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do 3 fold CV here so that we save some running time on Kaggle Kernel\n",
    "kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 2 ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv1_lgb_train_pred = pd.read_csv('./lv1_lgb_train_pred.csv')\n",
    "lv1_lgb_test_pred = pd.read_csv('./lv1_lgb_test_pred.csv')\n",
    "\n",
    "lv1_knr_train_pred = pd.read_csv('./lv1_knr_train_pred.csv')\n",
    "lv1_knr_test_pred = pd.read_csv('./lv1_knr_test_pred.csv')\n",
    "\n",
    "lv1_xgb_train_pred = pd.read_csv('./lv1_xgb_train_pred.csv')\n",
    "lv1_xgb_test_pred = pd.read_csv('./lv1_xgb_test_pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate L1 output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['lgb','knr','xgb']\n",
    "train_pred_df_list=[lv1_lgb_train_pred, lv1_knr_train_pred, lv1_xgb_train_pred]\n",
    "\n",
    "test_pred_df_list=[lv1_lgb_test_pred, lv1_knr_test_pred, lv1_xgb_test_pred]\n",
    "\n",
    "lv1_train_df=pd.DataFrame(columns=columns)\n",
    "lv1_test_df=pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(columns)):\n",
    "    lv1_train_df[columns[i]]=train_pred_df_list[i]['visitors']\n",
    "    lv1_test_df[columns[i]]=test_pred_df_list[i]['visitors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgb</th>\n",
       "      <th>knr</th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.834165</td>\n",
       "      <td>23.417467</td>\n",
       "      <td>19.410961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.039239</td>\n",
       "      <td>21.654400</td>\n",
       "      <td>15.668269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.986361</td>\n",
       "      <td>32.959013</td>\n",
       "      <td>27.102802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.550680</td>\n",
       "      <td>14.427886</td>\n",
       "      <td>22.898842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.909067</td>\n",
       "      <td>19.189961</td>\n",
       "      <td>10.465945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lgb        knr        xgb\n",
       "0  19.834165  23.417467  19.410961\n",
       "1  15.039239  21.654400  15.668269\n",
       "2  27.986361  32.959013  27.102802\n",
       "3  23.550680  14.427886  22.898842\n",
       "4   9.909067  19.189961  10.465945"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lv1_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         25\n",
       "1         32\n",
       "2         29\n",
       "3         22\n",
       "4          6\n",
       "5          9\n",
       "6         31\n",
       "7         21\n",
       "8         18\n",
       "9         26\n",
       "10        21\n",
       "11        11\n",
       "12        24\n",
       "13        21\n",
       "14        26\n",
       "15         6\n",
       "16        18\n",
       "17        12\n",
       "18        45\n",
       "19        15\n",
       "20        19\n",
       "21        15\n",
       "22        32\n",
       "23         3\n",
       "24        26\n",
       "25         8\n",
       "26        14\n",
       "27        15\n",
       "28        17\n",
       "29        22\n",
       "          ..\n",
       "252078     4\n",
       "252079     7\n",
       "252080     2\n",
       "252081     8\n",
       "252082     8\n",
       "252083     7\n",
       "252084     2\n",
       "252085     4\n",
       "252086     7\n",
       "252087    11\n",
       "252088     7\n",
       "252089     8\n",
       "252090     6\n",
       "252091     4\n",
       "252092     2\n",
       "252093     2\n",
       "252094     8\n",
       "252095     8\n",
       "252096     8\n",
       "252097     4\n",
       "252098     2\n",
       "252099     8\n",
       "252100     6\n",
       "252101     2\n",
       "252102     7\n",
       "252103     6\n",
       "252104     6\n",
       "252105     7\n",
       "252106     8\n",
       "252107     5\n",
       "Name: visitors, Length: 252108, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgb</th>\n",
       "      <th>knr</th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.816367</td>\n",
       "      <td>4.254880</td>\n",
       "      <td>1.571628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.307163</td>\n",
       "      <td>25.588578</td>\n",
       "      <td>25.765758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.747711</td>\n",
       "      <td>28.872445</td>\n",
       "      <td>27.871529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.515937</td>\n",
       "      <td>26.434347</td>\n",
       "      <td>28.822088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.984652</td>\n",
       "      <td>30.552176</td>\n",
       "      <td>30.984531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lgb        knr        xgb\n",
       "0   1.816367   4.254880   1.571628\n",
       "1  20.307163  25.588578  25.765758\n",
       "2  24.747711  28.872445  27.871529\n",
       "3  31.515937  26.434347  28.822088\n",
       "4  33.984652  30.552176  30.984531"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lv1_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do 3 fold CV here so that we save some running time on Kaggle Kernel\n",
    "kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[0]\ttrain-rmse:2.32335\tval-rmse:2.32361\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.519569\tval-rmse:0.520198\n",
      "[100]\ttrain-rmse:0.48802\tval-rmse:0.490888\n",
      "[150]\ttrain-rmse:0.486915\tval-rmse:0.490597\n",
      "[200]\ttrain-rmse:0.486059\tval-rmse:0.490457\n",
      "[250]\ttrain-rmse:0.485249\tval-rmse:0.490299\n",
      "[300]\ttrain-rmse:0.484465\tval-rmse:0.490161\n",
      "[350]\ttrain-rmse:0.483727\tval-rmse:0.49001\n",
      "[400]\ttrain-rmse:0.483071\tval-rmse:0.489878\n",
      "[450]\ttrain-rmse:0.482358\tval-rmse:0.489781\n",
      "[500]\ttrain-rmse:0.481709\tval-rmse:0.489724\n",
      "[550]\ttrain-rmse:0.481114\tval-rmse:0.489667\n",
      "[600]\ttrain-rmse:0.480549\tval-rmse:0.489623\n",
      "[650]\ttrain-rmse:0.48001\tval-rmse:0.48957\n",
      "[700]\ttrain-rmse:0.479458\tval-rmse:0.489535\n",
      "[750]\ttrain-rmse:0.478925\tval-rmse:0.489503\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"objective\" : \"reg:linear\",\n",
    "    #\"num_class\" : 3,\n",
    "    #\"tree_method\" : \"hist\",\n",
    "    \"eval_metric\" : \"rmse\",\n",
    "    \"nthread\": 4,\n",
    "    \"seed\" : 0,\n",
    "    'silent': 1,\n",
    "\n",
    "    \"eta\":0.05,  # default 0.3\n",
    "    \"max_depth\" : 5, # default 6\n",
    "    \"subsample\" : 0.8, # default 1\n",
    "    \"colsample_bytree\" : 0.6, # default 1\n",
    "    \"gamma\": 0.5\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "cv_score =cross_validate_xgb(xgb_params, lv1_train_df, y, lv1_test_df, kf, verbose=False, verbose_eval=50, scoreonly=True)\n",
    "\n",
    "print('cv score is {:.6f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimsation - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'max_depth':(4,10),\n",
    "        'learning_rate':(0.05,0.3),\n",
    "        'subsample': (0.4, 1),\n",
    "        'colsample_bytree': (0.4, 1),\n",
    "        'gamma': (0.001, 10.0),\n",
    "        'min_child_weight': (0, 20),\n",
    "        'max_delta_step': (0, 10),\n",
    "        'n_estimators': (10, 25),\n",
    "        'min_samples_split': (2, 20),\n",
    "        'max_features': (0.1, 0.999)\n",
    "       }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(xgb_wrapper)\n",
    "def xgbcv_func(max_depth, learning_rate, subsample, \n",
    "               colsample_bytree, gamma, min_child_weight, \n",
    "               max_delta_step, n_estimators, \n",
    "               min_samples_split, max_features,nthread=4, seed=0):\n",
    "    params = {\n",
    "        \"objective\" : \"reg:linear\",\n",
    "        #\"num_class\" : 3,\n",
    "        #\"tree_method\" : \"hist\",\n",
    "        \"eval_metric\" : \"rmse\",\n",
    "        \"nthread\": nthread,\n",
    "        \"seed\" : 0,\n",
    "        'silent': 1,\n",
    "\n",
    "        \"eta\":learning_rate,  # default 0.3\n",
    "        \"max_depth\" : int(max_depth), # default 6\n",
    "        \"subsample\" : subsample, # default 1\n",
    "        \"colsample_bytree\" : colsample_bytree, # default 1\n",
    "\n",
    "        'gamma': gamma,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'max_delta_step': max_delta_step,\n",
    "        'n_estimators': n_estimators,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'max_features': max_features    \n",
    "\n",
    "    }\n",
    "    \n",
    "    # for a more ideal out-of-fold model prediction for this dataset, we use 10-fold CV\n",
    "    kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "    \n",
    "    # we will disable all the verbose setting in this functional call, so that we don't have too much information \n",
    "    # to read during the bayesian optimisation process.\n",
    "    #return 1-cross_validate_xgb(params, train_input, y, test_input, kf, verbose=False, verbose_eval=False, scoreonly=True)\n",
    "    return 1-cross_validate_xgb(params, lv1_train_df, y, lv1_test_df, kf, verbose=False, verbose_eval=False, scoreonly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   learning_rate |   max_delta_step |   max_depth |   max_features |   min_child_weight |   min_samples_split |   n_estimators |   subsample | \n",
      "    1 | 01m10s | \u001b[35m   0.51338\u001b[0m | \u001b[32m            0.9922\u001b[0m | \u001b[32m   4.5565\u001b[0m | \u001b[32m         0.1218\u001b[0m | \u001b[32m          9.9855\u001b[0m | \u001b[32m     6.2687\u001b[0m | \u001b[32m        0.4359\u001b[0m | \u001b[32m            4.2578\u001b[0m | \u001b[32m            11.5447\u001b[0m | \u001b[32m       12.6966\u001b[0m | \u001b[32m     0.5481\u001b[0m | \n",
      "    2 | 03m50s |    0.51112 |             0.5088 |    8.0821 |          0.0906 |           2.6852 |      7.2526 |         0.9515 |            18.0489 |             17.2851 |        23.8557 |      0.8965 | \n",
      "    3 | 01m00s | \u001b[35m   0.51341\u001b[0m | \u001b[32m            0.7192\u001b[0m | \u001b[32m   1.8943\u001b[0m | \u001b[32m         0.0826\u001b[0m | \u001b[32m          0.5877\u001b[0m | \u001b[32m     4.5657\u001b[0m | \u001b[32m        0.4316\u001b[0m | \u001b[32m            1.5765\u001b[0m | \u001b[32m             3.0852\u001b[0m | \u001b[32m       14.4718\u001b[0m | \u001b[32m     0.4604\u001b[0m | \n",
      "    4 | 04m32s |    0.51137 |             0.5923 |    6.1474 |          0.0960 |           7.3975 |      5.5129 |         0.3567 |            12.0826 |             19.1119 |        23.2068 |      0.4268 | \n",
      "    5 | 03m23s |    0.51336 |             0.9122 |    6.2463 |          0.0749 |           1.2550 |      8.7483 |         0.1737 |             2.8584 |              5.8343 |        12.2900 |      0.7061 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   learning_rate |   max_delta_step |   max_depth |   max_features |   min_child_weight |   min_samples_split |   n_estimators |   subsample | \n",
      "    6 | 02m51s |    0.51088 |             0.6113 |    9.0162 |          0.1069 |           9.9869 |      4.3315 |         0.2045 |             1.2817 |              2.3632 |        10.3474 |      0.6002 | \n",
      "    7 | 01m44s |    0.51148 |             0.5702 |    0.1854 |          0.1815 |           0.1068 |      4.0641 |         0.2388 |            17.8259 |             18.4846 |        10.2336 |      0.5197 | \n",
      "    8 | 01m47s |    0.51190 |             0.7086 |    0.1478 |          0.2187 |           7.3092 |      9.9370 |         0.5099 |            18.2782 |              3.5423 |        24.6620 |      0.8586 | \n",
      "    9 | 02m21s |    0.50893 |             0.4069 |    1.4655 |          0.2724 |           9.0680 |      9.5872 |         0.9891 |             0.5622 |             19.6455 |        13.9374 |      0.7634 | \n",
      "   10 | 02m05s |    0.51284 |             0.9668 |    9.7325 |          0.2492 |           0.9355 |      4.8063 |         0.9348 |            16.5664 |              2.1677 |        21.3892 |      0.4021 | \n",
      "   11 | 01m58s |    0.51300 |             0.9600 |    1.1647 |          0.2413 |           0.1211 |      4.0077 |         0.6070 |             1.7916 |              8.3727 |        23.1342 |      0.9933 | \n",
      "   12 | 02m32s |    0.51256 |             0.9979 |    0.1947 |          0.0548 |           4.9184 |      9.5523 |         0.9708 |            13.2459 |              2.6339 |        10.0487 |      0.4854 | \n",
      "   13 | 01m55s |    0.51031 |             0.9807 |    0.2132 |          0.2317 |           0.4377 |      9.7768 |         0.3550 |            19.2344 |             18.4524 |        24.0282 |      0.4426 | \n",
      "   14 | 02m14s |    0.51315 |             0.9124 |    9.8509 |          0.1410 |           8.5399 |      6.3621 |         0.1752 |            19.8428 |             18.4509 |        14.3799 |      0.9230 | \n",
      "   15 | 02m01s |    0.51313 |             0.9821 |    9.7337 |          0.1177 |           1.1294 |      4.0865 |         0.6607 |             1.3812 |             18.5502 |        10.0938 |      0.4764 | \n",
      "   16 | 02m43s |    0.51334 |             0.9315 |    0.0025 |          0.0522 |           0.3132 |      4.2869 |         0.3125 |            17.3605 |              5.2652 |        10.6647 |      0.9884 | \n",
      "   17 | 01m19s |    0.51321 |             0.9766 |    1.2126 |          0.1859 |           9.7927 |      4.0038 |         0.6206 |            18.7309 |             14.4810 |        19.4100 |      0.8897 | \n",
      "   18 | 01m39s |    0.51277 |             0.9169 |    0.0464 |          0.2477 |           0.7548 |      6.1168 |         0.1129 |             2.0562 |             10.0653 |        10.3395 |      0.9764 | \n",
      "   19 | 01m58s |    0.51297 |             0.9973 |    0.9813 |          0.0676 |           9.8850 |      7.4021 |         0.3503 |             2.6593 |              2.2332 |        24.1542 |      0.4789 | \n",
      "   20 | 02m28s |    0.51323 |             0.9976 |    8.2673 |          0.0997 |           0.9633 |      4.5518 |         0.9955 |            10.6102 |              9.8157 |        12.8665 |      0.6471 | \n"
     ]
    }
   ],
   "source": [
    "xgb_bo=BayesianOptimization(xgbcv_func, params)\n",
    "xgb_bo.maximize(init_points=5, n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Maximum value: 0.513412\n",
      "Best parameters:  {'max_depth': 4.5657095721499896, 'learning_rate': 0.082629984850500091, 'subsample': 0.46044832678006442, 'colsample_bytree': 0.71917268781058374, 'gamma': 1.8943463790062087, 'min_child_weight': 1.5765226386520559, 'max_delta_step': 0.58769473261240268, 'n_estimators': 14.471774675441567, 'min_samples_split': 3.0851570445698262, 'max_features': 0.43156864998201261}\n"
     ]
    }
   ],
   "source": [
    "print('-'*30)\n",
    "print('Maximum value: %f' % xgb_bo.res['max']['max_val'])\n",
    "print('Best parameters: ', xgb_bo.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2 XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do 3 fold CV here so that we save some running time on Kaggle Kernel\n",
    "kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting xgboost...\n",
      "[0]\ttrain-rmse:2.39327\tval-rmse:2.39978\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.607467\tval-rmse:0.617005\n",
      "[100]\ttrain-rmse:0.483058\tval-rmse:0.491415\n",
      "[150]\ttrain-rmse:0.482406\tval-rmse:0.491384\n",
      "[200]\ttrain-rmse:0.481786\tval-rmse:0.491528\n",
      "Stopping. Best iteration:\n",
      "[139]\ttrain-rmse:0.482559\tval-rmse:0.491327\n",
      "\n",
      "fold cv 0 rmsle score is 0.491327\n",
      "[0]\ttrain-rmse:2.39439\tval-rmse:2.39529\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.608942\tval-rmse:0.610099\n",
      "[100]\ttrain-rmse:0.483954\tval-rmse:0.487744\n",
      "[150]\ttrain-rmse:0.483334\tval-rmse:0.487728\n",
      "[200]\ttrain-rmse:0.482719\tval-rmse:0.487854\n",
      "Stopping. Best iteration:\n",
      "[127]\ttrain-rmse:0.483615\tval-rmse:0.48768\n",
      "\n",
      "fold cv 1 rmsle score is 0.487680\n",
      "[0]\ttrain-rmse:2.39493\tval-rmse:2.39316\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.609666\tval-rmse:0.608237\n",
      "[100]\ttrain-rmse:0.484911\tval-rmse:0.484288\n",
      "[150]\ttrain-rmse:0.484336\tval-rmse:0.484299\n",
      "[200]\ttrain-rmse:0.483776\tval-rmse:0.484368\n",
      "Stopping. Best iteration:\n",
      "[123]\ttrain-rmse:0.48464\tval-rmse:0.484238\n",
      "\n",
      "fold cv 2 rmsle score is 0.484238\n",
      "[0]\ttrain-rmse:2.39514\tval-rmse:2.39229\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.609855\tval-rmse:0.608336\n",
      "[100]\ttrain-rmse:0.48441\tval-rmse:0.486292\n",
      "[150]\ttrain-rmse:0.483832\tval-rmse:0.486386\n",
      "[200]\ttrain-rmse:0.483265\tval-rmse:0.48647\n",
      "Stopping. Best iteration:\n",
      "[100]\ttrain-rmse:0.48441\tval-rmse:0.486292\n",
      "\n",
      "fold cv 3 rmsle score is 0.486292\n",
      "[0]\ttrain-rmse:2.39513\tval-rmse:2.39231\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[50]\ttrain-rmse:0.609909\tval-rmse:0.607809\n",
      "[100]\ttrain-rmse:0.485009\tval-rmse:0.483683\n",
      "[150]\ttrain-rmse:0.484368\tval-rmse:0.483712\n",
      "[200]\ttrain-rmse:0.483707\tval-rmse:0.483869\n",
      "Stopping. Best iteration:\n",
      "[108]\ttrain-rmse:0.484868\tval-rmse:0.483651\n",
      "\n",
      "fold cv 4 rmsle score is 0.483651\n",
      "cv rmsle score is 0.486649\n",
      "it takes 69.289 seconds to perform cross validation\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    'max_depth': 5,#4.5657095721499896, \n",
    "    'learning_rate': 0.082629984850500091, \n",
    "    'subsample': 0.46044832678006442, \n",
    "    'colsample_bytree': 0.71917268781058374, \n",
    "    'gamma': 1.8943463790062087, \n",
    "    'min_child_weight': 1.5765226386520559, \n",
    "    'max_delta_step': 0.58769473261240268, \n",
    "    'n_estimators': 14.471774675441567, \n",
    "    'min_samples_split': 3.0851570445698262, \n",
    "    'max_features': 0.43156864998201261\n",
    "}\n",
    "print(\"Starting xgboost...\")\n",
    "\n",
    "xgb_lv2_outcomes=cross_validate_xgb(xgb_params, lv1_train_df, y, lv1_test_df, kf)\n",
    "\n",
    "xgb_lv2_cv=xgb_lv2_outcomes[0]\n",
    "xgb_lv2_train_pred=xgb_lv2_outcomes[1]\n",
    "xgb_lv2_test_pred=xgb_lv2_outcomes[2]\n",
    "\n",
    "xgb_lv2_train_pred_df=pd.DataFrame(columns=['visitors'], data=xgb_lv2_train_pred)\n",
    "xgb_lv2_test_pred_df=pd.DataFrame(columns=['visitors'], data=xgb_lv2_test_pred)\n",
    "\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv2_xgb_train_pred = xgb_lv2_train_pred_df.copy()\n",
    "lv2_xgb_train_pred.to_csv('lv2_xgb_train_pred.csv', index=False)\n",
    "\n",
    "lv2_xgb_test_pred = xgb_lv2_test_pred_df.copy()\n",
    "lv2_xgb_test_pred.to_csv('lv2_xgb_test_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['visitors'] = xgb_lv2_test_pred_df.values\n",
    "sub = test[['id','visitors']].copy()\n",
    "sub1 = sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../../../mltestdata/05_recruit/*.csv')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n",
    "    on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_merge['visitors'] = (sub_merge['visitors_x'] + sub_merge['visitors_y']* 1.1)/2\n",
    "sub_merge[['id', 'visitors']].to_csv('submission_rs_recruit_v17_stacking_01.csv', index=False)\n",
    "\n",
    "# gw_flag\n",
    "# xgb\n",
    "# Bopt\n",
    "# weight\n",
    "# Stacking (lv1:knr,lgb,xgb, lv2:xgb)\n",
    "# LB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
