{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import time\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contributions from:\n",
    "DSEverything - Mean Mix - Math, Geo, Harmonic (LB 0.493) \n",
    "https://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493\n",
    "JdPaletto - Surprised Yet? - Part2 - (LB: 0.503)\n",
    "https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503\n",
    "hklee - weighted mean comparisons, LB 0.497, 1ST\n",
    "https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st\n",
    "\n",
    "Also all comments for changes, encouragement, and forked scripts rock\n",
    "\n",
    "Keep the Surprise Going\n",
    "\"\"\"\n",
    "\n",
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../../mltestdata/05_recruit/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "    }\n",
    "\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "#sure it can be compressed...\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n",
    "\n",
    "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n",
    "\n",
    "# NEW FEATURES FROM JMBULL\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "\n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for LightGBM ...\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nProcessing data for LightGBM ...\" )\n",
    "for c, dtype in zip(train.columns, train.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        train[c] = train[c].astype(np.float32)\n",
    "\n",
    "for c, dtype in zip(test.columns, test.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        test[c] = test[c].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "y_train=train['visitors']\n",
    "x_train=train.drop(drop_cols, axis=1)\n",
    "\n",
    "x_test=test.copy()\n",
    "x_test=x_test.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.visitors\n",
    "train_input = train.copy()\n",
    "test_input = test.copy()\n",
    "\n",
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "train_input=train_input.drop(drop_cols, axis=1)\n",
    "test_input=test_input.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "localtrain, localval = train_test_split(train,test_size=0.3,random_state=2018)\n",
    "\n",
    "y_localtrain=localtrain['visitors']\n",
    "x_localtrain=localtrain.drop(drop_cols, axis=1)\n",
    "\n",
    "y_localval=localval['visitors']\n",
    "x_localval=localval.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation function\n",
    "\n",
    "def rmsle(preds, true):\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(true), np.log1p(preds)))\n",
    "    return rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation matrix \n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "RMSLE = make_scorer(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for comparing predictions and true data.\n",
    "def compare_result(preds, true):\n",
    "    compare = pd.DataFrame({\"test_id\": true.index,\n",
    "                           \"real_cost\": true,\n",
    "                           \"pred_cost\": preds})\n",
    "    compare = compare[[\"test_id\", \"real_cost\", \"pred_cost\"]].reset_index(drop=True)\n",
    "    \n",
    "    compare[\"error_percent_(%)\"] = np.abs(compare.real_cost - compare.pred_cost) / compare.real_cost * 100\n",
    "    \n",
    "    return compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_lgb(params, x_train, y_train, kf, verbose=True, verbose_eval=50,df_input=True):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "\n",
    "    # self-defined eval metric\n",
    "    # f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "    # binary error\n",
    "    def feval_rmsle(preds, train_data):\n",
    "        #print(\"preds    \"+str(preds[0:5]))\n",
    "        #print(\"train_data   \"+str(train_data.get_label()[0:5]))\n",
    "        #preds = np.expm1(preds)\n",
    "        #true = np.expm1(train_data.get_label())\n",
    "        true = train_data.get_label()\n",
    "        return 'my rmsle', rmsle(preds,true), False\n",
    "\n",
    "    # use the k-fold object to enumerate indexes for each training and validation fold\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)): # folds 1, 2 ,3 ,4, 5\n",
    "        # example: training from 1,2,3,4; validation from 5\n",
    "\n",
    "        if df_input:\n",
    "            x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "        else:\n",
    "            x_train_kf, x_val_kf = x_train[train_index], x_train[val_index]\n",
    "            \n",
    "        y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n",
    "        #y_train_kf, y_val_kf = np.log1p(y_train[train_index]), np.log1p(y_train[val_index])\n",
    "\n",
    "        lgb_train = lgb.Dataset(x_train_kf, y_train_kf)\n",
    "        lgb_val = lgb.Dataset(x_val_kf, y_val_kf, reference=lgb_train)\n",
    "        \n",
    "        watchlist= [lgb_train, lgb_val]\n",
    "\n",
    "        gbm = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=8000,\n",
    "                        #valid_sets=lgb_val,\n",
    "                        valid_sets=watchlist, \n",
    "                        early_stopping_rounds=100,\n",
    "                        verbose_eval=verbose_eval,\n",
    "                        feval=feval_rmsle\n",
    "                       )\n",
    "\n",
    "        ###print(\"gbm.predict  \"+str(gbm.predict(x_val_kf)))\n",
    "        ###print(\"gbm.predict  \"+str(np.expm1(gbm.predict(x_val_kf))))\n",
    "\n",
    "        #val_pred = np.expm1(gbm.predict(x_val_kf, num_iteration=gbm.best_iteration))\n",
    "        #val_pred = np.expm1(gbm.predict(x_val_kf))\n",
    "        val_pred = gbm.predict(x_val_kf, num_iteration=gbm.best_iteration)\n",
    "\n",
    "        train_pred[val_index] += val_pred\n",
    "        #print(\"\\nBefore fold_rmsle\")\n",
    "        #print(\"gbm.predict  \"+str(gbm.predict(x_val_kf, num_iteration=gbm.best_iteration)))\n",
    "        #print(\"train_pred[val_index]  \"+str(train_pred[val_index][0:5]))\n",
    "\n",
    "        ###print(\"np.expm1(y_val_kf.values)  \"+str(np.expm1(y_val_kf.values)[0:5]))\n",
    "        ###print(\"y_val_kf.values  \"+str(y_val_kf.values[0:5]))\n",
    " \n",
    "        #fold_rmsle = eval_rmsle(val_pred, y_val_kf.values)\n",
    "        #fold_rmsle = rmsle(val_pred, np.expm1(y_val_kf.values))\n",
    "        #fold_rmsle = eval_rmsle(train_pred[val_index], np.expm1(y_val_kf.values))\n",
    "        fold_rmsle = rmsle(val_pred, y_val_kf.values)\n",
    "        #Yifan: fold_rmsle = rmsle(np.expm1(y_val_kf.values), train_pred[val_index])\n",
    "\n",
    "        if verbose:\n",
    "            print('fold cv {} RMSLE score is {:.6f}\\n'.format(i, fold_rmsle))\n",
    " \n",
    "    #cv_rmsle = eval_rmsle(train_pred, np.log1p(y_train))\n",
    "    #cv_rmsle = rmsle(train_pred, y_train)\n",
    "    cv_rmsle = rmsle(train_pred, y_train)\n",
    "\n",
    "    if verbose:\n",
    "        print('cv RMSLE score is {:.6f}'.format(cv_rmsle))\n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    "\n",
    "    return cv_rmsle\n",
    "#    return cv_rmsle, train_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\ttraining's my rmsle: 0.519235\tvalid_1's my rmsle: 0.525025\n",
      "[100]\ttraining's my rmsle: 0.513931\tvalid_1's my rmsle: 0.519803\n",
      "[150]\ttraining's my rmsle: 0.513553\tvalid_1's my rmsle: 0.519514\n",
      "[200]\ttraining's my rmsle: 0.517118\tvalid_1's my rmsle: 0.523332\n",
      "Early stopping, best iteration is:\n",
      "[136]\ttraining's my rmsle: 0.510945\tvalid_1's my rmsle: 0.516755\n",
      "fold cv 0 RMSLE score is 0.599870\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\ttraining's my rmsle: 0.52042\tvalid_1's my rmsle: 0.522396\n",
      "[100]\ttraining's my rmsle: 0.514981\tvalid_1's my rmsle: 0.516974\n",
      "[150]\ttraining's my rmsle: 0.514943\tvalid_1's my rmsle: 0.517211\n",
      "[200]\ttraining's my rmsle: 0.51871\tvalid_1's my rmsle: 0.521434\n",
      "Early stopping, best iteration is:\n",
      "[136]\ttraining's my rmsle: 0.512434\tvalid_1's my rmsle: 0.514343\n",
      "fold cv 1 RMSLE score is 0.596215\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\ttraining's my rmsle: 0.52124\tvalid_1's my rmsle: 0.520336\n",
      "[100]\ttraining's my rmsle: 0.515764\tvalid_1's my rmsle: 0.515129\n",
      "[150]\ttraining's my rmsle: 0.515522\tvalid_1's my rmsle: 0.515097\n",
      "[200]\ttraining's my rmsle: 0.519055\tvalid_1's my rmsle: 0.518901\n",
      "[250]\ttraining's my rmsle: 0.518529\tvalid_1's my rmsle: 0.518764\n",
      "Early stopping, best iteration is:\n",
      "[153]\ttraining's my rmsle: 0.512911\tvalid_1's my rmsle: 0.512749\n",
      "fold cv 2 RMSLE score is 0.571349\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\ttraining's my rmsle: 0.520943\tvalid_1's my rmsle: 0.521994\n",
      "[100]\ttraining's my rmsle: 0.51534\tvalid_1's my rmsle: 0.516663\n",
      "[150]\ttraining's my rmsle: 0.515025\tvalid_1's my rmsle: 0.516476\n",
      "[200]\ttraining's my rmsle: 0.518729\tvalid_1's my rmsle: 0.520478\n",
      "[250]\ttraining's my rmsle: 0.518191\tvalid_1's my rmsle: 0.520265\n",
      "Early stopping, best iteration is:\n",
      "[153]\ttraining's my rmsle: 0.512357\tvalid_1's my rmsle: 0.513945\n",
      "fold cv 3 RMSLE score is 0.571622\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\ttraining's my rmsle: 0.521264\tvalid_1's my rmsle: 0.519301\n",
      "[100]\ttraining's my rmsle: 0.515888\tvalid_1's my rmsle: 0.51414\n",
      "[150]\ttraining's my rmsle: 0.515874\tvalid_1's my rmsle: 0.514108\n",
      "[200]\ttraining's my rmsle: 0.519322\tvalid_1's my rmsle: 0.517651\n",
      "Early stopping, best iteration is:\n",
      "[136]\ttraining's my rmsle: 0.513122\tvalid_1's my rmsle: 0.511647\n",
      "fold cv 4 RMSLE score is 0.596341\n",
      "\n",
      "cv RMSLE score is 0.587228\n",
      "it takes 101.042 seconds to perform cross validation\n",
      "cv score is 0.587228\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "    'boosting_type': 'dart',\n",
    "    'max_depth' : 5,\n",
    "    'max_bin' : 500,\n",
    "    'learning_rate': 0.1,  # 0.618580\n",
    "    'num_leaves': 22,\n",
    "#    'metric': 'RMSE'\n",
    "}\n",
    "\n",
    "# only do 5 fold CV here so that we save some running time on Kaggle Kernel\n",
    "kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "cv_score =cross_validate_lgb(lgb_params, train_input, y, kf, verbose=True, verbose_eval=50,df_input=True)\n",
    "\n",
    "print('cv score is {:.6f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Start training...\n",
    "preds    [ 21.15093175  20.87278066  22.22167965  21.48732884  20.02349571]\n",
    "train_data   0    25\n",
    "1    32\n",
    "2    29\n",
    "3    22\n",
    "4     6\n",
    "Name: visitors, dtype: int64\n",
    "preds    [ 22.22167965  20.47679023  20.87278066  22.22167965  21.48732884]\n",
    "train_data   8     18\n",
    "11    11\n",
    "17    12\n",
    "18    45\n",
    "19    15\n",
    "Name: visitors, dtype: int64\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "preds    [ 21.24943     20.97127891  23.37412446  21.9696604   19.39810651]\n",
    "train_data   0    25\n",
    "1    32\n",
    "2    29\n",
    "3    22\n",
    "4     6\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Start training...\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmse: 0.582389\n",
    "[100]\tvalid_0's rmse: 0.588296\n",
    "Early stopping, best iteration is:\n",
    "[45]\tvalid_0's rmse: 0.515019\n",
    "gbm.predict  [ 1.42741231  1.16785115  1.19904827 ...,  0.59878107  0.69817213\n",
    "  0.66574006]\n",
    "gbm.predict  [ 3.16789999  2.21507649  2.31695857 ...,  0.81989912  1.01007519\n",
    "  0.9459301 ]\n",
    "np.expm1(y_val_kf.values)  [ 18.  11.  12.  45.  15.]\n",
    "y_val_kf.values  [ 2.94443898  2.48490665  2.56494936  3.8286414   2.77258872]\n",
    "fold cv 0 RMSLE score is 1.784524\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmse: 0.578917\n",
    "[100]\tvalid_0's rmse: 0.584797\n",
    "Early stopping, best iteration is:\n",
    "[45]\tvalid_0's rmse: 0.51222\n",
    "gbm.predict  [ 1.0005289   1.1695398   1.18349952 ...,  0.67099202  0.68324909\n",
    "  0.68324909]\n",
    "gbm.predict  [ 1.71971992  2.22051023  2.26578291 ...,  0.95617693  0.98030146\n",
    "  0.98030146]\n",
    "np.expm1(y_val_kf.values)  [  6.   9.  21.  26.   6.]\n",
    "y_val_kf.values  [ 1.94591015  2.30258509  3.09104245  3.29583687  1.94591015]\n",
    "fold cv 1 RMSLE score is 1.780887\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmse: 0.578507\n",
    "[100]\tvalid_0's rmse: 0.584762\n",
    "Early stopping, best iteration is:\n",
    "[45]\tvalid_0's rmse: 0.509925\n",
    "gbm.predict  [ 1.29143688  1.17206736  1.42941167 ...,  0.67207122  0.66017064\n",
    "  0.67207122]\n",
    "gbm.predict  [ 2.6380102   2.22866054  3.17624146 ...,  0.95828917  0.93512252\n",
    "  0.95828917]\n",
    "np.expm1(y_val_kf.values)  [ 25.  32.  29.  31.  26.]\n",
    "y_val_kf.values  [ 3.25809654  3.49650756  3.40119738  3.4657359   3.29583687]\n",
    "fold cv 2 RMSLE score is 1.781198\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmse: 0.578578\n",
    "[100]\tvalid_0's rmse: 0.584745\n",
    "Early stopping, best iteration is:\n",
    "[45]\tvalid_0's rmse: 0.511196\n",
    "gbm.predict  [ 1.31281624  1.28247999  1.16827171 ...,  0.69396583  0.65322372\n",
    "  0.68092431]\n",
    "gbm.predict  [ 2.7166259   2.60557041  2.21642891 ...,  1.00163798  0.92172597\n",
    "  0.97570305]\n",
    "np.expm1(y_val_kf.values)  [ 22.  24.  21.  15.  23.]\n",
    "y_val_kf.values  [ 3.13549422  3.21887582  3.09104245  2.77258872  3.17805383]\n",
    "fold cv 3 RMSLE score is 1.779868\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmse: 0.576969\n",
    "[100]\tvalid_0's rmse: 0.583153\n",
    "Early stopping, best iteration is:\n",
    "[45]\tvalid_0's rmse: 0.508495\n",
    "gbm.predict  [ 1.42674843  1.43787536  1.00130438 ...,  0.66073807  0.67613772\n",
    "  0.66073807]\n",
    "gbm.predict  [ 3.16513392  3.21173788  1.72182982 ...,  0.93622088  0.96626876\n",
    "  0.93622088]\n",
    "np.expm1(y_val_kf.values)  [ 32.  51.  29.  30.  37.]\n",
    "y_val_kf.values  [ 3.49650756  3.95124372  3.40119738  3.4339872   3.63758616]\n",
    "fold cv 4 RMSLE score is 1.780247\n",
    "\n",
    "cv RMSLE score is 1.781348\n",
    "it takes 38.238 seconds to perform cross validation\n",
    "cv score is 1.781348\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Start training...\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmsle: 0.525025\n",
    "[100]\tvalid_0's rmsle: 0.519803\n",
    "[150]\tvalid_0's rmsle: 0.519514\n",
    "[200]\tvalid_0's rmsle: 0.523332\n",
    "Early stopping, best iteration is:\n",
    "[136]\tvalid_0's rmsle: 0.516755\n",
    "fold cv 0 RMSLE score is 0.599870\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmsle: 0.522396\n",
    "[100]\tvalid_0's rmsle: 0.516974\n",
    "[150]\tvalid_0's rmsle: 0.517211\n",
    "[200]\tvalid_0's rmsle: 0.521434\n",
    "Early stopping, best iteration is:\n",
    "[136]\tvalid_0's rmsle: 0.514343\n",
    "fold cv 1 RMSLE score is 0.596215\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmsle: 0.520336\n",
    "[100]\tvalid_0's rmsle: 0.515129\n",
    "[150]\tvalid_0's rmsle: 0.515097\n",
    "[200]\tvalid_0's rmsle: 0.518901\n",
    "[250]\tvalid_0's rmsle: 0.518764\n",
    "Early stopping, best iteration is:\n",
    "[153]\tvalid_0's rmsle: 0.512749\n",
    "fold cv 2 RMSLE score is 0.571349\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmsle: 0.521994\n",
    "[100]\tvalid_0's rmsle: 0.516663\n",
    "[150]\tvalid_0's rmsle: 0.516476\n",
    "[200]\tvalid_0's rmsle: 0.520478\n",
    "[250]\tvalid_0's rmsle: 0.520265\n",
    "Early stopping, best iteration is:\n",
    "[153]\tvalid_0's rmsle: 0.513945\n",
    "fold cv 3 RMSLE score is 0.571622\n",
    "\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[50]\tvalid_0's rmsle: 0.519301\n",
    "[100]\tvalid_0's rmsle: 0.51414\n",
    "[150]\tvalid_0's rmsle: 0.514108\n",
    "[200]\tvalid_0's rmsle: 0.517651\n",
    "Early stopping, best iteration is:\n",
    "[138]\tvalid_0's rmsle: 0.511618\n",
    "fold cv 4 RMSLE score is 0.591054\n",
    "\n",
    "cv RMSLE score is 0.586159\n",
    "it takes 74.017 seconds to perform cross validation\n",
    "cv score is 0.586159\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimsation - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "#            dict(name=\"max_bin\", type=\"int\", bounds=dict(min=20, max=20000)),\n",
    "            dict(name=\"num_leaves\", type=\"int\", bounds=dict(min=100, max=4095)),\n",
    "            dict(name=\"max_depth\", type=\"int\", bounds=dict(min=2, max=50)),\n",
    "            dict(name=\"learning_rate\", type=\"double\", bounds=dict(min=0.001, max=0.3)),\n",
    "            dict(name=\"min_sum_hessian_in_leaf\", type=\"int\", bounds=dict(min=2, max=30)),\n",
    "             # dict(name=\"num_leaves\", type=\"int\", bounds=dict(min=100, max=45000)),\n",
    "#            dict(name=\"scale_pos_weight\", type=\"double\", bounds=dict(min=0.01, max=2000.0)),\n",
    "#            dict(name=\"n_estimators\", type=\"int\", bounds=dict(min=10, max=10000)),\n",
    "#            dict(name=\"min_child_weight\", type=\"int\", bounds=dict(min=1, max=2000)),\n",
    "            dict(name=\"subsample\", type=\"double\", bounds=dict(min=0.4, max=1)),\n",
    "            dict(name=\"colsample_bytree\", type=\"double\", bounds=dict(min=0.4, max=1)),\n",
    "#            dict(name=\"bagging_fraction\", type=\"double\", bounds=dict(min=0.3, max=1)),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'num_leaves':(7,4095),\n",
    "    'max_depth':(2,63),\n",
    "    'learning_rate':(0.05,0.3),\n",
    "    #'scale_pos_weight':(1,10000),\n",
    "    'min_sum_hessian_in_leaf':(2,30),\n",
    "    'subsample':(0.4,1.0),\n",
    "    'colsample_bytree':(0.4,1.0),\n",
    "    #'feature_fraction':(0.0,1.0),\n",
    "    #'bagging_fraction':(0.0,1.0),\n",
    "    #'bagging_freq':(0,2),\n",
    "    #'lambda_l1':(0.0,1.0),\n",
    "    #'lambda_l2':(0.0,1.0),\n",
    "    #'n_estimators':(2,30), \n",
    "    #'reg_lambda':(0.0,2.0),\n",
    "    #'min_gain_to_split':(0.0,1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(lgb_wrapper)\n",
    "#def lgbcv_func(max_depth, learning_rate, subsample, colsample_bytree, nthread=4, seed=0):\n",
    "def lgbcv_func(num_leaves, max_depth, learning_rate,\n",
    "               #scale_pos_weight, \n",
    "               min_sum_hessian_in_leaf, \n",
    "               subsample, \n",
    "               colsample_bytree,\n",
    "               #feature_fraction, bagging_fraction, \n",
    "               #bagging_freq, lambda_l1, lambda_l2,\n",
    "               #n_estimators,reg_lambda,min_gain_to_split,\n",
    "               nthread=4):\n",
    "\n",
    "    params = {\n",
    "        'objective' : \"regression\",\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'dart',\n",
    "                \n",
    "        'num_leaves': num_leaves,\n",
    "        'max_depth': max_depth, \n",
    "        'learning_rate': learning_rate,\n",
    "        #'scale_pos_weight':scale_pos_weight,\n",
    "        'min_sum_hessian_in_leaf':min_sum_hessian_in_leaf, \n",
    "        'subsample':subsample,\n",
    "        'colsample_bytree':colsample_bytree,\n",
    "        #'feature_fraction':feature_fraction, \n",
    "        #'bagging_fraction':bagging_fraction,\n",
    "        #'bagging_freq':bagging_freq, \n",
    "        #'lambda_l1':lambda_l1, \n",
    "        #'lambda_l2':lambda_l2,\n",
    "        #'n_estimators':n_estimators,\n",
    "        #'reg_lambda':reg_lambda,\n",
    "        #'min_gain_to_split':min_gain_to_split       \n",
    "        #'metric': 'RMSE'\n",
    "    }\n",
    "    \n",
    "    # for a more ideal out-of-fold model prediction for this dataset, we use 10-fold CV\n",
    "    kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)\n",
    "    \n",
    "    # we will disable all the verbose setting in this functional call, so that we don't have too much information \n",
    "    # to read during the bayesian optimisation process.\n",
    "    return 1-cross_validate_lgb(params, train_input, y, kf, verbose=False, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_bo=BayesianOptimization(lgbcv_func, params)\n",
    "\n",
    "#lgb_bo=BayesianOptimization(lgbcv_func, **parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_sum_hessian_in_leaf |   num_leaves |   subsample | \n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "b'Parameter num_leaves should be of type int, got \"3497.49080107\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-067a6f427821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlgb_bo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, init_points)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Evaluate target function at all initialization points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observe_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# Add the points from `self.initialize` to the observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36m_observe_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_observe_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mobserve_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# measure the target function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-2bd2fbdb31d1>\u001b[0m in \u001b[0;36mlgbcv_func\u001b[0;34m(num_leaves, max_depth, learning_rate, min_sum_hessian_in_leaf, subsample, colsample_bytree, nthread)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# we will disable all the verbose setting in this functional call, so that we don't have too much information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# to read during the bayesian optimisation process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcross_validate_lgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-7bd05017e88e>\u001b[0m in \u001b[0;36mcross_validate_lgb\u001b[0;34m(params, x_train, y_train, kf, verbose, verbose_eval, df_input)\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                         \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval_rmsle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                        )\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;31m# construct booster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, train_set, model_file, silent)\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[0;32m-> 1294\u001b[0;31m                 \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                 ctypes.byref(self.handle)))\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    843\u001b[0m                                 \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                                 \u001b[0mpredictor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[1;32m    846\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m(self, data, label, max_bin, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_from_np2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init_from_np2d\u001b[0;34m(self, mat, params_str, ref_dataset)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mref_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             ctypes.byref(self.handle)))\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init_from_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \"\"\"\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLightGBMError\u001b[0m: b'Parameter num_leaves should be of type int, got \"3497.49080107\"'"
     ]
    }
   ],
   "source": [
    "lgb_bo.maximize(init_points=5, n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.17995806067430897,\n",
       " 'bagging_freq': 1.3430342823014347,\n",
       " 'feature_fraction': 0.51232300441712064,\n",
       " 'lambda_l1': 0.54056564353676506,\n",
       " 'max_depth': 20.348482273206898,\n",
       " 'min_sum_hessian_in_leaf': 29.260145258218444,\n",
       " 'num_leaves': 15.619747584846419}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_bo.res['max']['max_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "{'bagging_fraction': 0.17995806067430897,\n",
    " 'bagging_freq': 1.3430342823014347,\n",
    " 'feature_fraction': 0.51232300441712064,\n",
    " 'lambda_l1': 0.54056564353676506,\n",
    " 'max_depth': 20.348482273206898,\n",
    " 'min_sum_hessian_in_leaf': 29.260145258218444,\n",
    " 'num_leaves': 15.619747584846419}\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Final Results\n",
      "Maximum value: 0.422800\n",
      "Best parameters:  {'num_leaves': 15.619747584846419, 'min_sum_hessian_in_leaf': 29.260145258218444, 'max_depth': 20.348482273206898, 'feature_fraction': 0.51232300441712064, 'bagging_fraction': 0.17995806067430897, 'bagging_freq': 1.3430342823014347, 'lambda_l1': 0.54056564353676506}\n"
     ]
    }
   ],
   "source": [
    "print('-'*30)\n",
    "print('Final Results')\n",
    "print('Maximum value: %f' % lgb_bo.res['max']['max_val'])\n",
    "print('Best parameters: ', lgb_bo.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "------------------------------\n",
    "Final Results\n",
    "Maximum value: 0.422800\n",
    "Best parameters:  {'num_leaves': 15.619747584846419, 'min_sum_hessian_in_leaf': 29.260145258218444, 'max_depth': 20.348482273206898, 'feature_fraction': 0.51232300441712064, 'bagging_fraction': 0.17995806067430897, 'bagging_freq': 1.3430342823014347, 'lambda_l1': 0.54056564353676506}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective' : \"regression\",\n",
    "    'learning_rate': 0.1,       \n",
    "    'task': 'train',\n",
    "    'boosting_type': 'dart',\n",
    "    'num_leaves': 16,#15.619747584846419, \n",
    "    'min_sum_hessian_in_leaf': 29.260145258218444, \n",
    "    'max_depth': 20,#20.348482273206898,\n",
    "    'feature_fraction': 0.51232300441712064, \n",
    "    'bagging_fraction': 0.17995806067430897, \n",
    "    'bagging_freq': 1, #1.3430342823014347, \n",
    "    'lambda_l1': 0.54056564353676506\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\ttraining's rmsle: 0.580172\tvalid_1's rmsle: 0.579043\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's rmsle: 0.511956\tvalid_1's rmsle: 0.512371\n",
      "fold cv 0 RMSLE score is 0.577125\n",
      "\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\ttraining's rmsle: 0.578437\tvalid_1's rmsle: 0.584288\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's rmsle: 0.510283\tvalid_1's rmsle: 0.516068\n",
      "fold cv 1 RMSLE score is 0.575701\n",
      "\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\ttraining's rmsle: 0.580905\tvalid_1's rmsle: 0.578431\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's rmsle: 0.513352\tvalid_1's rmsle: 0.509707\n",
      "fold cv 2 RMSLE score is 0.573614\n",
      "\n",
      "cv RMSLE score is 0.575483\n",
      "it takes 13.225 seconds to perform cross validation\n",
      "cv score is 0.575483\n"
     ]
    }
   ],
   "source": [
    "# only do 3 fold CV here so that we save some running time on Kaggle Kernel\n",
    "kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "lgb_cv_score, lgb_train_pred =cross_validate_lgb(lgb_params, train_input, y, kf, verbose=True, verbose_eval=50,df_input=True)\n",
    "\n",
    "print('cv score is {:.6f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.69600288,  5.47287828,  8.57551433, ...,  1.95046062,\n",
       "        2.10087429,  2.00777837])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = lgb.train(lgb_params,\n",
    "                ,\n",
    "                num_boost_round=4000,\n",
    "                valid_sets=lgb_val,\n",
    "                early_stopping_rounds=30,\n",
    "                verbose_eval=verbose_eval)\n",
    "\n",
    "\n",
    "lgb_model = lgb.train(lgb_params, train_set=d_train, num_boost_round=rounds, \n",
    "                          valid_sets=watchlist, verbose_eval=1000, early_stopping_rounds = 300)\n",
    "test_pred = lgb_model.predict(X_v)\n",
    "\n",
    "params, train_set=d_train, num_boost_round=7500, valid_sets=watchlist, \\\n",
    "    early_stopping_rounds=500, verbose_eval=500\n",
    "\n",
    "    \n",
    "xgr_g = xgb.XGBRegressor(**grid_xgb.best_params_)\n",
    "xgr_g.fit(X_train, y_train)\n",
    "y_pred_gs = xgr_g.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
