{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from keras.layers import Embedding, Input, Dense\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suzukiry/lab/March/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/suzukiry/lab/March/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/Users/suzukiry/lab/March/lib/python3.6/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "### Thanks to Shinji Suzuki\n",
    "\n",
    "# OS\n",
    "import glob, re\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# data science tool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "# machine learning\n",
    "from sklearn import *\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# データの読み込み\n",
    "# 事前にcalendar_dateをvisit_dataに変更しています。airとhpgで同じことですが、別名で使用されているようです。\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../../mltestdata/05_recruit/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "}\n",
    "\n",
    "# それぞれのデータをマージするために、まずは、relation用のものをマージします\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how = 'inner', on = ['hpg_store_id'])\n",
    "\n",
    "for df in ['ar', 'hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r:(r['visit_datetime']- r['reserve_datetime']).days, axis = 1)\n",
    "    tmp1 = data[df].groupby(['air_store_id', 'visit_datetime'], as_index =False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns = {'visit_datetime':'visit_date', 'reserve_datetime_diff':'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id', 'visit_datetime'], as_index =False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns = {'visit_datetime':'visit_date', 'reserve_datetime_diff':'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how = 'inner', on = ['air_store_id', 'visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id':unique_stores, 'dow':[i]*len(unique_stores)}) for i in range(7)], axis =0, ignore_index = True).reset_index(drop = True) \n",
    "\n",
    "#曜日だけでなく、月も追加\n",
    "stores_m = pd.concat([pd.DataFrame({'air_store_id':unique_stores, 'month':[i]*len(unique_stores)}) for i in range(1,13)], axis =0, ignore_index = True).reset_index(drop = True)\n",
    "stores = pd.merge(stores_m, stores,on=('air_store_id'), how='left')\n",
    "\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].min().rename(columns = {'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].median().rename(columns = {'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].max().rename(columns = {'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index = False)['visitors'].count().rename(columns = {'visitors':'count_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'dow'])\n",
    "\n",
    "#曜日だけでなく、ID×月も追加\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].min().rename(columns = {'visitors':'m_min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'m_mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].median().rename(columns = {'visitors':'m_median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].max().rename(columns = {'visitors':'m_max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "tmp = data['tra'].groupby(['air_store_id', 'month'], as_index = False)['visitors'].count().rename(columns = {'visitors':'m_count_visitors'})\n",
    "stores = pd.merge(stores, tmp, how ='left', on = ['air_store_id', 'month'])\n",
    "\n",
    "stores = pd.merge(stores, data['as'], how= \"left\", on = ['air_store_id'])\n",
    "\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/', ' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-', ' ')))\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name' + str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ' '))\n",
    "    stores['air_area_name' + str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ' '))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "#土日フラグ(day_of_week_1)と、休日前(holi_2)フラグを追加\n",
    "data['hol']['day_of_week_1']= data['hol']['day_of_week'].replace(['Saturday', 'Sunday','Monday','Tuesday','Wednesday','Thursday','Friday'],['1', '1','0','0','0','0','0']).astype('int')\n",
    "data['hol']['holi_2'] = data['hol'][['holiday_flg', 'day_of_week_1']].sum(axis = 1)\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].apply( lambda x: 0 if x < 1 else 1 )\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].shift(-1)\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].fillna(1)\n",
    "data['hol']['holi_2'] = data['hol']['holi_2'].astype('int')\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how ='left', on = ['visit_date'])\n",
    "test = pd.merge(data['tes'], data['hol'], how ='left', on = ['visit_date'])\n",
    "\n",
    "#曜日と月でmerge\n",
    "train = pd.merge(train, stores, how ='left', on = ['air_store_id', 'dow','month'])\n",
    "test = pd.merge(test, stores, how ='left', on = ['air_store_id', 'dow','month'])\n",
    "\n",
    "#ID×休日前でのvisitorsの平均、中央値等を追加\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].min().rename(columns = {'visitors':'h_min_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'h_mean_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].median().rename(columns = {'visitors':'h_median_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].max().rename(columns = {'visitors':'h_max_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "tmp = train.groupby(['air_store_id','holi_2'], as_index = False)['visitors'].count().rename(columns = {'visitors':'h_count_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','holi_2'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1) \n",
    "\n",
    "train['total_reserve_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserve_mean'] = (train['rv2_x'] + train['rv2_y'])/2\n",
    "train['total_reserve_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y'])/2\n",
    "\n",
    "test['total_reserve_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserve_mean'] = (test['rv2_x'] + test['rv2_y'])/2\n",
    "test['total_reserve_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y'])/2\n",
    "\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2']= lbl.fit_transform(test['air_store_id'])\n",
    "\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date', 'visitors']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "\n",
    "all_data = pd.concat([train, test]) \n",
    "\n",
    "#指数移動平均の追加。これはなくても良いかも\n",
    "#https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/46179#266344\n",
    "#def calc_shifted_ewm(series, alpha, adjust=True):\n",
    "#    return series.shift().ewm(alpha=alpha, adjust=adjust).mean()\n",
    "\n",
    "#train['ewm'] = train.groupby(['air_store_id', 'dow']).apply(lambda g: calc_shifted_ewm(g['visitors'], 0.1)).sort_index(level=['air_store_id']).values\n",
    "\n",
    "#以下、気象データの追加\n",
    "df_air_store_weather_station = pd.read_csv('../../../mltestdata/05_recruit/air_store_info_with_nearest_active_station.csv')\n",
    "\n",
    "cols = ['air_store_id', 'station_id', 'station_latitude', 'station_longitude', 'station_vincenty', 'station_great_circle']\n",
    "all_data = pd.merge(all_data, df_air_store_weather_station[cols], on='air_store_id', how='left')\n",
    "\n",
    "combine = all_data\n",
    "filenames = []\n",
    "df_weather = None\n",
    "for station_id in combine['station_id'].unique():\n",
    "    fn = f\"../../../mltestdata/05_recruit/1-1-16_5-31-17_Weather/{station_id}.csv\"\n",
    "    if not fn in filenames:\n",
    "        df = pd.read_csv(fn)\n",
    "        df['station_id'] = station_id\n",
    "        if df_weather is None:\n",
    "            df_weather = df\n",
    "        else:\n",
    "            df_weather = pd.concat([df_weather, df])\n",
    "        del df\n",
    "\n",
    "        filenames.append(fn)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "#欠損値を平均で穴埋め（median, ffillで試すも特に差は出なかった）\n",
    "df_weather = df_weather.fillna(df_weather.mean())\n",
    "\n",
    "df_weather = df_weather.rename(columns={'calendar_date': 'visit_date'})\n",
    "\n",
    "df_weather['visit_date'] = pd.to_datetime(df_weather['visit_date'])\n",
    "df_weather['visit_date'] = df_weather['visit_date'].dt.date\n",
    "\n",
    "#なんとなく対数化\n",
    "df_weather['precipitation'] = np.log1p(df_weather['precipitation'])\n",
    "\n",
    "#使いそうなデータだけ結合（その他の気象データは試していません。特に意味はなし）\n",
    "cols = ['station_id', \n",
    "    'visit_date', \n",
    "    'precipitation', \n",
    "    'hours_sunlight',\n",
    "    'avg_temperature',\n",
    "    'high_temperature',\n",
    "    'low_temperature']\n",
    "\n",
    "combine = pd.merge(combine, df_weather[cols], on=['station_id', 'visit_date'], how='left')\n",
    "\n",
    "#降水量をカテゴリ化\n",
    "def simplify_pre(df):\n",
    "    df.precipitation = df.precipitation.fillna(0)\n",
    "    bins = ( -1, 0.01, 2,  5)\n",
    "    group_names = ['1', '2', '3']\n",
    "    categories = pd.cut(df.precipitation, bins, labels=group_names)\n",
    "    df.precipitation = categories\n",
    "    return df\n",
    "\n",
    "combine = simplify_pre(combine) \n",
    "all_data = combine \n",
    "\n",
    "#不要そうなデータを削除\n",
    "drop_col =['station_id', 'station_latitude','station_longitude','station_vincenty', 'station_great_circle','hours_sunlight','high_temperature','low_temperature']\n",
    "all_data = all_data.drop(drop_col, axis = 1)\n",
    "\n",
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]\n",
    "\n",
    "#ID×降水量で平均、中央値等を追加\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].min().rename(columns = {'visitors':'p_min_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].mean().rename(columns = {'visitors':'p_mean_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].median().rename(columns = {'visitors':'p_median_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation',])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].max().rename(columns = {'visitors':'p_max_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "tmp = train.groupby(['air_store_id','precipitation'], as_index = False)['visitors'].count().rename(columns = {'visitors':'p_count_visitors'})\n",
    "train = pd.merge(train, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "test = pd.merge(test, tmp, how ='left', on = ['air_store_id','precipitation'])\n",
    "\n",
    "#countをLabel Encoder化。なんとなく試してみたら結果が良かった。\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['count_visitors'] = lbl.fit_transform(train['count_visitors']) \n",
    "test['count_visitors']= lbl.fit_transform(test['count_visitors'])\n",
    "train['m_count_visitors'] = lbl.fit_transform(train['m_count_visitors'])\n",
    "test['m_count_visitors']= lbl.fit_transform(test['m_count_visitors'])\n",
    "train['h_count_visitors'] = lbl.fit_transform(train['h_count_visitors'])\n",
    "test['h_count_visitors']= lbl.fit_transform(test['h_count_visitors'])\n",
    "#train['h_count_reserves'] = lbl.fit_transform(train['h_count_visitors'])\n",
    "#test['h_count_reserves']= lbl.fit_transform(test['h_count_visitors'])\n",
    "train['p_count_visitors'] = lbl.fit_transform(train['p_count_visitors'])\n",
    "test['p_count_visitors']= lbl.fit_transform(test['p_count_visitors'])\n",
    "\n",
    "# GW flag\n",
    "combine = [train, test]\n",
    "gw_list = ['2016-04-29','2016-04-30','2016-05-01','2016-05-02','2016-05-03','2016-05-04','2016-05-05','2017-04-29','2017-04-30','2017-05-01','2017-05-02','2017-05-03','2017-05-04','2017-05-05']\n",
    "post_gw_list=['2016-05-06']\n",
    "train['gw_flg'] = 0\n",
    "train['post_gw_flg'] = 0\n",
    "test['gw_flg'] = 0\n",
    "test['post_gw_flg'] = 0\n",
    "update_gw_list = [[\"0\" for i in range(3)] for j in range(len(gw_list))]\n",
    "update_post_gw_list = [[\"0\" for i in range(3)] for j in range(len(post_gw_list))]\n",
    "\n",
    "from datetime import date\n",
    "for index, gw_date in enumerate(gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_gw_list[index][col_i]=int(temp_figure)\n",
    "        \n",
    "    #print(\"{}  {}  {}\".format(update_list[index][0],update_list[index][1],update_list[index][2]))\n",
    "    \n",
    "for index, gw_date in enumerate(post_gw_list):\n",
    "    temp_list = gw_date.split(\"-\")\n",
    "    for col_i, temp_figure in enumerate(temp_list):\n",
    "        update_post_gw_list[index][col_i]=int(temp_figure)\n",
    "\n",
    "for dataset in combine:\n",
    "    for index in range(len(update_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_gw_list[index][0],update_gw_list[index][1],update_gw_list[index][2]), 'gw_flg'] = 1\n",
    "        \n",
    "for dataset in combine:\n",
    "    for index in range(len(update_post_gw_list)):\n",
    "        dataset.loc[dataset.visit_date == date(update_post_gw_list[index][0],update_post_gw_list[index][1],update_post_gw_list[index][2]), 'post_gw_flg'] = 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['precipitation'] = train['precipitation'].values.astype(float)\n",
    "test['precipitation'] = test['precipitation'].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NaN\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "#drop_cols=['air_store_id','visit_date','id']\n",
    "y_train=train['visitors']\n",
    "x_train=train.drop(drop_cols, axis=1)\n",
    "\n",
    "x_test=test.copy()\n",
    "x_test=x_test.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.visitors\n",
    "train_input = train.copy()\n",
    "test_input = test.copy()\n",
    "\n",
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "train_input=train_input.drop(drop_cols, axis=1)\n",
    "test_input=test_input.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define utility tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252108, 75)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.precipitation.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_col(n=21): ['air_store_id', 'air_store_id2', 'count_visitors', 'date_int', 'day_of_week', 'day_of_week_1', 'dow', 'h_count_visitors', 'h_max_visitors', 'h_min_visitors', 'holi_2', 'holiday_flg', 'id', 'm_count_visitors', 'month', 'visit_date', 'visitors', 'year', 'p_count_visitors', 'gw_flg', 'post_gw_flg'] \n",
      "\n",
      "nn_col(n=54): ['air_area_name', 'air_area_name0', 'air_area_name1', 'air_area_name2', 'air_area_name3', 'air_area_name4', 'air_area_name5', 'air_area_name6', 'air_area_name7', 'air_area_name8', 'air_area_name9', 'air_genre_name', 'air_genre_name0', 'air_genre_name1', 'air_genre_name2', 'air_genre_name3', 'air_genre_name4', 'air_genre_name5', 'air_genre_name6', 'air_genre_name7', 'air_genre_name8', 'air_genre_name9', 'h_mean_visitors', 'h_median_visitors', 'latitude', 'lon_plus_lat', 'longitude', 'm_max_visitors', 'm_mean_visitors', 'm_median_visitors', 'm_min_visitors', 'max_visitors', 'mean_visitors', 'median_visitors', 'min_visitors', 'rs1_x', 'rs1_y', 'rs2_x', 'rs2_y', 'rv1_x', 'rv1_y', 'rv2_x', 'rv2_y', 'total_reserve_dt_diff_mean', 'total_reserve_mean', 'total_reserve_sum', 'var_max_lat', 'var_max_long', 'precipitation', 'avg_temperature', 'p_min_visitors', 'p_mean_visitors', 'p_median_visitors', 'p_max_visitors'] \n"
     ]
    }
   ],
   "source": [
    "#value_col: taken as float input(which are normalized)\n",
    "#nn_col - value_col: taken as categorical inputs(embedding layers used)\n",
    "\n",
    "value_col_list = []\n",
    "nn_col_list = []\n",
    "\n",
    "for c, dtype in zip(train.columns, train.dtypes):\t\n",
    "    if dtype == np.object:\t\t\n",
    "        value_col_list.append(c)\n",
    "\n",
    "    elif dtype == np.int64 :\t\t\n",
    "        value_col_list.append(c)\n",
    "        \n",
    "    elif dtype == np.float64:\n",
    "        nn_col_list.append(c)\n",
    "\n",
    "    else:\n",
    "        print(c)\n",
    "\n",
    "print(\"value_col(n={}): {} \".format(len(value_col_list),value_col_list))\n",
    "print(\"\\nnn_col(n={}): {} \".format(len(nn_col_list),nn_col_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_col = ['holiday_flg',\n",
    "             'min_visitors',\n",
    "             'mean_visitors',\n",
    "             'median_visitors',\n",
    "             'max_visitors',\n",
    "             #'count_observations',\n",
    "             'rs1_x',\n",
    "             'rv1_x',\n",
    "             'rs2_x',\n",
    "             'rv2_x',\n",
    "             'rs1_y',\n",
    "             'rv1_y',\n",
    "             'rs2_y',\n",
    "             'rv2_y',\n",
    "             'total_reserve_sum',\n",
    "             'total_reserve_mean',\n",
    "             'total_reserve_dt_diff_mean',\n",
    "             'date_int',\n",
    "             'var_max_lat',\n",
    "             'var_max_long',\n",
    "             'lon_plus_lat',\n",
    "             'h_max_visitors',\n",
    "             'h_mean_visitors',\n",
    "             'h_median_visitors',\n",
    "             'h_min_visitors',\n",
    "             'm_max_visitors', \n",
    "             'm_mean_visitors', \n",
    "             'm_median_visitors', \n",
    "             'm_min_visitors',\n",
    "             'p_min_visitors', \n",
    "             'p_mean_visitors', \n",
    "             'p_median_visitors', \n",
    "             'p_max_visitors',\n",
    "             'latitude',\n",
    "             'longitude',  \n",
    "            ]\n",
    "\n",
    "nn_col = value_col + ['dow', \n",
    "                      'day_of_week', \n",
    "                      'day_of_week_1', \n",
    "                      'gw_flg',\n",
    "                      'post_gw_flg',\n",
    "                      'holi_2',             \n",
    "                      'h_count_visitors',\n",
    "                      'm_count_visitors',\n",
    "                      'p_count_visitors',\n",
    "                      'precipitation',\n",
    "                      'year', \n",
    "                      'month', \n",
    "                      'air_store_id2', \n",
    "                      'air_area_name', \n",
    "                      'air_genre_name',\n",
    "                      'air_area_name0', \n",
    "                      'air_area_name1', \n",
    "                      'air_area_name2', \n",
    "                      'air_area_name3', \n",
    "                      'air_area_name4',\n",
    "                      'air_area_name5', \n",
    "                      'air_area_name6', \n",
    "                      'air_area_name7',\n",
    "                      'air_area_name8', \n",
    "                      'air_area_name9', \n",
    "                      'air_genre_name0', \n",
    "                      'air_genre_name1',\n",
    "                      'air_genre_name2', \n",
    "                      'air_genre_name3', \n",
    "                      'air_genre_name4',\n",
    "                      'air_genre_name5',\n",
    "                      'air_genre_name6', \n",
    "                      'air_genre_name7', \n",
    "                      'air_genre_name8',\n",
    "                      'air_genre_name9'\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test data prepared\n"
     ]
    }
   ],
   "source": [
    "X = train.copy()\n",
    "X_test = test[nn_col].copy()\n",
    "\n",
    "value_scaler = preprocessing.MinMaxScaler()\n",
    "for vcol in value_col:\n",
    "    X[vcol] = value_scaler.fit_transform(X[vcol].values.astype(np.float64).reshape(-1, 1))\n",
    "    X_test[vcol] = value_scaler.transform(X_test[vcol].values.astype(np.float64).reshape(-1, 1))\n",
    "\n",
    "X_train = list(X[nn_col].T.as_matrix())\n",
    "Y_train = np.log1p(X['visitors']).values\n",
    "nn_train = [X_train, Y_train]\n",
    "nn_test = [list(X_test[nn_col].T.as_matrix())]\n",
    "print(\"Train and test data prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_complete_model(train, hidden1_neurons=35, hidden2_neurons=15):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        train:           train dataframe(used to define the input size of the embedding layer)\n",
    "        hidden1_neurons: number of neurons in the first hidden layer\n",
    "        hidden2_neurons: number of neurons in the first hidden layer\n",
    "    Output:\n",
    "        return 'keras neural network model'\n",
    "    \"\"\"\n",
    "    K.clear_session()\n",
    "\n",
    "    air_store_id = Input(shape=(1,), dtype='int32', name='air_store_id')\n",
    "    air_store_id_emb = Embedding(len(train['air_store_id2'].unique()) + 1, 15, input_shape=(1,),\n",
    "                                 name='air_store_id_emb')(air_store_id)\n",
    "    air_store_id_emb = keras.layers.Flatten(name='air_store_id_emb_flatten')(air_store_id_emb)\n",
    "\n",
    "    dow = Input(shape=(1,), dtype='int32', name='dow')\n",
    "    dow_emb = Embedding(8, 3, input_shape=(1,), name='dow_emb')(dow)\n",
    "    dow_emb = keras.layers.Flatten(name='dow_emb_flatten')(dow_emb)\n",
    "\n",
    "    month = Input(shape=(1,), dtype='int32', name='month')\n",
    "    month_emb = Embedding(13, 3, input_shape=(1,), name='month_emb')(month)\n",
    "    month_emb = keras.layers.Flatten(name='month_emb_flatten')(month_emb)\n",
    "\n",
    "    air_area_name, air_genre_name = [], []\n",
    "    air_area_name_emb, air_genre_name_emb = [], []\n",
    "    for i in range(7):\n",
    "        area_name_col = 'air_area_name' + str(i)\n",
    "        air_area_name.append(Input(shape=(1,), dtype='int32', name=area_name_col))\n",
    "        tmp = Embedding(len(train[area_name_col].unique()), 3, input_shape=(1,),\n",
    "                        name=area_name_col + '_emb')(air_area_name[-1])\n",
    "        tmp = keras.layers.Flatten(name=area_name_col + '_emb_flatten')(tmp)\n",
    "        air_area_name_emb.append(tmp)\n",
    "\n",
    "        if i > 4:\n",
    "            continue\n",
    "        area_genre_col = 'air_genre_name' + str(i)\n",
    "        air_genre_name.append(Input(shape=(1,), dtype='int32', name=area_genre_col))\n",
    "        tmp = Embedding(len(train[area_genre_col].unique()), 3, input_shape=(1,),\n",
    "                        name=area_genre_col + '_emb')(air_genre_name[-1])\n",
    "        tmp = keras.layers.Flatten(name=area_genre_col + '_emb_flatten')(tmp)\n",
    "        air_genre_name_emb.append(tmp)\n",
    "\n",
    "    air_genre_name_emb = keras.layers.concatenate(air_genre_name_emb)\n",
    "    air_genre_name_emb = Dense(4, activation='sigmoid', name='final_air_genre_emb')(air_genre_name_emb)\n",
    "\n",
    "    air_area_name_emb = keras.layers.concatenate(air_area_name_emb)\n",
    "    air_area_name_emb = Dense(4, activation='sigmoid', name='final_air_area_emb')(air_area_name_emb)\n",
    "    \n",
    "    air_area_code = Input(shape=(1,), dtype='int32', name='air_area_code')\n",
    "    air_area_code_emb = Embedding(len(train['air_area_name'].unique()), 8, input_shape=(1,), name='air_area_code_emb')(air_area_code)\n",
    "    air_area_code_emb = keras.layers.Flatten(name='air_area_code_emb_flatten')(air_area_code_emb)\n",
    "    \n",
    "    air_genre_code = Input(shape=(1,), dtype='int32', name='air_genre_code')\n",
    "    air_genre_code_emb = Embedding(len(train['air_genre_name'].unique()), 5, input_shape=(1,),\n",
    "                                   name='air_genre_code_emb')(air_genre_code)\n",
    "    air_genre_code_emb = keras.layers.Flatten(name='air_genre_code_emb_flatten')(air_genre_code_emb)\n",
    "\n",
    "    \n",
    "    holiday_flg = Input(shape=(1,), dtype='float32', name='holiday_flg')\n",
    "    year = Input(shape=(1,), dtype='float32', name='year')\n",
    "    min_visitors = Input(shape=(1,), dtype='float32', name='min_visitors')\n",
    "    mean_visitors = Input(shape=(1,), dtype='float32', name='mean_visitors')\n",
    "    median_visitors = Input(shape=(1,), dtype='float32', name='median_visitors')\n",
    "    max_visitors = Input(shape=(1,), dtype='float32', name='max_visitors')\n",
    "    count_observations = Input(shape=(1,), dtype='float32', name='count_observations')\n",
    "    rs1_x = Input(shape=(1,), dtype='float32', name='rs1_x')\n",
    "    rv1_x = Input(shape=(1,), dtype='float32', name='rv1_x')\n",
    "    rs2_x = Input(shape=(1,), dtype='float32', name='rs2_x')\n",
    "    rv2_x = Input(shape=(1,), dtype='float32', name='rv2_x')\n",
    "    rs1_y = Input(shape=(1,), dtype='float32', name='rs1_y')\n",
    "    rv1_y = Input(shape=(1,), dtype='float32', name='rv1_y')\n",
    "    rs2_y = Input(shape=(1,), dtype='float32', name='rs2_y')\n",
    "    rv2_y = Input(shape=(1,), dtype='float32', name='rv2_y')\n",
    "    total_reserv_sum = Input(shape=(1,), dtype='float32', name='total_reserv_sum')\n",
    "    total_reserv_mean = Input(shape=(1,), dtype='float32', name='total_reserv_mean')\n",
    "    total_reserv_dt_diff_mean = Input(shape=(1,), dtype='float32', name='total_reserv_dt_diff_mean')\n",
    "    date_int = Input(shape=(1,), dtype='float32', name='date_int')\n",
    "    var_max_lat = Input(shape=(1,), dtype='float32', name='var_max_lat')\n",
    "    var_max_long = Input(shape=(1,), dtype='float32', name='var_max_long')\n",
    "    lon_plus_lat = Input(shape=(1,), dtype='float32', name='lon_plus_lat')\n",
    "\n",
    "    date_emb = keras.layers.concatenate([dow_emb, month_emb, year, holiday_flg])\n",
    "    date_emb = Dense(5, activation='sigmoid', name='date_merged_emb')(date_emb)\n",
    "\n",
    "    cat_layer = keras.layers.concatenate([holiday_flg, min_visitors, mean_visitors,\n",
    "                    median_visitors, max_visitors, count_observations, rs1_x, rv1_x,\n",
    "                    rs2_x, rv2_x, rs1_y, rv1_y, rs2_y, rv2_y,\n",
    "                    total_reserv_sum, total_reserv_mean, total_reserv_dt_diff_mean,\n",
    "                    date_int, var_max_lat, var_max_long, lon_plus_lat,\n",
    "                    date_emb, air_area_name_emb, air_genre_name_emb,\n",
    "                    air_area_code_emb, air_genre_code_emb, air_store_id_emb])\n",
    "\n",
    "    m = Dense(hidden1_neurons, name='hidden1',\n",
    "             kernel_initializer=keras.initializers.RandomNormal(mean=0.0,\n",
    "                            stddev=0.05, seed=None))(cat_layer)\n",
    "    m = keras.layers.LeakyReLU(alpha=0.2)(m)\n",
    "    m = keras.layers.BatchNormalization()(m)\n",
    "    \n",
    "    m1 = Dense(hidden2_neurons, name='sub1')(m)\n",
    "    m1 = keras.layers.LeakyReLU(alpha=0.2)(m1)\n",
    "    m = Dense(1, activation='relu')(m1)\n",
    "\n",
    "    #27?\n",
    "    inp_ten = [\n",
    "        holiday_flg, min_visitors, mean_visitors, median_visitors, max_visitors, count_observations,\n",
    "        rs1_x, rv1_x, rs2_x, rv2_x, rs1_y, rv1_y, rs2_y, rv2_y, total_reserv_sum, total_reserv_mean,\n",
    "        total_reserv_dt_diff_mean, date_int, var_max_lat, var_max_long, lon_plus_lat,\n",
    "        dow, year, month, air_store_id, air_area_code, air_genre_code\n",
    "    ]\n",
    "    inp_ten += air_area_name\n",
    "    inp_ten += air_genre_name\n",
    "    model = keras.Model(inp_ten, m)\n",
    "    model.compile(loss='mse', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ensemble.GradientBoostingRegressor(learning_rate=0.2, random_state=3,\n",
    "                    n_estimators=200, subsample=0.8, max_depth =10)\n",
    "model2 = neighbors.KNeighborsRegressor(n_jobs=-1, n_neighbors=4)\n",
    "model3 = XGBRegressor(learning_rate=0.2, random_state=3, n_estimators=200, subsample=0.8, \n",
    "                      colsample_bytree=0.8, max_depth =10)\n",
    "model4 = get_nn_complete_model(train, hidden2_neurons=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(train[col], np.log1p(train['visitors'].values))\n",
    "print(\"Model1 trained\")\n",
    "model2.fit(train[col], np.log1p(train['visitors'].values))\n",
    "print(\"Model2 trained\")\n",
    "model3.fit(train[col], np.log1p(train['visitors'].values))\n",
    "print(\"Model3 trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 39 array(s), but instead got the following list of 69 arrays: [array([ 0.,  0.,  0., ...,  0.,  0.,  0.]), array([ 0.06015038,  0.02255639,  0.03759398, ...,  0.02255639,\n        0.02255639,  0.02255639]), array([ 0.16737849,  0.14345154,  0.24077886, ...,  0.04...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-0ba92996d394>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     model4.fit(nn_train[0], nn_train[1], epochs=3, verbose=1,\n\u001b[0;32m----> 3\u001b[0;31m         batch_size=256, shuffle=True, validation_split=0.15)\n\u001b[0m\u001b[1;32m      4\u001b[0m     model4.fit(nn_train[0], nn_train[1], epochs=8, verbose=0,\n\u001b[1;32m      5\u001b[0m         batch_size=256, shuffle=True)\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1582\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1412\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1415\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1416\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     86\u001b[0m                                  \u001b[0;34m'the following list of '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                                  \u001b[0;34m' arrays: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                                  '...')\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 39 array(s), but instead got the following list of 69 arrays: [array([ 0.,  0.,  0., ...,  0.,  0.,  0.]), array([ 0.06015038,  0.02255639,  0.03759398, ...,  0.02255639,\n        0.02255639,  0.02255639]), array([ 0.16737849,  0.14345154,  0.24077886, ...,  0.04..."
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model4.fit(nn_train[0], nn_train[1], epochs=3, verbose=1,\n",
    "        batch_size=256, shuffle=True, validation_split=0.15)\n",
    "    model4.fit(nn_train[0], nn_train[1], epochs=8, verbose=0,\n",
    "        batch_size=256, shuffle=True)\n",
    "print(\"Model4 trained\")\n",
    "\n",
    "preds1 = model1.predict(train[col])\n",
    "preds2 = model2.predict(train[col])\n",
    "preds3 = model3.predict(train[col])\n",
    "preds4 = pd.Series(model4.predict(nn_train[0]).reshape(-1)).clip(0, 6.8).values\n",
    "\n",
    "print('RMSE GradientBoostingRegressor: ', RMSLE(np.log1p(train['visitors'].values), preds1))\n",
    "print('RMSE KNeighborsRegressor: ', RMSLE(np.log1p(train['visitors'].values), preds2))\n",
    "print('RMSE XGBRegressor: ', RMSLE(np.log1p(train['visitors'].values), preds3))\n",
    "print('RMSE NeuralNetwork: ', RMSLE(np.log1p(train['visitors'].values), preds4))\n",
    "\n",
    "preds1 = model1.predict(test[col])\n",
    "preds2 = model2.predict(test[col])\n",
    "preds3 = model3.predict(test[col])\n",
    "preds4 = pd.Series(model4.predict(nn_test[0]).reshape(-1)).clip(0, 6.8).values\n",
    "\n",
    "test['visitors'] = 0.2*preds1+0.2*preds2+0.3*preds3+0.3*preds4\n",
    "test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\n",
    "sub1 = test[['id','visitors']].copy()\n",
    "print(\"Model predictions done.\")\n",
    "# del train; del data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../input/*.csv')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub2 = sub2.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_visitors(x, alt=False):\n",
    "    visitors_x, visitors_y = x['visitors_x'], x['visitors_y']\n",
    "    if x['visitors_y'] == -1:\n",
    "        return visitors_x\n",
    "    else:\n",
    "        return 0.7*visitors_x + 0.3*visitors_y* 1.1\n",
    "\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')\n",
    "sub_merge['visitors'] = sub_merge.apply(lambda x: final_visitors(x), axis=1)\n",
    "print(\"Done\")\n",
    "sub_merge[['id', 'visitors']].to_csv('submission_rs_recruit_v19_keras_fe.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
