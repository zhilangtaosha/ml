{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1482535, 9)\n",
      "5 folds scaling the test_df\n",
      "(693359, 7)\n",
      "new shape  (3493359, 7)\n",
      "[11.543760299682617] Finished scaling test set...\n",
      "Handling missing values...\n",
      "(1482535, 9)\n",
      "(3493359, 7)\n",
      "[12.69942831993103] Finished handling missing data...\n",
      "Handling categorical variables...\n",
      "[36.50656223297119] Finished PROCESSING CATEGORICAL DATA...\n",
      "Text to seq process...\n",
      "   Fitting tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Transforming text to seq...\n",
      "[303.80174016952515] Finished PROCESSING TEXT DATA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suzukiry/lab/March/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1467709, 13)\n",
      "(14826, 13)\n",
      "[307.83780813217163] Finished EMBEDDINGS MAX VALUE...\n",
      "[379.1826841831207] Finished DATA PREPARARTION...\n",
      "[379.1837360858917] Finished DEFINEING MODEL...\n",
      "Train on 1453031 samples, validate on 14678 samples\n",
      "Epoch 1/2\n",
      "Epoch 2/2\n",
      "[1429.8054032325745] Finished FITTING MODEL...\n",
      " RMSLE error on dev test: 0.4411411978364853\n",
      "[1433.0065820217133] Finished predicting valid set...\n",
      "[1714.2832472324371] Finished predicting test set...\n",
      "[1717.293442249298] Finished predicting test set...\n",
      "[1723.3316674232483] Finished to handle missing\n",
      "[1729.4279792308807] Finished to cut\n",
      "[1730.3090052604675] Finished to convert categorical\n",
      "[1745.3592960834503] Finished count vectorize `name`\n",
      "[1758.9350593090057] Finished count vectorize `category_name`\n",
      "[2032.5536572933197] Finished TFIDF vectorize `item_description`\n",
      "[2040.6630532741547] Finished label binarize `brand_name`\n",
      "[2045.1820712089539] Finished to get dummies on `item_condition_id` and `shipping`\n",
      "[2073.172366142273] Finished to create sparse merge\n",
      "[2172.949141025543] Finished to train ridge\n",
      "[2173.022608280182] Finished to predict ridge\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# mainly forking from notebook\n",
    "# https://www.kaggle.com/johnfarrell/simple-rnn-with-keras-script\n",
    "\n",
    "# ADDED\n",
    "# 5x scaled test set\n",
    "# category name embedding\n",
    "# some small changes like lr, decay, batch_size~\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "start_time = time.time()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "NUM_BRANDS = 4000\n",
    "NUM_CATEGORIES = 1000\n",
    "NAME_MIN_DF = 10\n",
    "MAX_FEATURES_ITEM_DESCRIPTION = 40000\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['category_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='missing', inplace=True)\n",
    "\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category = dataset['category_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['category_name'].isin(pop_category), 'category_name'] = 'missing'\n",
    "\n",
    "\n",
    "def to_categorical(dataset):\n",
    "    dataset['category_name'] = dataset['category_name'].astype('category')\n",
    "    dataset['brand_name'] = dataset['brand_name'].astype('category')\n",
    "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n",
    "\n",
    "\n",
    "df_train = train = pd.read_csv('../input/train.tsv', sep='\\t')\n",
    "df_test = test = pd.read_csv('../input/test.tsv', sep='\\t')\n",
    "\n",
    "train['target'] = np.log1p(train['price'])\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "print('5 folds scaling the test_df')\n",
    "print(test.shape)\n",
    "test_len = test.shape[0]\n",
    "def simulate_test(test):\n",
    "    if test.shape[0] < 800000:\n",
    "        indices = np.random.choice(test.index.values, 2800000)\n",
    "        test_ = pd.concat([test, test.iloc[indices]], axis=0)\n",
    "        return test_.copy()\n",
    "    else:\n",
    "        return test\n",
    "test = simulate_test(test)\n",
    "print('new shape ', test.shape)\n",
    "print('[{}] Finished scaling test set...'.format(time.time() - start_time))\n",
    "\n",
    "#HANDLE MISSING VALUES\n",
    "print(\"Handling missing values...\")\n",
    "def handle_missing(dataset):\n",
    "    dataset.category_name.fillna(value=\"missing\", inplace=True)\n",
    "    dataset.brand_name.fillna(value=\"missing\", inplace=True)\n",
    "    dataset.item_description.fillna(value=\"missing\", inplace=True)\n",
    "    return (dataset)\n",
    "\n",
    "train = handle_missing(train)\n",
    "test = handle_missing(test)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "print('[{}] Finished handling missing data...'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "\n",
    "#PROCESS CATEGORICAL DATA\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "print(\"Handling categorical variables...\")\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(np.hstack([train.category_name, test.category_name]))\n",
    "train['category'] = le.transform(train.category_name)\n",
    "test['category'] = le.transform(test.category_name)\n",
    "\n",
    "le.fit(np.hstack([train.brand_name, test.brand_name]))\n",
    "train['brand'] = le.transform(train.brand_name)\n",
    "test['brand'] = le.transform(test.brand_name)\n",
    "del le, train['brand_name'], test['brand_name']\n",
    "\n",
    "print('[{}] Finished PROCESSING CATEGORICAL DATA...'.format(time.time() - start_time))\n",
    "train.head(3)\n",
    "\n",
    "\n",
    "#PROCESS TEXT: RAW\n",
    "print(\"Text to seq process...\")\n",
    "print(\"   Fitting tokenizer...\")\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "raw_text = np.hstack([train.category_name.str.lower(), \n",
    "                      train.item_description.str.lower(), \n",
    "                      train.name.str.lower()])\n",
    "\n",
    "tok_raw = Tokenizer()\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "print(\"   Transforming text to seq...\")\n",
    "train[\"seq_category_name\"] = tok_raw.texts_to_sequences(train.category_name.str.lower())\n",
    "test[\"seq_category_name\"] = tok_raw.texts_to_sequences(test.category_name.str.lower())\n",
    "train[\"seq_item_description\"] = tok_raw.texts_to_sequences(train.item_description.str.lower())\n",
    "test[\"seq_item_description\"] = tok_raw.texts_to_sequences(test.item_description.str.lower())\n",
    "train[\"seq_name\"] = tok_raw.texts_to_sequences(train.name.str.lower())\n",
    "test[\"seq_name\"] = tok_raw.texts_to_sequences(test.name.str.lower())\n",
    "train.head(3)\n",
    "\n",
    "print('[{}] Finished PROCESSING TEXT DATA...'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "#EXTRACT DEVELOPTMENT TEST\n",
    "from sklearn.model_selection import train_test_split\n",
    "dtrain, dvalid = train_test_split(train, random_state=666, train_size=0.99)\n",
    "print(dtrain.shape)\n",
    "print(dvalid.shape)\n",
    "\n",
    "\n",
    "#EMBEDDINGS MAX VALUE\n",
    "#Base on the histograms, we select the next lengths\n",
    "MAX_NAME_SEQ = 20 #17\n",
    "MAX_ITEM_DESC_SEQ = 60 #269\n",
    "MAX_CATEGORY_NAME_SEQ = 20 #8\n",
    "MAX_TEXT = np.max([np.max(train.seq_name.max())\n",
    "                   , np.max(test.seq_name.max())\n",
    "                   , np.max(train.seq_category_name.max())\n",
    "                   , np.max(test.seq_category_name.max())\n",
    "                   , np.max(train.seq_item_description.max())\n",
    "                   , np.max(test.seq_item_description.max())])+2\n",
    "MAX_CATEGORY = np.max([train.category.max(), test.category.max()])+1\n",
    "MAX_BRAND = np.max([train.brand.max(), test.brand.max()])+1\n",
    "MAX_CONDITION = np.max([train.item_condition_id.max(), \n",
    "                        test.item_condition_id.max()])+1\n",
    "\n",
    "print('[{}] Finished EMBEDDINGS MAX VALUE...'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "#KERAS DATA DEFINITION\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_keras_data(dataset):\n",
    "    X = {\n",
    "        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n",
    "        ,'item_desc': pad_sequences(dataset.seq_item_description\n",
    "                                    , maxlen=MAX_ITEM_DESC_SEQ)\n",
    "        ,'brand': np.array(dataset.brand)\n",
    "        ,'category': np.array(dataset.category)\n",
    "        ,'category_name': pad_sequences(dataset.seq_category_name\n",
    "                                        , maxlen=MAX_CATEGORY_NAME_SEQ)\n",
    "        ,'item_condition': np.array(dataset.item_condition_id)\n",
    "        ,'num_vars': np.array(dataset[[\"shipping\"]])\n",
    "    }\n",
    "    return X\n",
    "\n",
    "X_train = get_keras_data(dtrain)\n",
    "X_valid = get_keras_data(dvalid)\n",
    "X_test = get_keras_data(test)\n",
    "\n",
    "print('[{}] Finished DATA PREPARARTION...'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "\n",
    "#KERAS MODEL DEFINITION\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, \\\n",
    "    Activation, concatenate, GRU, Embedding, Flatten\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping#, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    import math\n",
    "    assert len(y) == len(y_pred)\n",
    "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 \\\n",
    "              for i, pred in enumerate(y_pred)]\n",
    "    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n",
    "\n",
    "dr = 0.25\n",
    "\n",
    "def get_model():\n",
    "    #params\n",
    "    dr_r = dr\n",
    "    \n",
    "    #Inputs\n",
    "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n",
    "    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n",
    "    brand = Input(shape=[1], name=\"brand\")\n",
    "    category = Input(shape=[1], name=\"category\")\n",
    "    category_name = Input(shape=[X_train[\"category_name\"].shape[1]], \n",
    "                          name=\"category_name\")\n",
    "    item_condition = Input(shape=[1], name=\"item_condition\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    \n",
    "    #Embeddings layers\n",
    "    emb_size = 60\n",
    "    \n",
    "    emb_name = Embedding(MAX_TEXT, emb_size//3)(name)\n",
    "    emb_item_desc = Embedding(MAX_TEXT, emb_size)(item_desc)\n",
    "    emb_category_name = Embedding(MAX_TEXT, emb_size//3)(category_name)\n",
    "    emb_brand = Embedding(MAX_BRAND, 10)(brand)\n",
    "    emb_category = Embedding(MAX_CATEGORY, 10)(category)\n",
    "    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n",
    "    \n",
    "    rnn_layer1 = GRU(16) (emb_item_desc)\n",
    "    rnn_layer2 = GRU(8) (emb_category_name)\n",
    "    rnn_layer3 = GRU(8) (emb_name)\n",
    "    \n",
    "    #main layer\n",
    "    main_l = concatenate([\n",
    "        Flatten() (emb_brand)\n",
    "        , Flatten() (emb_category)\n",
    "        , Flatten() (emb_item_condition)\n",
    "        , rnn_layer1\n",
    "        , rnn_layer2\n",
    "        , rnn_layer3\n",
    "        , num_vars\n",
    "    ])\n",
    "    main_l = Dropout(0.3)(Dense(512,activation='relu') (main_l))\n",
    "    main_l = Dropout(0.2)(Dense(88,activation='relu') (main_l))\n",
    "    \n",
    "    #output\n",
    "    output = Dense(1,activation=\"linear\") (main_l)\n",
    "    \n",
    "    #model\n",
    "    model = Model([name, item_desc, brand\n",
    "                   , category, category_name\n",
    "                   , item_condition, num_vars], output)\n",
    "    #optimizer = optimizers.RMSprop()\n",
    "    optimizer = optimizers.Adam()\n",
    "    model.compile(loss=\"mse\", \n",
    "                  optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def eval_model(model):\n",
    "    val_preds = model.predict(X_valid)\n",
    "    val_preds = np.expm1(val_preds)\n",
    "    \n",
    "    y_true = np.array(dvalid.price.values)\n",
    "    y_pred = val_preds[:, 0]\n",
    "    v_rmsle = rmsle(y_true, y_pred)\n",
    "    print(\" RMSLE error on dev test: \"+str(v_rmsle))\n",
    "    return v_rmsle\n",
    "#fin_lr=init_lr * (1/(1+decay))**(steps-1)\n",
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "\n",
    "print('[{}] Finished DEFINEING MODEL...'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "#FITTING THE MODEL\n",
    "epochs = 2\n",
    "BATCH_SIZE = 512 * 3\n",
    "steps = int(len(X_train['name'])/BATCH_SIZE) * epochs\n",
    "lr_init, lr_fin = 0.009, 0.006\n",
    "lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "log_subdir = '_'.join(['ep', str(epochs),\n",
    "                    'bs', str(BATCH_SIZE),\n",
    "                    'lrI', str(lr_init),\n",
    "                    'lrF', str(lr_fin),\n",
    "                    'dr', str(dr)])\n",
    "\n",
    "model = get_model()\n",
    "K.set_value(model.optimizer.lr, lr_init)\n",
    "K.set_value(model.optimizer.decay, lr_decay)\n",
    "\n",
    "history = model.fit(X_train, dtrain.target\n",
    "                    , epochs=epochs\n",
    "                    , batch_size=BATCH_SIZE\n",
    "                    , validation_split=0.01\n",
    "                    #, callbacks=[TensorBoard('./logs/'+log_subdir)]\n",
    "                    , verbose=10\n",
    "                    )\n",
    "print('[{}] Finished FITTING MODEL...'.format(time.time() - start_time))\n",
    "#EVLUEATE THE MODEL ON DEV TEST\n",
    "v_rmsle = eval_model(model)\n",
    "print('[{}] Finished predicting valid set...'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "#CREATE PREDICTIONS\n",
    "preds = model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "preds = np.expm1(preds)\n",
    "print('[{}] Finished predicting test set...'.format(time.time() - start_time))\n",
    "submission = test[[\"test_id\"]][:test_len]\n",
    "submission[\"price\"] = preds[:test_len]*0.9\n",
    "print('[{}] Finished predicting test set...'.format(time.time() - start_time))\n",
    "\n",
    "del train\n",
    "del test\n",
    "gc.collect()\n",
    "\n",
    "#Ridge https://www.kaggle.com/apapiu/ridge-script\n",
    "\n",
    "nrow_train = df_train.shape[0]\n",
    "y = np.log1p(df_train[\"price\"])\n",
    "merge: pd.DataFrame = pd.concat([df_train, df_test])\n",
    "\n",
    "del df_train\n",
    "del df_test\n",
    "gc.collect()\n",
    "\n",
    "handle_missing_inplace(merge)\n",
    "print('[{}] Finished to handle missing'.format(time.time() - start_time))\n",
    "\n",
    "cutting(merge)\n",
    "print('[{}] Finished to cut'.format(time.time() - start_time))\n",
    "\n",
    "to_categorical(merge)\n",
    "print('[{}] Finished to convert categorical'.format(time.time() - start_time))\n",
    "\n",
    "cv = CountVectorizer(min_df=NAME_MIN_DF)\n",
    "X_name = cv.fit_transform(merge['name'])\n",
    "print('[{}] Finished count vectorize `name`'.format(time.time() - start_time))\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_category = cv.fit_transform(merge['category_name'])\n",
    "print('[{}] Finished count vectorize `category_name`'.format(time.time() - start_time))\n",
    "\n",
    "tv = TfidfVectorizer(max_features=MAX_FEATURES_ITEM_DESCRIPTION,\n",
    "                         ngram_range=(1, 3),\n",
    "                         stop_words='english')\n",
    "X_description = tv.fit_transform(merge['item_description'])\n",
    "print('[{}] Finished TFIDF vectorize `item_description`'.format(time.time() - start_time))\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_brand = lb.fit_transform(merge['brand_name'])\n",
    "print('[{}] Finished label binarize `brand_name`'.format(time.time() - start_time))\n",
    "\n",
    "X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n",
    "                                          sparse=True).values)\n",
    "print('[{}] Finished to get dummies on `item_condition_id` and `shipping`'.format(time.time() - start_time))\n",
    "\n",
    "sparse_merge = hstack((X_dummies, X_description, X_brand, X_category, X_name)).tocsr()\n",
    "print('[{}] Finished to create sparse merge'.format(time.time() - start_time))\n",
    "\n",
    "X = sparse_merge[:nrow_train]\n",
    "X_test = sparse_merge[nrow_train:]\n",
    "model = Ridge(solver=\"sag\", fit_intercept=True, alpha = 3.5, random_state=666)\n",
    "model.fit(X, y)\n",
    "print('[{}] Finished to train ridge'.format(time.time() - start_time))\n",
    "predsR = model.predict(X=X_test)\n",
    "print('[{}] Finished to predict ridge'.format(time.time() - start_time))\n",
    "predsR = np.expm1(predsR)\n",
    "predsR = predsR*0.07\n",
    "submission[\"price\"] += predsR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3413.316430091858] Finished to train ridge\n",
      "[3413.42072224617] Finished to predict ridge\n"
     ]
    }
   ],
   "source": [
    "model = Ridge(solver=\"sag\", fit_intercept=False, alpha = 1.4, random_state=666)\n",
    "model.fit(X, y)\n",
    "print('[{}] Finished to train ridge'.format(time.time() - start_time))\n",
    "predsRR = model.predict(X=X_test)\n",
    "print('[{}] Finished to predict ridge'.format(time.time() - start_time))\n",
    "predsRR = np.expm1(predsRR)\n",
    "predsRR = predsRR*0.03\n",
    "submission[\"price\"] += predsRR\n",
    "\n",
    "submission.to_csv(\"submission_rnn_and_ridge.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
