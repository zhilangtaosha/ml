{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../../../mltestdata/05_recruit/air_reserve.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5016e82a656d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading Data ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# air_reserve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf_ar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../../mltestdata/05_recruit/air_reserve.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m# air_store_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mdf_as\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../../mltestdata/05_recruit/air_store_info.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/March/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../../../mltestdata/05_recruit/air_reserve.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Nov 29 10:49:08 2017\n",
    "\n",
    "@author: Mengfei Li\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "\n",
    "#from ml_metrics import rmsle\n",
    "\n",
    "print(\"Loading Data ...\")\n",
    "# air_reserve\n",
    "df_ar = pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv')\n",
    "# air_store_info\n",
    "df_as = pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv')\n",
    "# air_visit_data\n",
    "df_av = pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv')\n",
    "# hpg_reserve\n",
    "df_hr = pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv')\n",
    "# hpg_store_info\n",
    "df_hs = pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv')\n",
    "# date_info\n",
    "df_di = pd.read_csv('../../../mltestdata/05_recruit/date_info.csv')\n",
    "# sample_submission\n",
    "df_ss = pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv')\n",
    "# store_id_relation\n",
    "df_si = pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv')\n",
    "\n",
    "# df_test\n",
    "df_test = pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv')\n",
    "df_test['air_store_id'] = df_test['id'].apply(lambda x: '_'.join(x.split('_')[:2]))\n",
    "df_test['visit_date'] = df_test['id'].apply(lambda x: x.split('_')[-1])\n",
    "index_test = df_test['id']\n",
    "del df_test['id'], df_test['visitors']\n",
    "\n",
    "gc.collect()\n",
    "print(\"Loading Data Compelete.\")\n",
    "\n",
    "print(\"=========================================================================================\")\n",
    "print(\"Data Exploring ...\")\n",
    "print(\"=========================================================================================\")\n",
    "print(\"Unique store id in different dataset :\")\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "num_store_ar = np.unique(df_ar['air_store_id'])\n",
    "print(\"Number of unique stores in 'df_ar' is:\" + str(len(num_store_ar)))\n",
    "\n",
    "num_store_as = np.unique(df_as['air_store_id'])\n",
    "print(\"Number of unique stores in 'df_as' is:\" + str(len(num_store_as)))\n",
    "\n",
    "num_store_av = np.unique(df_av['air_store_id'])\n",
    "print(\"Number of unique stores in 'df_av' is:\" + str(len(num_store_av)))\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "num_store_in_hr = np.unique(df_hr['hpg_store_id'])\n",
    "print(\"Number of unique stores in 'df_hr' is:\" + str(len(num_store_in_hr)))\n",
    "\n",
    "num_store_in_hs = np.unique(df_hs['hpg_store_id'])\n",
    "print(\"Number of unique stores in 'df_hs' is:\" + str(len(num_store_in_hs)))\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "num_store_in_test = np.unique(df_test['air_store_id'])\n",
    "print(\"Number of unique stores in 'df_test' is:\" + str(len(num_store_in_test)))\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "num_store_in_si = np.unique(df_si['air_store_id'])\n",
    "print(\"Number of unique stores in 'df_test' is:\" + str(len(num_store_in_si)))\n",
    "print(\"=========================================================================================\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# remove outliers\n",
    "# =============================================================================\n",
    "def remove_outliers(data):\n",
    "    df_0 = data.loc[data.visitors == 0]   \n",
    "    q1 = np.percentile(data.visitors, 25, axis=0)\n",
    "    q3 = np.percentile(data.visitors, 75, axis=0)\n",
    "#    k = 5\n",
    "#    k = 2.5\n",
    "    k = 2.8\n",
    "#    k = 2\n",
    "#    k = 1.5\n",
    "    iqr = q3 - q1\n",
    "    df_temp = data.loc[data.visitors > q1 - k*iqr]\n",
    "    df_temp = data.loc[data.visitors < q3 + k*iqr]\n",
    "    frames = [df_0, df_temp]\n",
    "    result = pd.concat(frames)\n",
    "    return result\n",
    "\n",
    "df_av = remove_outliers(df_av)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# df to dict for mapping and dropping\n",
    "# =============================================================================\n",
    "print('mapping and dropping useless information in df_hr ...')\n",
    "s_1 = df_si['air_store_id']\n",
    "s_2 = df_si['hpg_store_id']\n",
    "a_h_map = dict(zip(s_2.values, s_1.values))\n",
    "del s_1, s_2\n",
    "\n",
    "df_hr['air_store_id'] = df_hr['hpg_store_id'].map(a_h_map)\n",
    "df_hr = df_hr.drop('hpg_store_id', axis=1).dropna()\n",
    "\n",
    "\n",
    "print('mapping and dropping useless information in df_hr Done!')\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "print('mapping and dropping useless information in df_hr ...')\n",
    "\n",
    "df_hs['air_store_id'] = df_hs['hpg_store_id'].map(a_h_map)\n",
    "df_hs = df_hs.drop('hpg_store_id', axis=1).dropna()\n",
    "print('mapping and dropping useless information in df_hs Done!')\n",
    "gc.collect()\n",
    "print(\"=========================================================================================\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# handle datetime (no clock info)\n",
    "# =============================================================================\n",
    "print('seperating date time features ...')\n",
    "\n",
    "time_format = '%Y-%m-%d'\n",
    "def seperate_date(data):     \n",
    "    # split date feature in real visit datetime\n",
    "    data_time = pd.to_datetime(data.visit_date, format=time_format)\n",
    "    data['Year_visit']= data_time.dt.year\n",
    "    data['Month_visit'] = data_time.dt.month\n",
    "    data['DayOfYear_visit'] = data_time.dt.dayofyear\n",
    "    # data['DayOfMonth_visit'] = data_time.dt.day\n",
    "#    data['WeekOfYear_visit'] = data_time.dt.week\n",
    "    data['DayOfWeek_visit'] = data_time.dt.dayofweek\n",
    "#    del data['visit_date']\n",
    "    return data\n",
    "\n",
    "seperate_date(df_av)\n",
    "seperate_date(df_test)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "def seperate_date(data):\n",
    "    # split date feature in reservation datetime\n",
    "    data_time = pd.to_datetime(data.reserve_datetime, format=time_format)\n",
    "    data['Year_re']= data_time.dt.year\n",
    "    data['Month_re'] = data_time.dt.month\n",
    "    data['DayOfYear_re'] = data_time.dt.dayofyear\n",
    "    # data['DayOfMonth_re'] = data_time.dt.day\n",
    "#    data['WeekOfYear_re'] = data_time.dt.week\n",
    "    data['DayOfWeek_re'] = data_time.dt.dayofweek\n",
    "    data['Hour_re'] = data_time.dt.hour\n",
    "#    del data['reserve_datetime']\n",
    "    return data\n",
    "\n",
    "seperate_date(df_ar)\n",
    "\n",
    "\n",
    "def seperate_date(data):\n",
    "    # split date feature in reservation datetime\n",
    "    data_time = pd.to_datetime(data.reserve_datetime, format=time_format)\n",
    "    data['Year_re_h']= data_time.dt.year\n",
    "    data['Month_re_h'] = data_time.dt.month\n",
    "    data['DayOfYear_re_h'] = data_time.dt.dayofyear\n",
    "    # data['DayOfMonth_re_h'] = data_time.dt.day\n",
    "#    data['WeekOfYear_re_h'] = data_time.dt.week\n",
    "    data['DayOfWeek_re_h'] = data_time.dt.dayofweek\n",
    "    data['Hour_re_h'] = data_time.dt.hour\n",
    "#    del data['reserve_datetime']\n",
    "    return data\n",
    "\n",
    "seperate_date(df_hr)\n",
    "\n",
    "\n",
    "time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "def seperate_date(data):\n",
    "    # split date feature in reserved visiting datetime\n",
    "    data_time = pd.to_datetime(data.visit_datetime, format=time_format)\n",
    "    data['Year_re_visit']= data_time.dt.year\n",
    "    data['Month_re_visit'] = data_time.dt.month\n",
    "    data['DayOfYear_re_visit'] = data_time.dt.dayofyear\n",
    "    # data['DayOfMonth_re_visit'] = data_time.dt.day\n",
    "#    data['WeekOfYear_re_visit'] = data_time.dt.week\n",
    "    data['DayOfWeek_re_visit'] = data_time.dt.dayofweek\n",
    "    data['Hour_re_visit'] = data_time.dt.hour\n",
    "#    del data['visit_datetime']\n",
    "    return data\n",
    "\n",
    "seperate_date(df_ar)\n",
    "\n",
    "\n",
    "def seperate_date(data):\n",
    "    # split date feature in reserved visiting datetime\n",
    "    data_time = pd.to_datetime(data.visit_datetime, format=time_format)\n",
    "    data['Year_re_visit_h']= data_time.dt.year\n",
    "    data['Month_re_visit_h'] = data_time.dt.month\n",
    "    data['DayOfYear_re_visit_h'] = data_time.dt.dayofyear\n",
    "    # data['DayOfMonth_re_visit_h'] = data_time.dt.day\n",
    "    data['WeekOfYear_re_visit_h'] = data_time.dt.week\n",
    "    data['DayOfWeek_re_visit_h'] = data_time.dt.dayofweek\n",
    "    data['Hour_re_visit_h'] = data_time.dt.hour\n",
    "#    del data['visit_datetime']\n",
    "    return data\n",
    "\n",
    "seperate_date(df_hr)\n",
    "\n",
    "print('seperating date time features done! ...')\n",
    "gc.collect()\n",
    "print(\"=========================================================================================\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# label encoding\n",
    "# =============================================================================\n",
    "print('label encoding ...')\n",
    "\n",
    "le.fit(df_as['air_genre_name'])\n",
    "df_as['air_genre_name'] = le.fit_transform(df_as['air_genre_name'])\n",
    "\n",
    "le.fit(df_as['air_area_name'])\n",
    "df_as['air_area_name'] = le.fit_transform(df_as['air_area_name'])\n",
    "\n",
    "le.fit(df_hs['hpg_genre_name'])\n",
    "df_hs['hpg_genre_name'] = le.fit_transform(df_hs['hpg_genre_name'])\n",
    "\n",
    "le.fit(df_hs['hpg_area_name'])\n",
    "df_hs['hpg_area_name'] = le.fit_transform(df_hs['hpg_area_name'])\n",
    "\n",
    "\n",
    "\n",
    "le.fit(df_as['air_store_id'])\n",
    "\n",
    "\n",
    "df_ar['air_store_id'] = le.transform(df_ar['air_store_id'])\n",
    "df_as['air_store_id'] = le.transform(df_as['air_store_id'])\n",
    "df_av['air_store_id'] = le.transform(df_av['air_store_id'])\n",
    "df_hr['air_store_id'] = le.transform(df_hr['air_store_id'])\n",
    "df_hs['air_store_id'] = le.transform(df_hs['air_store_id'])\n",
    "\n",
    "df_test['air_store_id'] = le.transform(df_test['air_store_id'])\n",
    "\n",
    "\n",
    "print('label encoding done !')\n",
    "gc.collect()\n",
    "print(\"=========================================================================================\")\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Merge dataset\n",
    "# =============================================================================\n",
    "features_to_drop = [\n",
    "        'air_store_id__'\n",
    "        ]\n",
    "\n",
    "def merge_df(data, data_to_join):\n",
    "    # merge dataframes        \n",
    "    data = data.join(data_to_join, on='air_store_id', rsuffix='__', how='left')   \n",
    "    return data\n",
    "\n",
    "def fix_data(data):\n",
    "    # drop __ data    \n",
    "    for feature in features_to_drop:\n",
    "        del data[feature]\n",
    "    return data\n",
    "\n",
    "# Merge to df_train\n",
    "print('merging dataframes ...')\n",
    "df_train = merge_df(df_av, df_ar)\n",
    "df_train = merge_df(df_train, df_as)\n",
    "\n",
    "df_hr['reserve_visitors_hr'] = df_hr['reserve_visitors'] \n",
    "del df_hr['reserve_visitors'] \n",
    "\n",
    "df_hs['latitude_hr'] = df_hs['latitude'] \n",
    "del df_hs['latitude'] \n",
    "\n",
    "df_hs['longitude_hr'] = df_hs['longitude'] \n",
    "del df_hs['longitude'] \n",
    "\n",
    "df_train = merge_df(df_train, df_hs)\n",
    "df_train = merge_df(df_train, df_hr)\n",
    "gc.collect()\n",
    "fix_data(df_train)\n",
    "\n",
    "# Merge to df_test\n",
    "\n",
    "df_test = merge_df(df_test, df_ar)\n",
    "df_test = merge_df(df_test, df_as)\n",
    "\n",
    "df_test = merge_df(df_test, df_hs)\n",
    "df_test = merge_df(df_test, df_hr)\n",
    "gc.collect()\n",
    "fix_data(df_test)\n",
    "\n",
    "\n",
    "print('merging dataframes done!')\n",
    "gc.collect()\n",
    "print(\"=========================================================================================\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# add holiday feature (for the visiting day)\n",
    "# =============================================================================\n",
    "df_di['visit_date'] = df_di['calendar_date']\n",
    "del df_di['calendar_date'] \n",
    "\n",
    "def add_is_holiday(data):\n",
    "    # merge dataframes        \n",
    "    data = data.merge(df_di, on='visit_date', how='left')\n",
    "    del data['day_of_week']\n",
    "    return data\n",
    "\n",
    "df_train = add_is_holiday(df_train)\n",
    "df_test = add_is_holiday(df_test)\n",
    "\n",
    "# =============================================================================\n",
    "# drop date-time-hour info\n",
    "# =============================================================================\n",
    "def drop_datetime_info(data):\n",
    "    del data['visit_date'], data['visit_datetime'], data['reserve_datetime'], data['visit_datetime__'], data['reserve_datetime__']\n",
    "#    del data['visit_date'], data['visit_datetime'], data['reserve_datetime']\n",
    "    return data\n",
    "df_train = drop_datetime_info(df_train)\n",
    "\n",
    "def drop_datetime_info(data):\n",
    "    del data['visit_date'], data['visit_datetime'], data['reserve_datetime'], data['visit_datetime__'], data['reserve_datetime__']\n",
    "#    del data['visit_date'], data['visit_datetime'], data['reserve_datetime']\n",
    "    return data\n",
    "df_test = drop_datetime_info(df_test)\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# autoclean\n",
    "# =============================================================================\n",
    "#df_train_clean = autoclean(df_train)\n",
    "#df_test_clean = autoclean(df_test)\n",
    "#\n",
    "train = df_train.fillna(-1)\n",
    "test = df_test.fillna(-1)\n",
    "#\n",
    "# =============================================================================\n",
    "# shuffle dataset\n",
    "# =============================================================================\n",
    "from sklearn.utils import shuffle\n",
    "train =  shuffle(train, random_state=21)\n",
    "\n",
    "\n",
    "X_train, X_valid = train_test_split(train, test_size=0.05, random_state=42, shuffle=False)\n",
    "\n",
    "X = X_train.drop(['visitors'], axis=1)\n",
    "y = np.log1p(X_train['visitors'].values)\n",
    "d_train = lgb.Dataset(X, y)\n",
    "\n",
    "X = X_valid.drop(['visitors'], axis=1)\n",
    "y = np.log1p(X_valid['visitors'].values)\n",
    "d_valid = lgb.Dataset(X, y)\n",
    "\n",
    "watchlist = [d_train, d_valid]\n",
    "\n",
    "print('Training LGBM model...')\n",
    "params = {}\n",
    "params['application'] = 'regression'\n",
    "params['boosting'] = 'gbdt'\n",
    "params['learning_rate'] = 0.01\n",
    "params['num_leaves'] = 32\n",
    "params['min_sum_hessian_in_leaf'] = 1e-2\n",
    "params['min_gain_to_split'] = 0\n",
    "\n",
    "params['bagging_fraction'] = 0.8\n",
    "params['feature_fraction'] = 0.8\n",
    "params['num_threads'] = 4\n",
    "params['metric'] = 'rmse'\n",
    "\n",
    "lgb_model1 = lgb.train(params, train_set=d_train, num_boost_round=50000, valid_sets=watchlist, \\\n",
    "verbose_eval=10)\n",
    "\n",
    "test_probs = lgb_model1.predict(test)\n",
    "test_probs = np.expm1(test_probs)\n",
    "\n",
    "result = pd.DataFrame({\"id\": index_test, \"visitors\": test_probs})\n",
    "    \n",
    "result.to_csv('LGB_sub.csv', index=False)\n",
    "    \n",
    "    # gbm.save_model(r\"..\\output\\models\\LGB_\"+str(file_name)+'.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-711c74f5b217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
