{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YieFan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, I would say the following need to be followed in a very often iterative manners:\n",
    "\n",
    "- Understand problem definition\n",
    "- Understand the evaluation metric in the context of the problem. \n",
    "- Understand how public and private leaderboard are split \n",
    "- Initial Data exploration, establish initial understand the nature of dataset\n",
    "- Build first batch of models, create first batch of submissions\n",
    "- Create first cross-validation scheme, and start evaluating relationship between cross-validation and leaderboard score\n",
    "- Start designing experiment for rigorous cross-validation, this shall include both the methodology of cross-validation, the actual implementation, a data capturing mechanism for experiment data\n",
    "- In-depth data manipulation for feature engineering, \n",
    "- For deep learning dominated approaches, design and evaluate model architecture\n",
    "- use established cross-validation method to decide the effectiveness of features and model architecture \n",
    "- Apply ensemble methods - this can range from simple weigh averaging to stacking\n",
    "- post-process of prediction from machine learning model. this is optional depends on specific problem and evaluation metric.\n",
    "\n",
    "\n",
    "[8:17] \n",
    "Additionally, the following shall be strongly encouraged throughout the competition:\n",
    "- Pay close attention to what others are sharing on forum and competition specific kernels \n",
    "- Review and check past competition top solutions - especially the ones with similar problems and similar evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "Generally speaking, I would say the following need to be followed in a very often iterative manners:\n",
    "\n",
    "- Understand problem definition\n",
    "- Understand the evaluation metric in the context of the problem. \n",
    "- Understand how public and private leaderboard are split \n",
    "- Initial Data exploration, establish initial understand the nature of dataset\n",
    "- Build first batch of models, create first batch of submissions\n",
    "- Create first cross-validation scheme, and start evaluating relationship between cross-validation and leaderboard score\n",
    "- Start designing experiment for rigorous cross-validation, this shall include both the methodology of cross-validation, the actual implementation, a data capturing mechanism for experiment data\n",
    "- In-depth data manipulation for feature engineering, \n",
    "- For deep learning dominated approaches, design and evaluate model architecture\n",
    "- use established cross-validation method to decide the effectiveness of features and model architecture \n",
    "- Apply ensemble methods - this can range from simple weigh averaging to stacking\n",
    "- post-process of prediction from machine learning model. this is optional depends on specific problem and evaluation metric.\n",
    "\n",
    "\n",
    "[8:17] \n",
    "Additionally, the following shall be strongly encouraged throughout the competition:\n",
    "- Pay close attention to what others are sharing on forum and competition specific kernels \n",
    "- Review and check past competition top solutions - especially the ones with similar problems and similar evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "for LightGBM, and also if you want to try out other algorithms without going through the pain of the installation, I fully recommend you to use the kernel facility that is available in each competition.\n",
    "\n",
    "\n",
    "[11:20] \n",
    "you just need to create your scrip on Kaggle, and most of the libraries are there, I think you can even save your prediction and submit it on there\n",
    "\n",
    "\n",
    "[11:25] \n",
    "Soon, I will also use the Kaggle Kernel to share some of my code with you, for instance there are more useful techniques than one hot encoding to treat categorical variables - I will start working on this today :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "for more reading on how to do feature engineering, you can have a look at this https://www.slideshare.net/HJvanVeen/feature-engineering-72376750\n",
    "slideshare.net\n",
    "Feature Engineering\n",
    "Tips & Tricks for Feature Engineering / Applied Machine Learning. This presentation was given as part of the Sao Paulo ML meetup.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    " Yifan Xie\n",
    "@here #porto-seguro-a  #porto-seguro-b  #porto-seguro-c I have created a kernel in the competition's Kernel facility to demonstrate the following techniques:\n",
    "1. Capturing statistic overview of the features\n",
    "2. Categorical encoding - particularly useful for this competition\n",
    "3. Model training and prediction with Sklearn for Random Forest and Logistic Regression.\n",
    "\n",
    "The link is here https://www.kaggle.com/yifanxie/porto-seguro-tutorial-simple-e2e-pipeline\n",
    "Posted in #ai-kaggle-porto-segurNov 23rd at 7:56 AM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "For this competition, the most important aspect is cross validation - in fact, the next topic I would like to share is probably exactly this,\n",
    "\n",
    "* * *\n",
    "here is my understanding: Cross validation is a method of model selection - hint: in sklearn, you do `from sklearn.model_selection import StratifiedKFold` :slightly_smiling_face:\n",
    "\n",
    "why do we call that model selection? because when you are using a tool like Xgboost or any of the sklearn apis (i.e. RandomForest), you are using IMPLEMENTATION of machine learning model algorithm to generate models. And every time you use different set of parameters (i.e. different values for `max_depth`)  you are generating different model.\n",
    "\n",
    "The point here about CV and model selection, is that by doing Cross Validation, you are validating if the model you are going to generate with some parameters is \"good\". \n",
    "\n",
    "What we mean by \"good\" is interesting: let's say we want our model to strike the right balance between bias (underfitting) and variance (overfitting) - and most importantly, you want your model CV performance to be well correlated with the modeling performance with UNSEEN data\n",
    "\n",
    "\n",
    "\n",
    "[6:31] \n",
    "So, in  a Kaggle competition, this requirement of \"well correlation\" is simulated in the following way:\n",
    "- 1) you want your CV score on the train data to be well correlated with the public LB data - as much as possible, and sometime it is difficult if you have a noisy dataset - like this one in Porto\n",
    "- 2) you NEED your CV score to be well correlated with private LB data - this is the UNSEEN data (edited)\n",
    "\n",
    "\n",
    "[6:33] \n",
    "here is the bottom line: you need a good CV strategy - and a good CV strategy is one that can simulate as closely as possible how the public LB and private LB is split - this is the BIGGEST hint I can give you :grinning:\n",
    "\n",
    "\n",
    "[6:37] \n",
    "@wol_tar how about this, how about you do a small experiment, with the basic XGB classifier you tried, can you try to run it with Kfold 5 for several time, each time with different random seed?\n",
    "\n",
    "\n",
    "[6:37] \n",
    "can you then let me know, what is the average of your Kfold 5 CV between different run, and the standard deviation?\n",
    "\n",
    "\n",
    "[6:39] \n",
    "Also, within each Kfold, you have 5 in-fold CV score - the CV score for each validation fold - can you check the standard deviation between each fold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
