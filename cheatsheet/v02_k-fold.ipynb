{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../../mltestdata/03_predict_hourly_wage/Income_training.csv\")\n",
    "test = pd.read_csv(\"../../../mltestdata/03_predict_hourly_wage/Income_testing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessiong\n",
    "* * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠損値も特にないため、省略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交差検証いろいろ\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train.compositeHourlyWages.values\n",
    "\n",
    "test_ID = test['ID']\n",
    "test.drop(\"ID\", axis = 1, inplace = True)\n",
    "\n",
    "train.drop(['compositeHourlyWages'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train,train_target,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Validation with local train\n",
    "ホールドアウトして（train_test_splitを使ってx_trainなどを作る）、trainとtestを使ってから評価するケース。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgboostモデルの作成\n",
    "reg = xgb.XGBRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# パラメータを設定したい場合は下記のように設定も可能\n",
    "#ind_params = {'objective': 'reg:linear'}\n",
    "#reg = xgb.XGBRegressor(**ind_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainprediction = reg.predict(X_train)\n",
    "y_testprediction = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE from local train:  6.02475299555\n",
      "MSE from local train:  36.2976486573\n",
      "R2 from local train:  0.395235324935\n"
     ]
    }
   ],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "print(\"RMSE from local train: \", rmse(y_test, y_testprediction))\n",
    "print(\"MSE from local train: \", mean_squared_error(y_test, y_testprediction))\n",
    "print(\"R2 from local train: \", r2_score(y_test, y_testprediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "**Pros**\n",
    "- 簡単。\n",
    "\n",
    "**Cons**\n",
    "- 8対2で分けているだけなので、データの偏りがある場合評価は低いことが予想される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Validation with GridSearch\n",
    "実はGridsearchする際にも交差検証が実施できる。一挙両得で便利。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboostモデルの作成\n",
    "xgr = xgb.XGBRegressor()\n",
    "\n",
    "# XGBoost params\n",
    "xgb_params = {\n",
    "    'objective' : ['reg:gamma','reg:linear'],\n",
    "    'learning_rate' : [0.05,0.1],\n",
    "    'n_estimators' : [50,100,200],\n",
    "    'max_depth' : [2,4,6],\n",
    "    'subsample' : [0.8,0.85,0.9,0.95],\n",
    "    'colsample_bytree' : [0.5,1.0],\n",
    "    'min_child_weight' : [5,10,15]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'objective': ['reg:gamma', 'reg:linear'], 'learning_rate': [0.05, 0.1], 'n_estimators': [50, 100, 200], 'max_depth': [2, 4, 6], 'subsample': [0.8, 0.85, 0.9, 0.95], 'colsample_bytree': [0.5, 1.0], 'min_child_weight': [5, 10, 15]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scoring_func = make_scorer(mean_squared_error)\n",
    "#grid = GridSearchCV(xgr,xgb_params,scoring=scoring_func,cv=5,n_jobs=-1)\n",
    "\n",
    "grid = GridSearchCV(xgr,xgb_params,scoring='neg_mean_squared_error',cv=5,n_jobs=-1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 2, 'min_child_weight': 15, 'n_estimators': 100, 'objective': 'reg:gamma', 'subsample': 0.8}\n",
      "Best Estimator:  XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1.0, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=2, min_child_weight=15, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='reg:gamma', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.8)\n",
      "MSE:  -38.8108460369\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", grid.best_params_)\n",
    "print(\"Best Estimator: \", grid.best_estimator_)\n",
    "print(\"MSE: \", grid.best_score_)\n",
    "\n",
    "# best_score_ : Mean cross-validated score of the best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result of combination\n",
    "#index = 1\n",
    "#for params, mean_score, scores in grid.cv_results_:\n",
    "#    print(\"%d) %0.3f (+/-%0.03f) \" % (index, mean_score, scores.std() / 2))\n",
    "#    print(\"Params: %r\" % params)\n",
    "#    print(\"_\"*30)\n",
    "#    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改めて最適パラメータで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.95, eta=0.1, eval_metric='auc', gamma=0.7,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=10, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=6, objective='reg:gamma', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=0, silent=1, subsample=0.76,\n",
       "       tree_method='hist')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gdm = xgb.XGBRegressor(**grid.best_params_) でもよし\n",
    "\n",
    "xgb_params = {\n",
    "    \"booster\"  :  \"gbtree\", \n",
    "#    \"objective\"         :  \"binary:logistic\",\n",
    "    \"objective\"         :  \"reg:gamma\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_child_weight\": 10,\n",
    "    \"gamma\": 0.70,\n",
    "    \"subsample\": 0.76,\n",
    "    \"colsample_bytree\": 0.95,\n",
    "    \"nthread\": 6,\n",
    "    \"seed\": 0,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "xgr = xgb.XGBRegressor(**xgb_params) \n",
    "xgr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_testprediction = xgr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE from local train:  6.00850970197\n",
      "MSE from local train:  36.1021888387\n",
      "R2 from local train:  0.398491932404\n"
     ]
    }
   ],
   "source": [
    "# The error metric: RMSE\n",
    "print(\"RMSE from local train: \", rmse(y_test, y_testprediction))\n",
    "print(\"MSE from local train: \", mean_squared_error(y_test, y_testprediction))\n",
    "print(\"R2 from local train: \", r2_score(y_test, y_testprediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "**Pros**\n",
    "- GridsearchしながらCVもできて簡単。\n",
    "\n",
    "**Cons**\n",
    "- だがCVするために、いつもGridsearchしないといけないので、使うのはGridsearchする時の1回目くらいでしょうか。CVは別途行うので、cv=xの数を初めから小さく設定しておくか。とりあえず最初からcv=5で行くのが良さそう。\n",
    "- CVするから時間がかかる。。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Validation with sklearn cv (normal k-fold)\n",
    "sklearnにcross_val_scoreがあるのでライブラリを初めから使ってしまう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -38.69 (+/- 8.68)\n",
      "Fold    0: MSE= -42.1454030379\n",
      "Fold    1: MSE= -37.9734614077\n",
      "Fold    2: MSE= -33.1404770954\n",
      "Fold    3: MSE= -44.9406937328\n",
      "Fold    4: MSE= -35.2493473437\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(xgr, train, train_target, scoring = \"neg_mean_squared_error\",cv=5)\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    print('Fold %4d: MSE= %s'% (i, score))\n",
    "    \n",
    "# Scoring types: http://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "**Pros**\n",
    "- コードが短くて見やすいし、簡単にCVができて良い。\n",
    "- KfoldやStratified Kfoldなども考慮されている\n",
    "> [For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used.](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)。\n",
    "\n",
    "**Cons**\n",
    "- StackingでOut-of-foldの生成をすることができない。各foldごとのモデルを保存・予測が必要で、cross_val_scoreでは学習させたモデルを触ることができないため、利用できない。\n",
    "- Scoringの種類が限らていて、rmsleとかはできない？。よって、あとで紹介する自分でmetrics.KFoldなど使わう方法でないと無理そう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Validation with sklearn cv (Stratified k-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suzukiry/lab/March/lib/python3.6/site-packages/sklearn/model_selection/_split.py:597: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -33.20 (+/- 17.66)\n",
      "Stratified-Fold    0: MSE= -50.7753717825\n",
      "Stratified-Fold    1: MSE= -27.6839607022\n",
      "Stratified-Fold    2: MSE= -28.770991798\n",
      "Stratified-Fold    3: MSE= -28.4326640183\n",
      "Stratified-Fold    4: MSE= -30.3428421976\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "\n",
    "scores = cross_val_score(gdm, train, train_target, scoring = \"neg_mean_squared_error\",cv=skf)\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    print('Stratified-Fold %4d: MSE= %s'% (i, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "**3. Validation with sklearn cv (normal k-fold)** と同じ。\n",
    "\n",
    "※Stratified Kfoldは主に**分類**か、**yが整数またはクラスタ分類** 時に使うべき、な気がする。\n",
    "\n",
    "> [For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used.](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Validation without cross_val_score\n",
    "cross_val_scoreを使わないで自分でcvのコードを各方法。先人の知恵を拝借。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1. （参考）YifanのCV用関数**\n",
    "\n",
    "**5.1.1. sklearn K-fold & OOF function**\n",
    "\n",
    "CV用の関数がかっこよくまとまっていたので、参考に。そもそもStackingのOut-of-foldを作るように作成されていたのですた、簡単にCVスコアも見れるのでこちらをこれから使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "\n",
    "### Kfold or Stratified Kfold\n",
    "kf=KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "#kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "\n",
    "### Criterion function\n",
    "def auc_to_gini_norm(auc_score):\n",
    "    return 2*auc_score-1\n",
    "\n",
    "def eval_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def eval_rmsle(y, pred):\n",
    "    return mean_squared_error(y, pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modified cross-validation function copied from Yifan's cool kernel. \n",
    "### Source: https://www.kaggle.com/yifanxie/porto-seguro-tutorial-end-to-end-ensemble\n",
    "\n",
    "def cross_validate_sklearn(clf, x_train, y_train, x_test, kf, scale=False, verbose=True):\n",
    "    start_time=time.time()\n",
    "    \n",
    "    # initialise the size of out-of-fold train an test prediction\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "\n",
    "    # use the kfold object to generate the required folds\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        # generate training folds and validation fold\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[test_index, :]\n",
    "        y_train_kf, y_val_kf = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        # perform scaling if required i.e. for linear algorithms\n",
    "        if scale:\n",
    "            scaler = StandardScaler().fit(x_train_kf.values)\n",
    "            x_train_kf_values = scaler.transform(x_train_kf.values)\n",
    "            x_val_kf_values = scaler.transform(x_val_kf.values)\n",
    "            x_test_values = scaler.transform(x_test.values)\n",
    "        else:\n",
    "            x_train_kf_values = x_train_kf.values\n",
    "            x_val_kf_values = x_val_kf.values\n",
    "            x_test_values = x_test.values\n",
    "        \n",
    "        ###################################################\n",
    "        # Please update for your own criterion \n",
    "        ###################################################\n",
    "        # fit the input classifier and perform prediction.\n",
    "        #clf.fit(x_train_kf_values, y_train_kf.values)\n",
    "        clf.fit(x_train_kf_values, np.log1p(y_train_kf.values))\n",
    "\n",
    "        ###################################################\n",
    "        # Please update for your own criterion \n",
    "        ###################################################\n",
    "        #val_pred=clf.predict_proba(x_val_kf_values)[:,1]\n",
    "        val_pred=clf.predict(x_val_kf_values)\n",
    "\n",
    "        train_pred[test_index] += val_pred\n",
    "\n",
    "        ###################################################\n",
    "        # Please update for your own criterion \n",
    "        ###################################################\n",
    "        #y_test_preds = clf.predict_proba(x_test_values)[:,1]\n",
    "        y_test_preds = clf.predict(x_test_values)\n",
    "        test_pred += y_test_preds\n",
    "\n",
    "        ###################################################\n",
    "        # Please update for your own criterion \n",
    "        ###################################################\n",
    "        #fold_auc = roc_auc_score(y_val_kf.values, val_pred)\n",
    "        #fold_gini_norm = auc_to_gini_norm(fold_auc)\n",
    "        fold_rmsle = eval_rmsle(np.log1p(y_val_kf.values), val_pred)\n",
    "\n",
    "        if verbose:\n",
    "            ###################################################\n",
    "            # Please update for your own criterion\n",
    "            ###################################################\n",
    "\n",
    "            #print('fold cv {} AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(i, fold_auc, fold_gini_norm))\n",
    "            print('fold cv {} RMSLE score is {:.6f}'.format(i, fold_rmsle))\n",
    "\n",
    "    test_pred /= kf.n_splits\n",
    "\n",
    "    ###################################################\n",
    "    # Please update for your own criterion\n",
    "    ###################################################\n",
    "    #cv_auc = roc_auc_score(y_train, train_pred)\n",
    "    #cv_gini_norm = auc_to_gini_norm(cv_auc)\n",
    "    cv_rmsle = eval_rmsle(np.log1p(y_train), train_pred)\n",
    "        \n",
    "    #cv_score = [cv_auc, cv_gini_norm]\n",
    "    cv_score = [cv_rmsle]\n",
    "    if verbose:\n",
    "\n",
    "        ###################################################\n",
    "        # Please update for your own criterion\n",
    "        ###################################################\n",
    "        #print('cv AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(cv_auc, cv_gini_norm))\n",
    "        print('cv RMSLE score is {:.6f}'.format(cv_rmsle))\n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    "    return cv_score, train_pred, test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(criterion='mse',\n",
    "                            max_depth=5,\n",
    "                            n_estimators=500,\n",
    "                            min_samples_leaf=5,\n",
    "                            min_samples_split=2\n",
    "                           )\n",
    "\n",
    "outcomes =cross_validate_sklearn(rfr, x_train, y_train ,x_test, kf, scale=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1.2. Xgboost K-fold & OOF function**\n",
    "\n",
    "Work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_to_rank(prediction, scaler=1):\n",
    "    pred_df=pd.DataFrame(columns=['probability'])\n",
    "    pred_df['probability']=prediction\n",
    "    pred_df['rank']=pred_df['probability'].rank()/len(prediction)*scaler\n",
    "    return pred_df['rank'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_xgb(params, x_train, y_train, x_test, kf, cat_cols=[], verbose=True, \n",
    "                       verbose_eval=50, num_boost_round=4000, use_rank=True):\n",
    "    start_time=time.time()\n",
    "\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "\n",
    "    # use the k-fold object to enumerate indexes for each training and validation fold\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)): # folds 1, 2 ,3 ,4, 5\n",
    "        # example: training from 1,2,3,4; validation from 5\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "        y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n",
    "        x_test_kf=x_test.copy()\n",
    "\n",
    "        d_train_kf = xgb.DMatrix(x_train_kf, label=y_train_kf)\n",
    "        d_val_kf = xgb.DMatrix(x_val_kf, label=y_val_kf)\n",
    "        d_test = xgb.DMatrix(x_test_kf)\n",
    "\n",
    "        bst = xgb.train(params, d_train_kf, num_boost_round=num_boost_round,\n",
    "                        evals=[(d_train_kf, 'train'), (d_val_kf, 'val')], verbose_eval=verbose_eval,\n",
    "                        early_stopping_rounds=50)\n",
    "\n",
    "        val_pred = bst.predict(d_val_kf, ntree_limit=bst.best_ntree_limit)\n",
    "        if use_rank:\n",
    "            train_pred[val_index] += probability_to_rank(val_pred)\n",
    "            test_pred+=probability_to_rank(bst.predict(d_test))\n",
    "        else:\n",
    "            train_pred[val_index] += val_pred\n",
    "            test_pred+=bst.predict(d_test)\n",
    "\n",
    "        fold_auc = roc_auc_score(y_val_kf.values, val_pred)\n",
    "        fold_gini_norm = auc_to_gini_norm(fold_auc)\n",
    "\n",
    "        if verbose:\n",
    "            print('fold cv {} AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(i, fold_auc, \n",
    "                                                                                     fold_gini_norm))\n",
    "\n",
    "    test_pred /= kf.n_splits\n",
    "\n",
    "    cv_auc = roc_auc_score(y_train, train_pred)\n",
    "    cv_gini_norm = auc_to_gini_norm(cv_auc)\n",
    "    cv_score = [cv_auc, cv_gini_norm]\n",
    "    if verbose:\n",
    "        print('cv AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(cv_auc, cv_gini_norm))\n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    "\n",
    "        return cv_score, train_pred,test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1.3 LigthGBM K-fold & OOF function**\n",
    "\n",
    "Work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_lgb(params, x_train, y_train, x_test, kf, cat_cols=[],\n",
    "                       verbose=True, verbose_eval=50, use_cat=True, use_rank=True):\n",
    "    start_time = time.time()\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "\n",
    "    if len(cat_cols)==0: use_cat=False\n",
    "\n",
    "    # use the k-fold object to enumerate indexes for each training and validation fold\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)): # folds 1, 2 ,3 ,4, 5\n",
    "        # example: training from 1,2,3,4; validation from 5\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "        y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        if use_cat:\n",
    "            lgb_train = lgb.Dataset(x_train_kf, y_train_kf, categorical_feature=cat_cols)\n",
    "            lgb_val = lgb.Dataset(x_val_kf, y_val_kf, reference=lgb_train, categorical_feature=cat_cols)\n",
    "        else:\n",
    "            lgb_train = lgb.Dataset(x_train_kf, y_train_kf)\n",
    "            lgb_val = lgb.Dataset(x_val_kf, y_val_kf, reference=lgb_train)\n",
    "\n",
    "        gbm = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=4000,\n",
    "                        valid_sets=lgb_val,\n",
    "                        early_stopping_rounds=30,\n",
    "                        verbose_eval=verbose_eval)\n",
    "\n",
    "        val_pred = gbm.predict(x_val_kf)\n",
    "\n",
    "        if use_rank:\n",
    "            train_pred[val_index] += probability_to_rank(val_pred)\n",
    "            test_pred += probability_to_rank(gbm.predict(x_test))\n",
    "            # test_pred += gbm.predict(x_test)\n",
    "        else:\n",
    "            train_pred[val_index] += val_pred\n",
    "            test_pred += gbm.predict(x_test)\n",
    "\n",
    "        # test_pred += gbm.predict(x_test)\n",
    "        fold_auc = roc_auc_score(y_val_kf.values, val_pred)\n",
    "        fold_gini_norm = auc_to_gini_norm(fold_auc)\n",
    "        if verbose:\n",
    "            print('fold cv {} AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(i, fold_auc, fold_gini_norm))\n",
    "\n",
    "    test_pred /= kf.n_splits\n",
    "\n",
    "    cv_auc = roc_auc_score(y_train, train_pred)\n",
    "    cv_gini_norm = auc_to_gini_norm(cv_auc)\n",
    "    cv_score = [cv_auc, cv_gini_norm]\n",
    "    if verbose:\n",
    "        print('cv AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(cv_auc, cv_gini_norm))\n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    "    return cv_score, train_pred,test_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful parameters which will come in handy later on\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "SEED = 0 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 RMSE score is 6.464016\n",
      "\n",
      "fold cv 1 RMSE score is 6.153830\n",
      "\n",
      "fold cv 2 RMSE score is 5.701085\n",
      "\n",
      "fold cv 3 RMSE score is 6.676302\n",
      "\n",
      "fold cv 4 RMSE score is 5.910690\n",
      "\n",
      "cv RMSE score is 6.181 +/- 0.355\n",
      "\n",
      "it takes 0.174 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=NFOLDS, random_state=SEED)\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "# initialise the size of out-of-fold train an test prediction\n",
    "scores = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train, train_target)):\n",
    "\n",
    "    x_train_kf, x_val_kf = train.loc[train_index, :], train.loc[test_index, :]\n",
    "    y_train_kf, y_val_kf = train_target[train_index], train_target[test_index]\n",
    " \n",
    "    # Make data as numpy\n",
    "    x_train_kf_values = x_train_kf.values\n",
    "    x_val_kf_values = x_val_kf.values\n",
    "\n",
    "    x_test_values = test.values\n",
    "\n",
    "    # fit the input classifier and perform prediction.\n",
    "    gdm.fit(x_train_kf_values, y_train_kf)\n",
    "\n",
    "    y_val_kf_pred = gdm.predict(x_val_kf_values)\n",
    "    \n",
    "    fold_rmse = eval_rmse(y_val_kf, y_val_kf_pred)\n",
    "    print('fold cv {} RMSE score is {:.6f}\\n'.format(i, fold_rmse))\n",
    "    scores.append(fold_rmse)\n",
    "\n",
    "print('cv RMSE score is {:.3f} +/- {:.3f}'.format(np.mean(scores),np.std(scores)))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"\\nit takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Stratified K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suzukiry/lab/March/lib/python3.6/site-packages/sklearn/model_selection/_split.py:597: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 RMSE score is 7.125684\n",
      "\n",
      "fold cv 1 RMSE score is 5.261555\n",
      "\n",
      "fold cv 2 RMSE score is 5.363860\n",
      "\n",
      "fold cv 3 RMSE score is 5.332229\n",
      "\n",
      "fold cv 4 RMSE score is 5.508434\n",
      "\n",
      "cv RMSE score is 5.718 +/- 0.708\n",
      "\n",
      "it takes 0.392 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "# initialise the size of out-of-fold train an test prediction\n",
    "scores = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(train, train_target)):\n",
    "\n",
    "    x_train_kf, x_val_kf = train.loc[train_index, :], train.loc[test_index, :]\n",
    "    y_train_kf, y_val_kf = train_target[train_index], train_target[test_index]\n",
    " \n",
    "    # Make data as numpy\n",
    "    x_train_kf_values = x_train_kf.values\n",
    "    x_val_kf_values = x_val_kf.values\n",
    "\n",
    "    x_test_values = test.values\n",
    "\n",
    "    # fit the input classifier and perform prediction.\n",
    "    gdm.fit(x_train_kf_values, y_train_kf)\n",
    "\n",
    "    y_val_kf_pred = gdm.predict(x_val_kf_values)\n",
    "    \n",
    "    fold_rmse = eval_rmse(y_val_kf, y_val_kf_pred)\n",
    "    print('fold cv {} RMSE score is {:.6f}\\n'.format(i, fold_rmse))\n",
    "    scores.append(fold_rmse)\n",
    "\n",
    "print('cv RMSE score is {:.3f} +/- {:.3f}\\n'.format(np.mean(scores),np.std(scores)))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gdm.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['ID'] = test_ID\n",
    "sub['compositeHourlyWages'] = predictions\n",
    "sub.to_csv('rs_hourly_submission_24-Dec-2017_v02.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * * * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAEICAYAAAA3PAFIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOEexOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10aeed7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feature importance のプロット\n",
    "importances = pd.Series(xgr.feature_importances_, index = train.columns)\n",
    "importances = importances.sort_values()\n",
    "importances.plot(kind = \"barh\")\n",
    "plt.title(\"imporance in the xgboost Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFoldの使い方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [model_selection](http://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html)\n",
    "- [cross_validation](http://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- [model_evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=4, random_state=None, shuffle=False)\n",
      "TRAIN: [2 3 4 5 6 7] TEST: [0 1]\n",
      "TRAIN: 6 TEST: 2\n",
      "TRAIN: [0 1 4 5 6 7] TEST: [2 3]\n",
      "TRAIN: 6 TEST: 2\n",
      "TRAIN: [0 1 2 3 6 7] TEST: [4 5]\n",
      "TRAIN: 6 TEST: 2\n",
      "TRAIN: [0 1 2 3 4 5] TEST: [6 7]\n",
      "TRAIN: 6 TEST: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4],[3, 4], [7, 8], [6, 7], [7, 4]])\n",
    "y = np.array([0, 0, 1, 1, 0, 0, 1, 0])\n",
    "kf = KFold(n_splits=4)\n",
    "\n",
    "print(kf)  \n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "   print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "   print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "   X_train, X_test = X[train_index], X[test_index]\n",
    "   y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StratifiedKFoldの使い方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=4, random_state=None, shuffle=False)\n",
      "TRAIN: [3 4 5 6 7] TEST: [0 1 2]\n",
      "TRAIN: 5 TEST: 3\n",
      "TRAIN: [0 1 2 5 6 7] TEST: [3 4]\n",
      "TRAIN: 6 TEST: 2\n",
      "TRAIN: [0 1 2 3 4 7] TEST: [5 6]\n",
      "TRAIN: 6 TEST: 2\n",
      "TRAIN: [0 1 2 3 4 5 6] TEST: [7]\n",
      "TRAIN: 7 TEST: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suzukiry/lab/March/lib/python3.6/site-packages/sklearn/model_selection/_split.py:597: Warning: The least populated class in y has only 3 members, which is too few. The minimum number of members in any class cannot be less than n_splits=4.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4],[3, 4], [7, 8], [6, 7], [7, 4]])\n",
    "y = np.array([0, 0, 1, 1, 0, 0, 1, 0])\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "print(skf)  \n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "   print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "   print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "   X_train, X_test = X[train_index], X[test_index]\n",
    "   y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの.scoreとmodel_selection.cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "svc = svm.SVC(C=1, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97999999999999998"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_digits[:-100], y_digits[:-100]).score(X_digits[-100:], y_digits[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_folds = np.array_split(X_digits, 3)\n",
    "y_folds = np.array_split(y_digits, 3)\n",
    "scores = list()\n",
    "for k in range(3):\n",
    "    # We use 'list' to copy, in order to 'pop' later on\n",
    "    X_train = list(X_folds)\n",
    "    X_test  = X_train.pop(k)\n",
    "    X_train = np.concatenate(X_train)\n",
    "    y_train = list(y_folds)\n",
    "    y_test  = y_train.pop(k)\n",
    "    y_train = np.concatenate(y_train)\n",
    "    scores.append(svc.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [2 3 4 5] | test: [0 1]\n",
      "Train: [0 1 4 5] | test: [2 3]\n",
      "Train: [0 1 2 3] | test: [4 5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "X = [\"a\", \"a\", \"b\", \"c\", \"c\", \"c\"]\n",
    "k_fold = KFold(n_splits=3)\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "     print('Train: %s | test: %s' % (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .scoreで出す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test]) \n",
    " for train, test in k_fold.split(X_digits)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross_val_scoreで出す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93489149,  0.95659432,  0.93989983])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(svc, X_digits, y_digits, cv=k_fold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93969761,  0.95911415,  0.94041254])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(svc, X_digits, y_digits, cv=k_fold,\n",
    "                scoring='precision_macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
