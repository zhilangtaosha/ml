{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import time\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contributions from:\n",
    "DSEverything - Mean Mix - Math, Geo, Harmonic (LB 0.493) \n",
    "https://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493\n",
    "JdPaletto - Surprised Yet? - Part2 - (LB: 0.503)\n",
    "https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503\n",
    "hklee - weighted mean comparisons, LB 0.497, 1ST\n",
    "https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st\n",
    "\n",
    "Also all comments for changes, encouragement, and forked scripts rock\n",
    "\n",
    "Keep the Surprise Going\n",
    "\"\"\"\n",
    "\n",
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../../mltestdata/05_recruit/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../../mltestdata/05_recruit/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../../mltestdata/05_recruit/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../../mltestdata/05_recruit/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../../mltestdata/05_recruit/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../../mltestdata/05_recruit/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../../mltestdata/05_recruit/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../../mltestdata/05_recruit/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "    }\n",
    "\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "#sure it can be compressed...\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n",
    "\n",
    "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n",
    "\n",
    "# NEW FEATURES FROM JMBULL\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "\n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for LightGBM ...\n"
     ]
    }
   ],
   "source": [
    "print( \"\\nProcessing data for LightGBM ...\" )\n",
    "for c, dtype in zip(train.columns, train.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        train[c] = train[c].astype(np.float32)\n",
    "\n",
    "for c, dtype in zip(test.columns, test.dtypes):\t\n",
    "    if dtype == np.float64:\t\t\n",
    "        test[c] = test[c].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "y_train=train['visitors']\n",
    "x_train=train.drop(drop_cols, axis=1)\n",
    "\n",
    "x_test=test.copy()\n",
    "x_test=x_test.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.visitors\n",
    "train_input = train.copy()\n",
    "test_input = test.copy()\n",
    "\n",
    "drop_cols=['visitors','air_store_id','visit_date','id']\n",
    "train_input=train_input.drop(drop_cols, axis=1)\n",
    "test_input=test_input.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "localtrain, localval = train_test_split(train,test_size=0.3,random_state=2018)\n",
    "\n",
    "y_localtrain=localtrain['visitors']\n",
    "x_localtrain=localtrain.drop(drop_cols, axis=1)\n",
    "\n",
    "y_localval=localval['visitors']\n",
    "x_localval=localval.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation function\n",
    "\n",
    "def eval_rmsle(preds, true):\n",
    "    eval_rmsle = (np.sum((np.log1p(preds)-np.log1p(true))**2)/len(true))**0.5\n",
    "    return eval_rmsle\n",
    "\n",
    "#def eval_rmsle(y, pred):\n",
    "#    return mean_squared_error(y, pred)**0.5\n",
    "\n",
    "def auc_to_gini_norm(auc_score):\n",
    "    return 2*auc_score-1\n",
    "\n",
    "def eval_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a evaluation matrix \n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "RMSLE = make_scorer(eval_rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for comparing predictions and true data.\n",
    "def compare_result(preds, true):\n",
    "    compare = pd.DataFrame({\"test_id\": true.index,\n",
    "                           \"real_cost\": true,\n",
    "                           \"pred_cost\": preds})\n",
    "    compare = compare[[\"test_id\", \"real_cost\", \"pred_cost\"]].reset_index(drop=True)\n",
    "    \n",
    "    compare[\"error_percent_(%)\"] = np.abs(compare.real_cost - compare.pred_cost) / compare.real_cost * 100\n",
    "    \n",
    "    return compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM Regression and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# model_selection.GridSearchCV\n",
    "# =================================================\n",
    "\n",
    "# 1) Create model\n",
    "#set static parameters\n",
    "params = {'boosting_type': 'dart',\n",
    "          'max_depth' : 5,\n",
    "          'max_bin' : 500,\n",
    "          'learning_rate': 0.1,  # 0.618580\n",
    "          'num_leaves': 22,\n",
    "         }\n",
    "\n",
    "#define XGB Grid Search model\n",
    "model = LGBMRegressor(boosting_type=params['boosting_type']\n",
    "                       )\n",
    "#set parameters\n",
    "gridParams = {\n",
    "    'colsample_bytree' :[0.9, 1.0], #0.6, 0.64, 0.85 /0.8, \n",
    "    'learning_rate': [0.075,0.1], # ,0.005, 0.06/0.05,\n",
    "    'max_depth' :[2,5,10], #2,3,4\n",
    "    'n_estimators': [8,24], #,100\n",
    "    #'num_leaves': [10,20], #2,4,5,6, 100\n",
    "    'objective': ['binary','regression'],\n",
    "    'reg_alpha' : [0.1,0.5], #,1.2\n",
    "    'reg_lambda' : [1.3,1.5], #1, 1.2,1.4\n",
    "    'subsample' :[0.7,0.9], #0.75 \n",
    "}\n",
    "\n",
    "# 3) Run GridSearch\n",
    "grid = GridSearchCV(model, gridParams, verbose=1, cv=5, n_jobs=-1, scoring=RMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
     ]
    }
   ],
   "source": [
    "#grid search experiment\n",
    "start = time.time()\n",
    "\n",
    "grid.fit(x_localtrain, y_localtrain)\n",
    "\n",
    "end = time.time()\n",
    "duration = end-start\n",
    "print (\"It takes {} seconds\".format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.561887739802\n",
      "colsample_bytree: 1.0\n",
      "learning_rate: 0.1\n",
      "max_depth: 10\n",
      "n_estimators: 24\n",
      "objective: 'regression'\n",
      "reg_alpha: 0.5\n",
      "reg_lambda: 1.3\n",
      "subsample: 0.9\n"
     ]
    }
   ],
   "source": [
    "#get/show the best parameters\n",
    "best_parameters, score, _ = min(grid.grid_scores_, key=lambda x: x[1])\n",
    "print('score:', score)\n",
    "\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM RMSLE is : 0.5120897445327701\n",
      "It takes 1.2458980083465576 seconds\n"
     ]
    }
   ],
   "source": [
    "#use best model to predict\n",
    "start = time.time()\n",
    "lgb_regressor = LGBMRegressor(\n",
    "    colsample_bytree=best_parameters[\"colsample_bytree\"],\n",
    "    learning_rate=best_parameters[\"learning_rate\"],\n",
    "    max_depth=best_parameters[\"max_depth\"],\n",
    "    n_estimators=best_parameters[\"n_estimators\"],\n",
    "    objective=best_parameters[\"objective\"],\n",
    "    reg_alpha=best_parameters[\"reg_alpha\"],\n",
    "    reg_lambda=best_parameters[\"reg_lambda\"],\n",
    "    subsample=best_parameters[\"subsample\"]\n",
    ")\n",
    "\n",
    "y_localtrain_log = np.log1p(y_localtrain)\n",
    "\n",
    "model = lgb_regressor.fit(x_localtrain, y_localtrain_log)\n",
    "y_localvalprediction1 = model.predict(x_localval)\n",
    "\n",
    "y_localvalprediction = np.expm1(y_localvalprediction1)\n",
    "\n",
    "rmsle_lgb = eval_rmsle(y_localvalprediction, y_localval)\n",
    "print (\"LGBM RMSLE is : {}\".format(rmsle_lgb))\n",
    "\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print (\"It takes {} seconds\".format(duration))\n",
    "\n",
    "compare_lgb = compare_result(preds=y_localvalprediction, true=y_localval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>real_cost</th>\n",
       "      <th>pred_cost</th>\n",
       "      <th>error_percent_(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236487</td>\n",
       "      <td>26</td>\n",
       "      <td>18.749872</td>\n",
       "      <td>27.885106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181786</td>\n",
       "      <td>29</td>\n",
       "      <td>25.677078</td>\n",
       "      <td>11.458353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163879</td>\n",
       "      <td>58</td>\n",
       "      <td>39.483905</td>\n",
       "      <td>31.924302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22266</td>\n",
       "      <td>4</td>\n",
       "      <td>10.235186</td>\n",
       "      <td>155.879645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>229422</td>\n",
       "      <td>53</td>\n",
       "      <td>17.266205</td>\n",
       "      <td>67.422255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>188981</td>\n",
       "      <td>22</td>\n",
       "      <td>21.267665</td>\n",
       "      <td>3.328795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>239241</td>\n",
       "      <td>35</td>\n",
       "      <td>41.632357</td>\n",
       "      <td>18.949592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>158829</td>\n",
       "      <td>22</td>\n",
       "      <td>21.575819</td>\n",
       "      <td>1.928094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>38316</td>\n",
       "      <td>1</td>\n",
       "      <td>18.007237</td>\n",
       "      <td>1700.723660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>97073</td>\n",
       "      <td>1</td>\n",
       "      <td>4.998981</td>\n",
       "      <td>399.898067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id  real_cost  pred_cost  error_percent_(%)\n",
       "0   236487         26  18.749872          27.885106\n",
       "1   181786         29  25.677078          11.458353\n",
       "2   163879         58  39.483905          31.924302\n",
       "3    22266          4  10.235186         155.879645\n",
       "4   229422         53  17.266205          67.422255\n",
       "5   188981         22  21.267665           3.328795\n",
       "6   239241         35  41.632357          18.949592\n",
       "7   158829         22  21.575819           1.928094\n",
       "8    38316          1  18.007237        1700.723660\n",
       "9    97073          1   4.998981         399.898067"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_lgb.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_lgb(params, x_train, y_train, kf, verbose=True, verbose_eval=50):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    #test_pred = np.zeros((x_test.shape[0]))\n",
    "\n",
    "    #if len(cat_cols)==0: use_cat=False\n",
    "\n",
    "    # use the k-fold object to enumerate indexes for each training and validation fold\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)): # folds 1, 2 ,3 ,4, 5\n",
    "        # example: training from 1,2,3,4; validation from 5\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n",
    "        y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n",
    "\n",
    "\n",
    "        # Might not be required in this competition\n",
    "        #if use_cat:\n",
    "        #    lgb_train = lgb.Dataset(x_train_kf, y_train_kf, categorical_feature=cat_cols)\n",
    "        #    lgb_val = lgb.Dataset(x_val_kf, y_val_kf, reference=lgb_train, categorical_feature=cat_cols)\n",
    "        #else:\n",
    "        lgb_train = lgb.Dataset(x_train_kf, y_train_kf)\n",
    "        lgb_val = lgb.Dataset(x_val_kf, y_val_kf, reference=lgb_train)\n",
    "        \n",
    "        #watchlist= [(lgb_train, \"train\"), (lgb_val, 'val')]\n",
    "        watchlist= [lgb_train, lgb_val]\n",
    "\n",
    "        gbm = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=4000,\n",
    "                        #valid_sets=lgb_val,\n",
    "                        valid_sets=watchlist, \n",
    "                        early_stopping_rounds=30,\n",
    "                        verbose_eval=verbose_eval,\n",
    "                        #feval=eval_rmsle\n",
    "                       )\n",
    "\n",
    "        val_pred = gbm.predict(x_val_kf)\n",
    "\n",
    "        # Might not be required in this competition\n",
    "        #if use_rank:\n",
    "        #    train_pred[val_index] += probability_to_rank(val_pred)\n",
    "        #    test_pred += probability_to_rank(gbm.predict(x_test))\n",
    "        #    # test_pred += gbm.predict(x_test)\n",
    "        #else:\n",
    "        train_pred[val_index] += val_pred\n",
    "        #test_pred += gbm.predict(x_test)\n",
    "            \n",
    "        #fold_auc = roc_auc_score(y_val_kf.values, val_pred)\n",
    "        #fold_gini_norm = auc_to_gini_norm(fold_auc)\n",
    "        fold_rmsle = eval_rmsle(val_pred, y_val_kf.values)\n",
    "\n",
    "        if verbose:\n",
    "            #print('fold cv {} AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(i, fold_auc, fold_gini_norm))\n",
    "            print('fold cv {} RMSLE score is {:.6f}'.format(i, fold_rmsle))\n",
    "\n",
    "    #test_pred /= kf.n_splits\n",
    "\n",
    "    #cv_auc = roc_auc_score(y_train, train_pred)\n",
    "    #cv_gini_norm = auc_to_gini_norm(cv_auc)\n",
    "    cv_rmsle = eval_rmsle(train_pred, y_train)\n",
    "    #cv_score = [cv_auc, cv_gini_norm]\n",
    "    #cv_score = [cv_rmsle]\n",
    "    if verbose:\n",
    "        #print('cv AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(cv_auc, cv_gini_norm))\n",
    "        print('cv RMSLE score is {:.6f}'.format(cv_rmsle))\n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    "\n",
    "    #return cv_score, train_pred,test_pred\n",
    "    return cv_rmsle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's test the above cross validation function with some simple lgbm parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\ttraining's rmse: 11.3071\tvalid_1's rmse: 10.5027\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's rmse: 11.0605\tvalid_1's rmse: 10.2589\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\ttraining's rmse: 10.2892\tvalid_1's rmse: 12.5188\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's rmse: 10.0179\tvalid_1's rmse: 12.2622\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[50]\ttraining's rmse: 11.4438\tvalid_1's rmse: 10.1763\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's rmse: 11.1957\tvalid_1's rmse: 9.92261\n",
      "cv score is 0.605949\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "    'boosting_type': 'dart',\n",
    "    'max_depth' : 5,\n",
    "    'max_bin' : 500,\n",
    "    'learning_rate': 0.1,  # 0.618580\n",
    "    'num_leaves': 22,\n",
    "    'metric': 'RMSE'\n",
    "}\n",
    "\n",
    "# only do 3 fold CV here so that we save some running time on Kaggle Kernel\n",
    "kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "#cv_score =cross_validate_xgb(xgb_params, train_input, y, kf, verbose=False, verbose_eval=50)\n",
    "cv_score =cross_validate_lgb(lgb_params, train_input, y, kf, verbose=False, verbose_eval=50)\n",
    "\n",
    "print('cv score is {:.6f}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimsation - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'num_leaves':(2,30),\n",
    "    'min_sum_hessian_in_leaf':(2,30),\n",
    "    'max_depth':(2,30),\n",
    "    'feature_fraction':(0.0,1.0),\n",
    "    'bagging_fraction':(0.0,1.0),\n",
    "#    'max_drop':(1,20),\n",
    "#    'drop_rate':(0.0,1.0),\n",
    "#    'min_data_in_leaf':(2,30),\n",
    "    'bagging_freq':(0,2),\n",
    "    'lambda_l1':(0.0,1.0)\n",
    "#    'lambda_l2':(0.0,1.0),\n",
    "#    'n_estimators':(2,30), \n",
    "#    'colsample_bytree':(0.4, 1),\n",
    "#    'reg_lambda':(0.0,2.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(lgb_wrapper)\n",
    "#def lgbcv_func(max_depth, learning_rate, subsample, colsample_bytree, nthread=4, seed=0):\n",
    "def lgbcv_func(num_leaves, min_sum_hessian_in_leaf, max_depth, \n",
    "               feature_fraction, bagging_fraction, bagging_freq, lambda_l1, nthread=4, seed=0):\n",
    "\n",
    "    params = {\n",
    "        'objective' : \"regression\",\n",
    "        'learning_rate': 0.1,       \n",
    "        'task': 'train',\n",
    "        'boosting_type': 'dart',\n",
    "        'metric': 'RMSE'\n",
    "    }\n",
    "    \n",
    "    # for a more ideal out-of-fold model prediction for this dataset, we use 10-fold CV\n",
    "    kf=StratifiedKFold(n_splits=3, shuffle=True, random_state=2018)\n",
    "    \n",
    "    # we will disable all the verbose setting in this functional call, so that we don't have too much information \n",
    "    # to read during the bayesian optimisation process.\n",
    "    return 1-cross_validate_lgb(params, train_input, y, kf, verbose=False, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_bo=BayesianOptimization(lgbcv_func, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   bagging_fraction |   bagging_freq |   feature_fraction |   lambda_l1 |   max_depth |   min_sum_hessian_in_leaf |   num_leaves | \n",
      "    1 | 00m09s | \u001b[35m   0.39466\u001b[0m | \u001b[32m            0.3508\u001b[0m | \u001b[32m        1.3121\u001b[0m | \u001b[32m            0.1093\u001b[0m | \u001b[32m     0.2764\u001b[0m | \u001b[32m    11.6430\u001b[0m | \u001b[32m                  24.7519\u001b[0m | \u001b[32m     25.8416\u001b[0m | \n",
      "    2 | 00m09s |    0.39466 |             0.8101 |         0.5614 |             0.9704 |      0.6502 |      5.0086 |                    7.4378 |      19.8911 | \n",
      "    3 | 00m09s |    0.39466 |             0.4453 |         0.9001 |             0.3496 |      0.2525 |      9.2718 |                    9.0385 |      17.8205 | \n",
      "    4 | 00m09s |    0.39466 |             0.3424 |         1.4994 |             0.9922 |      0.8231 |      8.2559 |                   12.7862 |       9.0365 | \n",
      "    5 | 00m10s |    0.39466 |             0.0665 |         1.6014 |             0.5564 |      0.2289 |     10.7105 |                   10.1070 |      24.2132 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   bagging_fraction |   bagging_freq |   feature_fraction |   lambda_l1 |   max_depth |   min_sum_hessian_in_leaf |   num_leaves | \n",
      "    6 | 00m10s |    0.39466 |             0.4886 |         0.5516 |             0.1130 |      0.2141 |     29.1371 |                   20.7532 |      25.6101 | \n",
      "    7 | 00m11s |    0.39466 |             0.6805 |         1.9181 |             0.1875 |      0.5116 |     19.7972 |                    3.9800 |       8.6022 | \n",
      "    8 | 00m09s |    0.39466 |             0.3127 |         0.5126 |             0.9259 |      0.4808 |     11.4745 |                   13.3308 |      10.5544 | \n",
      "    9 | 00m21s |    0.39466 |             0.3652 |         1.7772 |             0.8727 |      0.9179 |     24.0678 |                    4.7034 |      13.8923 | \n",
      "   10 | 00m14s |    0.39466 |             0.7944 |         0.7519 |             0.5649 |      0.2865 |     25.6572 |                    4.5721 |      24.7013 | \n",
      "   11 | 00m17s |    0.39466 |             0.0724 |         0.4114 |             0.1375 |      0.1193 |     24.2588 |                   21.9500 |      11.2746 | \n",
      "   12 | 00m21s |    0.39466 |             0.6952 |         0.0514 |             0.4183 |      0.1582 |      5.9952 |                   29.7172 |      29.2712 | \n",
      "   13 | 00m24s |    0.39466 |             0.3404 |         1.0781 |             0.4049 |      0.3734 |     19.8108 |                    5.5795 |       8.4762 | \n",
      "   14 | 00m25s |    0.39466 |             0.9528 |         1.6143 |             0.9216 |      0.3811 |     11.4582 |                   28.5783 |      15.7604 | \n",
      "   15 | 00m25s |    0.39466 |             0.8626 |         0.7207 |             0.5639 |      0.8403 |     26.1845 |                   14.8529 |      12.0548 | \n",
      "   16 | 00m23s |    0.39466 |             0.5813 |         0.8161 |             0.3297 |      0.6814 |     25.9043 |                   19.0704 |      23.5823 | \n",
      "   17 | 00m15s |    0.39466 |             0.7306 |         1.2855 |             0.8535 |      0.3032 |      8.1154 |                    3.6125 |      17.6610 | \n",
      "   18 | 00m14s |    0.39466 |             0.1293 |         1.2791 |             0.3401 |      0.3531 |     12.4214 |                    3.4855 |       2.6092 | \n",
      "   19 | 00m14s |    0.39466 |             0.1547 |         0.9679 |             0.6758 |      0.3114 |      8.7951 |                   20.9132 |      23.3301 | \n",
      "   20 | 00m14s |    0.39466 |             0.6410 |         0.4938 |             0.1515 |      0.0309 |      4.7425 |                   16.0205 |      19.6766 | \n",
      "   21 | 00m25s |    0.39466 |             0.0125 |         1.8893 |             0.1478 |      0.0411 |     13.7414 |                   13.8995 |      13.9192 | \n",
      "   22 | 00m25s |    0.39466 |             0.6011 |         1.3820 |             0.6257 |      0.1397 |     24.7640 |                   13.5101 |       6.2002 | \n",
      "   23 | 00m25s |    0.39466 |             0.2326 |         0.5529 |             0.3096 |      0.0369 |     14.7315 |                   25.6047 |       5.1183 | \n",
      "   24 | 00m26s |    0.39466 |             0.1757 |         0.1904 |             0.3896 |      0.0632 |      8.1919 |                   24.7309 |       4.5126 | \n",
      "   25 | 00m23s |    0.39466 |             0.1890 |         1.2885 |             0.5604 |      0.9954 |     25.8135 |                   27.6240 |       6.7999 | \n"
     ]
    }
   ],
   "source": [
    "lgb_bo.maximize(init_points=5, n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    'subsample' : \n",
    "    'reg_alpha' : \n",
    "    'min_split_gain' :[],\n",
    "    'subsample_for_bin' :[],\n",
    "    'max_drop' :[], \n",
    "    'gaussian_eta' :[], \n",
    "    'drop_rate' :[],\n",
    "    'silent' :[], \n",
    "    'boosting_type' :['gbdt'], \n",
    "    'min_child_weight' :[], \n",
    "    'skip_drop' :[], \n",
    "    'fair_c' :[], \n",
    "    'seed' :[], \n",
    "    'poisson_max_delta_step' :[], \n",
    "    'subsample_freq' :[], \n",
    "    'max_bin' :[],  #55\n",
    "    'nthread' :[], \n",
    "    'min_child_samples' :[], \n",
    "    'huber_delta' :[], \n",
    "    'use_missing' :[], \n",
    "    'uniform_drop' :[], \n",
    "    'bagging_fraction': [] #0.8,\n",
    "    'bagging_freq': [] # 5\n",
    "    'feature_fraction': [] # 0.2319,\n",
    "    'feature_fraction_seed': [] #9\n",
    "    'bagging_seed': [] #9,\n",
    "    'min_data_in_leaf': [] #6\n",
    "    'min_sum_hessian_in_leaf': [] # 11                              \n",
    "    'xgboost_dart_mode' :[]\n",
    "\n",
    "    'max_depth':(4,10),\n",
    "    'learning_rate':(0.05,0.3),\n",
    "    'subsample': (0.4, 1),\n",
    "    'colsample_bytree': (0.4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score: 0.561887739802\n",
    "colsample_bytree: 1.0\n",
    "learning_rate: 0.1\n",
    "max_depth: 10\n",
    "n_estimators: 24\n",
    "objective: 'regression'\n",
    "reg_alpha: 0.5\n",
    "reg_lambda: 1.3\n",
    "subsample: 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cross validation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "\n",
    "### Kfold or Stratified Kfold\n",
    "kf=KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "#kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "\n",
    "### Criterion function\n",
    "def auc_to_gini_norm(auc_score):\n",
    "    return 2*auc_score-1\n",
    "\n",
    "def eval_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def eval_rmsle(y, pred):\n",
    "    return mean_squared_error(y, pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modified cross-validation function copied from Yifan's cool kernel. \n",
    "### Source: https://www.kaggle.com/yifanxie/porto-seguro-tutorial-end-to-end-ensemble\n",
    "\n",
    "def cross_validate_sklearn(clf, x_train, y_train, x_test, kf, scale=False, verbose=True):\n",
    "    start_time=time.time()\n",
    "    \n",
    "    # initialise the size of out-of-fold train an test prediction\n",
    "    train_pred = np.zeros((x_train.shape[0]))\n",
    "    test_pred = np.zeros((x_test.shape[0]))\n",
    "\n",
    "    # use the kfold object to generate the required folds\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        # generate training folds and validation fold\n",
    "        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[test_index, :]\n",
    "        y_train_kf, y_val_kf = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        # perform scaling if required i.e. for linear algorithms\n",
    "        if scale:\n",
    "            scaler = StandardScaler().fit(x_train_kf.values)\n",
    "            x_train_kf_values = scaler.transform(x_train_kf.values)\n",
    "            x_val_kf_values = scaler.transform(x_val_kf.values)\n",
    "            x_test_values = scaler.transform(x_test.values)\n",
    "        else:\n",
    "            x_train_kf_values = x_train_kf.values\n",
    "            x_val_kf_values = x_val_kf.values\n",
    "            x_test_values = x_test.values\n",
    "        \n",
    "        ###################################################\n",
    "        # Please update for your own criterion \n",
    "        ###################################################\n",
    "        # fit the input classifier and perform prediction.\n",
    "        #clf.fit(x_train_kf_values, y_train_kf.values)\n",
    "        clf.fit(x_train_kf_values, np.log1p(y_train_kf.values))\n",
    "\n",
    "        ###################################################\n",
    "        # Please update for your own criterion \n",
    "        ###################################################\n",
    "        #val_pred=clf.predict_proba(x_val_kf_values)[:,1]\n",
    "        val_pred=clf.predict(x_val_kf_values)\n",
    "\n",
    "        train_pred[test_index] += val_pred\n",
    "\n",
    "        ###################################################\n",
    "        # Please update for your own criterion \n",
    "        ###################################################\n",
    "        #y_test_preds = clf.predict_proba(x_test_values)[:,1]\n",
    "        y_test_preds = clf.predict(x_test_values)\n",
    "        test_pred += y_test_preds\n",
    "\n",
    "        ###################################################\n",
    "        # Please update for your own criterion \n",
    "        ###################################################\n",
    "        #fold_auc = roc_auc_score(y_val_kf.values, val_pred)\n",
    "        #fold_gini_norm = auc_to_gini_norm(fold_auc)\n",
    "        fold_rmsle = eval_rmsle(np.log1p(y_val_kf.values), val_pred)\n",
    "\n",
    "        if verbose:\n",
    "            ###################################################\n",
    "            # Please update for your own criterion\n",
    "            ###################################################\n",
    "\n",
    "            #print('fold cv {} AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(i, fold_auc, fold_gini_norm))\n",
    "            print('fold cv {} RMSLE score is {:.6f}'.format(i, fold_rmsle))\n",
    "\n",
    "    test_pred /= kf.n_splits\n",
    "\n",
    "    ###################################################\n",
    "    # Please update for your own criterion\n",
    "    ###################################################\n",
    "    #cv_auc = roc_auc_score(y_train, train_pred)\n",
    "    #cv_gini_norm = auc_to_gini_norm(cv_auc)\n",
    "    cv_rmsle = eval_rmsle(np.log1p(y_train), train_pred)\n",
    "        \n",
    "    #cv_score = [cv_auc, cv_gini_norm]\n",
    "    cv_score = [cv_rmsle]\n",
    "    if verbose:\n",
    "\n",
    "        ###################################################\n",
    "        # Please update for your own criterion\n",
    "        ###################################################\n",
    "        #print('cv AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(cv_auc, cv_gini_norm))\n",
    "        print('cv RMSLE score is {:.6f}'.format(cv_rmsle))\n",
    "        end_time = time.time()\n",
    "        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n",
    "    return cv_score, train_pred, test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold cv 0 RMSLE score is 0.519012\n",
      "fold cv 1 RMSLE score is 0.519113\n",
      "fold cv 2 RMSLE score is 0.516890\n",
      "fold cv 3 RMSLE score is 0.515128\n",
      "fold cv 4 RMSLE score is 0.514956\n",
      "cv RMSLE score is 0.517023\n",
      "it takes 1131.644 seconds to perform cross validation\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(criterion='mse',\n",
    "                            max_depth=5,\n",
    "                            n_estimators=500,\n",
    "                            min_samples_leaf=5,\n",
    "                            min_samples_split=2\n",
    "                           )\n",
    "\n",
    "outcomes =cross_validate_sklearn(rfr, x_train, y_train ,x_test, kf, scale=False, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
